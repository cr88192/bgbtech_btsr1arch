== Instructions Organized by Name with Descriptions ==

Register Aliases:
* DLR will be an alias for R0.
* DHR will be an alias for R1.
* SP will be an alias for R15.

Major ISA Variants:
* XG1: Will include 16 bit ops.
* XG2: Will extend registers to 6-bits.
** Imm9 extended to Imm10, Disp9u to Disp10s.
** It is otherwise mostly the same as XG1.
** The liberated high 3 bits will encode WN, WM, and WI bits.
*** XG2 may not encode 16-bit ops.
*** Extending each register, or the immediate, as needed.
*** Typically, if no register exists, the immediate is extended.
*** In other cases, the bits will go to opcode.
* XG3: Will be mostly a bit-shuffled XG2.
** Lacks 16-bit instructions;
** Various special case differences exist.


Notation:
* 0..9, A..F: Literal Hexadecimal Values
* e: The 'E' bits 'QNMI'
** Q will give an operation size or type.
** N will give Bit 4 of the N field (usually Rn).
** M will give Bit 4 of the M field (usually Rm).
** I will give Bit 4 of the O field (usually Ro).
*** Alternately may serve as the MSB for an immediate.
*** If no Ro or Imm, serves a role similar to Q.
* f: -
* g: E bits (3R), but Q=0, thus 0nmi
* G: E bits (3R), but Q=1, thus 1nmi
* h: E bits (I,R), but Q=0, thus 0nii or 0iin
* H: E bits (I,R), but Q=1, thus 1nii or 1iin
** Two of the register fields will be interpreted as an immediate.
* i: Immediate (Signed)
* j: Immediate (Unsigned)
* m: Source Register
* n: Destination Register
* o: Index or Secondary Register
* p: E bits (2R), but Q=0, I=0, thus 0nm0
* P: E bits (2R), but Q=1, I=0, thus 1nm0
* q: E bits (2R), but Q=0, I=1, thus 0nm1
* Q: E bits (2R), but Q=1, I=1, thus 1nm1

Single register forms in 30zz..37zz will be mirrored in 38zz..3Fzz, but will encode on R16..R31 rather than R0..R15.

The instructions in the F0zz..F3zz range will repeat in F4zz..F7zz (Parallel), E0zz..E3zz (Execute if True), and E4zz..E7zz (Execute if False) ranges.

The instructions in the F8zz/F9zz range will repeat in FCzz/FDzz (Parallel), E8zz/E9zz (Execute if True), and ECzz/EDzz (Execute if False) ranges.


The 7wnm_ZeoZ range will repeat the F0nm_ZeoZ range, just with the register fields extended to 6 bits, and the high bit of W will encode the WEX flag.

The 9wnm_Zeii range will repeat the F2nm_Zeii and F1nm_Zeii and ranges, with 6 bit register IDs, just with Wi working as a range selector.

The FEii_iiii-Fzzz_zzzz space will extend the immediate field by 24 bits for encodings which have an immediate. 'FE' or 'FE-FE' may be used as a shorthand for encodings which have one or two jumbo prefixes.

The FFw0_0pii-Fzzz_zzzz space (W0) will repeat the entire 32-bit encoding space, just with registers expanded to 6 bits, and any immediate fields extended by 8 bits (MBZ for instructions which do not have an immediate field).

Given the same base instructions apply to every sub-range, typically only those in the base ranges will be listed, except when the sub-range instruction differs from the base-range instruction.



Immediates may have a suffix to indicate how the value is extended:
* u: Value is extended with zeroes.
* n: Value is extended with ones.
* s: Value is sign extended.


=== ADC ===

* 12nm       ADC		Rm, Rn
* F0nm_1g29  ADC		Rm, Rn

Add with Carry, Rn=Rn+Rm+SR.T, with SR.T being updated to reflect the carry-out bit.

WEX: ADC will only update SR.T in Lane 1. If an ADC is in another lane, the state of SR.T will be undefined following the operation.


=== ADD ===

* 10nm       ADD		Rm, Rn
* 58nm       ADD		Rm, DLR, Rn			//Rn=Rm+DLR
* Cnii       ADD		Imm8s, Rn
* F0nm_1go0  ADD		Rm, Ro, Rn			//Rn=Rm+Ro
* F0nm_1e09 /? ADD		Rm, Rn				//Rn=Rn+Rm (Deprecated)
* F2nm_0gjj  ADD		Rm, Imm9u, Rn		//
* F2nm_1gjj  ADD		Rm, Imm9n, Rn		//
* F2n0_Dhjj  ADD		Imm10u, Rn			//Rn=Rn+Imm10u
* F2n1_Dhjj  ADD		Imm10n, Rn			//Rn=Rn+Imm10n
* F84n_iiii  ADD		Imm16s, Rn
* F85n_iiii  ADD		Imm16s, Rk

*    FE F2nm_0gjj  ADD	Rm, Imm33s, Rn	//(Jumbo)
* FE-FE F2nm_0gjj  ADD	Rm, Imm57s, Rn	//(FPIMM)

*    FE F2n0_Dhjj  ADD	Imm33s, Rn		//Rn=Rn+Imm33s

* FE-FE F84n_iiii  ADD	Imm64, Rn		//R0 ..R15
* FE-FE F85n_iiii  ADD	Imm64, Rk		//R16..R31
* FE-FE F86n_iiii  ADD	Imm64, Rk		//R32..R47
* FE-FE F87n_iiii  ADD	Imm64, Rk		//R48..R63

Add the source and destination values and store the result in the destination register.


=== ADDx.L ===

* F0nm_5goC     ADDS.L		Rm, Ro, Rn
* F0nm_5GoC     ADDU.L		Rm, Ro, Rn
* F2gj_3gjj     ADDS.L		Rm, Imm9u, Rn
* F2nm_3Gjj     ADDU.L		Rm, Imm9u, Rn
* F2nm_4gjj     ADDS.L		Rm, Imm9n, Rn
* F2nm_4Gjj     ADDU.L		Rm, Imm9n, Rn
* FE F2nm_3gjj  ADDS.L		Rm, Imm33s, Rn		//(Jumbo)
* FE F2nm_3Gjj  ADDU.L		Rm, Imm33s, Rn		//(Jumbo)

Add the source and destination values and store the result in the destination register.

This form sign or zero extends the 32-bit result to 64 bits.


=== AND ===

* 15nm             AND		Rm, Rn
* 5Anm             AND		Rm, DLR, Rn
* F0nm_1go5        AND		Rm, Ro, Rn
* F0nm_1e59     /? AND		Rm, Rn			//Deprecated
* F2nm_5gjj        AND		Rm, Imm9us, Rn

* FE F2nm_5gjj     AND		Rm, Imm33s, Rn	//(Jumbo)

* FE-FE F2nm_5gjj  AND		Rm, Imm57s, Rn	//(FPIMM)

Perform a bitwise AND of the source and destination values and store the result in the destination register.

In Baseline mode, the Imm9 immediate is always zero extended.

In XG2, using the WI flag will encode a one-extended immediate (similar to Load/Store displacements).


=== BCDADC / BCDSBB (BCD) ===

* F0nm_1g88 BCDADC	Rm, Rn
* F0nm_1g98 BCDSBB	Rm, Rn

Perform addition or subtraction of Packed BCD numbers.

The SR.T flag is used as a Carry or Borrow flag, and is updated based the result of the leftmost digit.

For this instruction, the 64-bit register is interpreted as a 16-digit BCD number, with each 4 bits corresponding to a decimal digit (0..9). When performing an addition, if the result of adding a pair of digits (plus the carry) is larger than 9, then 6 will be added in this location (so A maps to 0, B to 1, ...), and a carry will be propagated to the next digit.

The SBB case will be similar to ADC, but will perform a 9s complement remapping (0123456789 -> 9876543210) on Rm and will invert the value of SR.T.


These instructions may be daisy-chained to support larger numbers.

This instruction is only allowed in Lane 1.


=== BF / JF ===

* 23dd       BF		(PC, Disp8s)
* 31n3       BF		(PC, Rn)
* / F013_3en0  BF		(PC, Rn)			//Fix32
* / F0dd_Fddd  BF		(PC, Disp20s)		//Branch False, +/- 1MB

* / 32n3       JF		Rn
* / F023_3en0  JF		Rn					//Fix32

* FFw0_0ddd_F0dd_Fddd /?  BF	(PC, Disp33s)

Branch if False (SR.T is 0).
The target address is computed and PC is updated to the new address if the condition is met.

This instruction is only allowed in Lane 1.

The BF Disp20s has been replaced with BRA?F Disp20s.


=== BITNN (BITNN) ==

* F0nm_7Go2  BITNN	Rm, Ro, Rn			//(BITNN) Bit Neural Net Eval

Binary Neural Net Evaluator.

This instruction may only be used in Lane 1, and will not be allowed in 3-wide bundles.

Rm:
* Will be 16x 1-bit inputs in 1-bit mode,
* or 16x 2b in 2-bit mode.

Ro:
* (47: 0), Weights, 3 bits each
* (55:48), Bias, 8 bit sign-extended.
* (57:56), Mode
* (   58), Range Flag
* (63:59), Reserved (MBZ)

Rn:
* Will hold the output bits, and will be both a 3rd source and destination.

Mode:
* 00: 1-bit in, 1-bit out
* 01: 2-bit in, 1 bit out
* 10: 1-bit in, 2 bit out
* 11: 2-bit in, 2-bit out

In 1-bit input mode, each input will be 1 bit:
* 0 => -1, 1 => 1

In 2 bit input mode, each input will be 2 bits:
* 00 => 0
* 01 => 1
* 10 => 0
* 11 => -1

Weights will be 3 bits and defined as:
* 000 => 0
* 001 => 1
* 010 => 3
* 011 => 7
* 100 => 0
* 101 => -1
* 110 => -3
* 111 => -7

Each input will be multiplied by each weight, then the bias will be added, and the final result will be compared against 0.

Output will shift the 3rd input to the left and add the new output into the low-order bit(s).

Output in 1 bit mode will be based on whether the result is greater than or equal to zero.
* 0: Result (after adding bias) was negative.
* 1: Result (after adding bias) was positive or zero.

In 2 bit output mode, the output will be +1 or -1.
* 01: Result is positive.
* 11: Result is negative.

The Range Flag will modify the behavior of bias handling:
* 0: The intermediate result and bias will be added with an 8 bit range.
* 1: The intermediate result and bias will be added with a 9 bit range.

The Range Flag will modify the behavior in the operation in the case where adding the bias would lead to an overflow outside of the 8-bit range. If this flag is set, the range will be expanded such that the overflow will not happen.


=== BNDCHK / BNDCMP (TTAG) ===

* F0nm_1g08 BNDCHK.B	Rm, Rn				//Bounds Check (Byte Scale)
* F0nm_1g18 BNDCHK.W	Rm, Rn				//Bounds Check (Word Scale)
* F0nm_1g28 BNDCHK.L	Rm, Rn				//Bounds Check (DWord Scale)
* F0nm_1g38 BNDCHK.Q	Rm, Rn				//Bounds Check (QWord Scale)

* F0nm_3g08 BNDCMP.B	Rm, Rn				//Bounds Compare (Byte Scale)
* F0nm_3g18 BNDCMP.W	Rm, Rn				//Bounds Compare (Word Scale)
* F0nm_3g28 BNDCMP.L	Rm, Rn				//Bounds Compare (DWord Scale)
* F0nm_3g38 BNDCMP.Q	Rm, Rn				//Bounds Compare (QWord Scale)

* F0nm_1g48 BNDCHK		RmImm6u, Rn			//Bounds Compare
* F0nm_1g58 BNDCMP		RmImm6u, Rn			//Bounds Compare

Check or Compare Array Bounds.

The Bounds Check operation will verify whether an index (given) in Rm, falls within the bounds of the array pointer given in Rn.

The immediate forms will give the bounds to check as a byte offset.


The pointer is assumed to follow the tagged pointer scheme defined in the ABI spec.

The Check operation will determine bounds and, if the bounds check fails, will raise a bounds check exception. It will also raise an exception if given a tagged pointer that does not represent either a raw pointer or a bounded array.


The Compare operation is similar, but will update SR.T based on the result.
SR.T will be set if the access is in-bounds, or cleared if the access is out-of-bounds or is an unhandled type.

Check and Compare will differ regarding the handling of Tagged Object:
* BNDCHK.x will allow access to all Tagged Object types.
* BNDCMP.x will only allow Tagged Object if the tag is 0000.


The Bounded Pointer format is:
* Bits (63:60)=Tag (0011)
* Bits (59:56)=Array Bias
* Bits (55:48)=Array Bound
* Bits (47: 0)=Address

The bound is given in bits (55:48) as an E5.F3 microfloat.
* 00:  0  01:  1  02:  2  03:  3  04:  4  05:  5  06:  6  07:  7
* 08:  8  09:  9  0A: 10  0B: 11  0C: 12  0D: 13  0E: 14  0F: 15
* 10: 16  11: 18  12: 20  13: 22  14: 24  15: 26  16: 28  17: 30
* 18: 32  19: 36  1A: 40  1B: 44  1C: 48  1D: 52  1E: 56  1F: 60
* 20: 64  21: 72  22: 80  23: 88  24: 96  25:104  26:112  27:120
* 28:128  29:144  2A:160  2B:176  2C:192  2D:208  2E:224  2F:240
* 30:256  31:288  32:320  33:352  34:384  35:416  36:448  37:480
* 38:512  39:576  3A:640  3B:704  3C:768  3D:832  3E:896  3F:960
* 40:1024 ...
* ...

The Bias is a value with the same exponent as the Bound, but represents a non-normalized value. It is added to the index prior to checking against the bound.

These instructions will be part of the TTAG extension.


=== BRA / JMP ===

* 20dd       BRA		(PC, Disp8s)
* 31n0       BRA		(PC, Rn)
* F010_3en0  BRA		(PC, Rn)			//Fix32
* F0dd_Cddd  BRA		(PC, Disp20s)		//Fix32

* 32n0       JMP		Rn
* F020_3en0  JMP		Rn					//Fix32

* F0ni_3G88 ? JMPMD		Imm5u, Rn		//JMP with Mode Change
* F0ni_3G98 ? JSRMD		Imm5u, Rn		//JSR with Mode Change

* FEjj_jjjj_F202_C4jj /	BRA		(PC, Disp33s)
* FFw0_0ddd_F0dd_Cddd	BRA		(PC, Disp33s)
* FFjj_jjjj_FAjj_jjjj	JMP		Abs48		//Abs48 Branch

Branch to Address.
For BRA, the target address is computed and PC is updated to the new address.

For JMP, this performs an absolute jump to the address given.

As a special case, if JMP is used with DHR, the high order bits of DHR will be interpreted as if this were an RTS instruction (copying these bits into the SR user flags). Otherwise, the contents of the SR user flags will be unmodified.

This instruction is only allowed in Lane 1.

In XG2 Mode, the WN, WM, and WI flags will extend Disp20s to Disp23s. In this case, the WN/WM/WI bits will be XOR'ed with the 20 bit sign-extension to give the additional bits.

The displacement will be defined in terms of 16-bit words relative to the start of the next instruction following the branch.

In Fix32 and XG2 Mode, only even word displacements will be valid (trying to branch to an address that doesn't have a 32-bit alignment will be invalid).



For JMPMD and JSRMD, there will be an Immediate field.
In this case, a default mode transition will exist (unless overridden by the pointer) giving the requested mode:
* { WX4, WX3, WX2, WXE }
** 0: Baseline (WEX Disabled)
** 1: Baseline (WEX Enabled)
** 2: RV64GC
** 3: Reserved
** 4: XG2 (WEX Disabled)
** 5: XG2 (WEX Enabled)
** 6: XG2RV (WEX Disabled)
** 7: XG2RV (WEX Enabled)

=== BREAK ===

* 3030       BREAK
* F000_3030  BREAK2	//Fix32

Trigger a Breakpoint exception.

The BREAK1/2/3 forms overlap with MOV.B, but will be regarded as invalid special cases which will result in an exception rather than a MOV.


=== BSWAP ===

* F0nm_1gFA  BSWAPU.L	Rm, Rn			//(TTAG) Byte Swap DWORD
* F0nm_1GFA  BSWAP.Q	Rm, Rn			//(TTAG) Byte Swap QWORD

Byte swap a DWORD or QWORD.
* BSWAP.L will swap the low 4 bytes, zero extending the result.
* BSWAP.Q will swap all 8 bytes.


=== BSR / JSR ===

* 21dd       BSR		(PC, Disp8s)
* 31n1       BSR		(PC, Rn)
* F011_3en0  BSR		(PC, Rn)			//Fix32
* F0dd_Dddd  BSR		(PC, Disp20s)		//Fix32

* 32n1       JSR		Rn
* F021_3en0  JSR		Rn					//Fix32

* FEjj_jjjj_F202_CCjj /	BSR		(PC, Disp33s)
* FFw0_0ddd_F0dd_Dddd	BSR		(PC, Disp33s)

* FFjj_jjjj_FBjj_jjjj	JSR		Abs48			//Abs48 Call

Branch to subroutine.
The target address is computed and PC is updated to the new address, with the prior value for PC being stored in LR(47:0).

The high order 16 bits of LR will contain saved user flag states:
* LR(63:52) will be copied from SR(15: 4)
* LR(51:50) will be copied from SR(27:26)
* LR(49:48) will be copied from SR( 1: 0)


This instruction is only allowed in Lane 1.

See BRA for general information about branches.


=== BT / JT ===

* 22dd       BT		(PC, Disp8s)		//Branch True
* 31n2       BT		(PC, Rn)			//Branch True to PC+(Rn*2)
* / F012_3en0  BT		(PC, Rn)			//Fix32
* / F0dd_Eddd  BT		(PC, Disp20s)		//Branch True, +/- 1MB

* / 32n2       JT		Rn
* / F022_3en0  JT		Rn					//Fix32

* /? FFw0_0ddd_F0dd_Eddd  BT	(PC, Disp33s)

Branch if True (SR.T is 1).
The target address is computed and PC is updated to the new address if the condition is met.

This instruction is only allowed in Lane 1.

The BT Disp20s has been replaced with BRA?T Disp20s.


=== BTRNS ===

* F0nm_1e4C  BTRNS{Q}	Rm, Rn				//(CLZ) Bit Transpose

Bit Transpose. This will transpose the bit pattern in a register such that the LSB of Rm becomes the MSB of Rn, and the MSB of Rm becomes the LSB of Rn.

This will come in both 32 and 64 bit variants.

This instruction is only allowed in Lane 1.


=== CLRS ===

* 3060       CLRS
* F000_3060  CLRS	//Fix32

Clear the SR.S flag.

This instruction is only allowed in Lane 1.


=== CLRT ===

* 3040       CLRT
* F000_3040  CLRT	//Fix32

Clear the SR.T flag.

This instruction is only allowed in Lane 1.


=== CLZ / CTZ ===

* F0nm_1e2C  CLZ{Q}		Rm, Rn		//(CLZ) Count Leading Zeroes
* F0nm_1e3C  CTZ{Q}		Rm, Rn		//(CLZ) Count Trailing Zeroes

Count Leading or Trailing Zeroes.

CLZ will start counting from the MSB.
CTZ will start counting from the LSB.
These will count the number of zeroes until the first non-zero bit is encountered. If the register is zero, then the result will be 32 or 64.

These instructions are only allowed in Lane 1.


=== CMPQxx (CMP3R) ===

* F0nm_9Go0 ? CMPQEQ	Rm, Imm5us, Rn		//(CMP3R) Rn = Rm==Imm5us
* F0nm_9Go1 ? CMPQGT	Rm, Imm5us, Rn		//(CMP3R) Rn = Rm> Imm5us
* F0nm_9Go2 ? CMPQNE	Rm, Imm5us, Rn		//(CMP3R) Rn = Rm!=Imm5us
* F0nm_9Go3 ? CMPQLT	Rm, Imm5us, Rn		//(CMP3R) Rn = Rm< Imm5us

* F0nm_9Go4 ? CMPQEQ	Rm, Ro, Rn			//(CMP3R) Rn = Rm==Ro
* F0nm_9Go5 ? CMPQGT	Rm, Ro, Rn			//(CMP3R) Rn = Rm> Ro
* F0nm_9Go6 ? CMPQNE	Rm, Ro, Rn			//(CMP3R) Rn = Rm!=Ro
* F0nm_9Go7 ? CMPQGE	Rm, Ro, Rn			//(CMP3R) Rn = Rm>=Ro

* FFw0_0vii_F0nm_9Gi0 ? CMPQEQ	Rm, Imm17s, Rn		//(CMP3R) Rn = Rm==Imm17s
* FFW0_0vii_F0nm_9Gi0 -
* FFw0_0vii_F0nm_9Go1 ? CMPQGT	Rm, Imm17s, Rn		//(CMP3R) Rn = Rm> Imm17s
* FFW0_0vii_F0nm_9Go1 ? CMPQHI	Rm, Imm17s, Rn		//(CMP3R) Rn = Rm> Imm17s
* FFw0_0vii_F0nm_9Go2 ? CMPQNE	Rm, Imm17s, Rn		//(CMP3R) Rn = Rm!=Imm17s
* FFW0_0vii_F0nm_9Go2 -
* FFw0_0vii_F0nm_9Go3 ? CMPQLT	Rm, Imm17s, Rn		//(CMP3R) Rn = Rm!=Imm17s
* FFW0_0vii_F0nm_9Go3 ? CMPQBI	Rm, Imm17s, Rn		//(CMP3R) Rn = Rm< Imm17s

* FEii_iiii_F0nm_9Gi0 ? CMPQEQ	Rm, Imm29s, Rn		//(CMP3R) Rn = Rm==Imm29s
* FEii_iiii_B0nm_9Gi0 ? CMPQHI	Rm, Imm29s, Rn		//(CMP3R, XG2)
* FEii_iiii_F0nm_9Go1 ? CMPQGT	Rm, Imm29s, Rn		//(CMP3R) Rn = Rm> Imm29s
* FEii_iiii_B0nm_9Go1 -
* FEii_iiii_F0nm_9Go2 ? CMPQNE	Rm, Imm29s, Rn		//(CMP3R) Rn = Rm!=Imm29s
* FEii_iiii_B0nm_9Go2 ? CMPQBI	Rm, Imm29s, Rn		//(CMP3R, XG2)
* FEii_iiii_F0nm_9Go3 ? CMPQLT	Rm, Imm29s, Rn		//(CMP3R) Rn = Rm>=Imm29s
* FEii_iiii_B0nm_9Go3 -

These instructions will compare two values with the result being directed to a register. The destination register will be set to 1 if the comparison is true, or set to 0 if the comparison is false.

With an Op64 prefix, the immediate forms will encode an alternate encoding if W.q is set. In XG2 Mode with a Jumbo-Imm prefix, the Wi bit will encode the alternate form.


=== CMPEQ ===

* 1Cnm       CMPEQ		Rm, Rn
* 2Cnj       CMPEQ		Imm4u, Rn
* 2Dnj       CMPEQ		Imm4n, Rn
* 6Cnj       CMPEQ		Imm4u, Rk
* 6Dnj       CMPEQ		Imm4n, Rk
* F0nm_1gC9  CMPEQ		Rm, Rn			//E.i=0, Update SR.T
* F0nm_1gC9  CMPEQS		Rm, Rn			//E.i=1, Update SR.S (PRED_S)
* F2nC_Cgjj  CMPEQ		Imm9u, Rn
* F2nD_Cgjj  CMPEQ		Imm9n, Rn

* FE F2nC_Cgjj  CMPEQ	Imm33s, Rn		//(Jumbo)

Compare if source and destination are Equal.

This provides both zero and one extended immediates, allowing a direct immediate to express values ranging between -16 and 15.


This updates SR.T based on the result of the comparison.
For the 2R form, if E.i is Set, SR.S is updated and SR.T is unchanged.
For the 2RI form, if W.m is Set, SR.S is updated and SR.T is unchanged.
The values of the other bits are unchanged by this operation.

This instruction is only allowed in Lane 1.


=== CMPGE ===

* 1Fnm       CMPGE		Rm, Rn
* 2Fnj       CMPGE		Imm4u, Rn
* 6Fnj       CMPGE		Imm4u, Rk
* F0nm_1gEC  CMPGE		Rm, Rn			//E.i=0, Signed Rn GE Rm
* F0nm_1gEC  CMPGES		Rm, Rn			//E.i=1, Update SR.S (PRED_S)
* F2nA_Cgjj  CMPGE		Imm9u, Rn
* F2nB_Cgjj  CMPGE		Imm9n, Rn

* FE F2nA_Cgjj  CMPGE	Imm33s, Rn		//(Jumbo)

Compare if Rn is Greater or Equal to the immediate.

Only exists for immediates because this can be easily emulated for the two-register forms via swapping the registers and inverting the branch condition.

This updates SR.T based on the result of the comparison.
For the 2R form, if E.i is Set, SR.S is updated and SR.T is unchanged.
For the 2RI form, if W.m is Set, SR.S is updated and SR.T is unchanged.
The values of the other bits are unchanged by this operation.

This instruction is only allowed in Lane 1.


=== CMPGT ===

* 1Enm       CMPGT		Rm, Rn
* 2Enj       CMPGT		Imm4u, Rn
* 6Enj       CMPGT		Imm4u, Rk
* F0nm_1gE9  CMPGT		Rm, Rn			//E.i=0
* F0nm_1gE9  CMPGTS		Rm, Rn			//E.i=1, Update SR.S (PRED_S)
* F2nE_Cgjj  CMPGT		Imm9u, Rn
* F2nF_Cgjj  CMPGT		Imm9n, Rn

* FE F2nE_Cgjj  CMPGT	Imm33s, Rn		//(Jumbo)

Signed Rn GT Rm.

Compare if destination is greater than the source using a signed comparison.

This updates SR.T based on the result of the comparison.
For the 2R form, if E.i is Set, SR.S is updated and SR.T is unchanged.
For the 2RI form, if W.m is Set, SR.S is updated and SR.T is unchanged.
The values of the other bits are unchanged by this operation.

This instruction is only allowed in Lane 1.


=== CMPHI ===

* 1Dnm       CMPHI		Rm, Rn
* F0nm_1gD9  CMPHI		Rm, Rn			//E.i=0
* F0nm_1gD9  CMPHIS		Rm, Rn			//(PRED_S) E.i=1, Update SR.S
* F2n8_Cgjj  CMPHI		Imm9u, Rn
* F2n8_Cgjj  CMPHI		Imm9n, Rn

* FE F2n8_Cgjj  CMPHI	Imm33s, Rn		//(Jumbo)

Unsigned Rn GT Rm.

Compare if destination is greater than the source using an unsigned comparison.

This updates SR.T based on the result of the comparison.
For the 2R form, if E.i is Set, SR.S is updated and SR.T is unchanged.
For the 2RI form, if W.m is Set, SR.S is updated and SR.T is unchanged.
The values of the other bits are unchanged by this operation.

This instruction is only allowed in Lane 1.


=== CMPHS ===

* 36nD       CMPHS		DLR, Rn
* F0nm_1gFC  CMPHS		Rm, Rn			//E.i=0, Unsigned Rn GE Rm
* F0nm_1gFC  CMPHSS		Rm, Rn			//(PRED_S) E.i=1, Update SR.S
* F2n6_Cgjj  CMPHS		Imm9u, Rn
* F2n7_Cgjj  CMPHS		Imm9n, Rn

* FE F2n6_Cgjj  CMPHS	Imm33s, Rn		//(Jumbo)

Unsigned (Rn GE DLR)

Compare if destination is greater than or equal to the source using an unsigned comparison.

This updates SR.T based on the result of the comparison.
For the 2R form, if E.i is Set, SR.S is updated and SR.T is unchanged.
For the 2RI form, if W.m is Set, SR.S is updated and SR.T is unchanged.
The values of the other bits are unchanged by this operation.

This instruction is only allowed in Lane 1.


=== CMPPEQ ===

* F0nm_3g68  CMPPEQ		Rm, Rn				//(ALUPTR) Compare Pointer
* F0nm_3G68  CMPPEQX	Xm, Xn				//(ALUXPTR) Compare 128b Pointer

Compare if source and destination pointers are Equal.
This instruction compares the low 48 bits for equality.

This is only valid if the ALUPTR extension is enabled (and CMPPEQX if ALUX is also enabled).

This instruction is only allowed in Lane 1.


=== CMPQEQ ===

* 5Fnm       CMPQEQ		Rm, Rn
* F0nm_1GC9  CMPQEQ		Rm, Rn			//E.i=0
* F0nm_1GC9  CMPQEQS	Rm, Rn			//(PRED_S) E.i=1, Update SR.S
* F2nC_CGjj  CMPQEQ		Imm9u, Rn
* F2nD_CGjj  CMPQEQ		Imm9n, Rn

* FE F2nC_CGjj  CMPQEQ	Imm33s, Rn		//(Jumbo)

Compare if source and destination are Equal.

This provides both zero and one extended immediates, allowing a direct immediate to express values ranging between -16 and 15.


This updates SR.T based on the result of the comparison.
For the 2R form, if E.i is Set, SR.S is updated and SR.T is unchanged.
For the 2RI form, if W.m is Set, SR.S is updated and SR.T is unchanged.
The values of the other bits are unchanged by this operation.

This instruction is only allowed in Lane 1.


=== CMPQGE ===

* F0nm_1GEC  CMPQGE		Rm, Rn			//E.i=0
* F0nm_1GEC  CMPQGES	Rm, Rn			//(PRED_S) E.i=1, Update SR.S
* F2nA_CGjj  CMPQGE		Imm9u, Rn
* F2nB_CGjj  CMPQGE		Imm9n, Rn

* FE F2nA_CGjj  CMPQGE	Imm33s, Rn		//(Jumbo)

Compare if Rn is Greater or Equal to the immediate.

Only exists for immediates because this can be easily emulated for the two-register forms via swapping the registers and inverting the branch condition.

This updates SR.T based on the result of the comparison.
For the 2R form, if E.i is Set, SR.S is updated and SR.T is unchanged.
For the 2RI form, if W.m is Set, SR.S is updated and SR.T is unchanged.
The values of the other bits are unchanged by this operation.

This instruction is only allowed in Lane 1.


=== CMPPGT ===

* F0nm_3g78  CMPPGT		Rm, Rn				//(ALUPTR) Compare Pointer
* F0nm_3G78  CMPPGTX	Xm, Xn				//(ALUXPTR) Compare 128b Pointer

Compare if source and destination pointers are Greater Than.
This instruction compares the low 48 bits, behaving as-if the result had been zero-extended.

This is only valid if the ALUPTR extension is enabled (and CMPPGTX if ALUX is also enabled).

This instruction is only allowed in Lane 1.


=== CMPQGT ===

* 5Enm       CMPQGT		Rm, Rn
* F0nm_1GE9  CMPQGT		Rm, Rn			//E.i=0
* F0nm_1GE9  CMPQGTS	Rm, Rn			//(PRED_S) E.i=1, Update SR.S
* F2nE_CGjj  CMPQGT		Imm9u, Rn
* F2nF_CGjj  CMPQGT		Imm9n, Rn

* FE F2nE_CGjj  CMPQGT	Imm33s, Rn		//(Jumbo)

Signed Rn GT Rm.

Compare if destination is greater than the source using a signed comparison.

This updates SR.T based on the result of the comparison.
For the 2R form, if E.i is Set, SR.S is updated and SR.T is unchanged.
For the 2RI form, if W.m is Set, SR.S is updated and SR.T is unchanged.
The values of the other bits are unchanged by this operation.

This instruction is only allowed in Lane 1.


=== CMPQHI ===

* 5Dnm       CMPQHI		Rm, Rn
* F0nm_1GD9  CMPQHI		Rm, Rn			//E.i=0
* F0nm_1GD9  CMPQHIS	Rm, Rn			//(PRED_S) E.i=1, Update SR.S
* F2n8_CGjj  CMPQHI		Imm9u, Rn
* F2n8_CGjj  CMPQHI		Imm9n, Rn

* FE F2n8_CGjj  CMPQHI	Imm33s, Rn		//(Jumbo)

Unsigned Rn GT Rm.

Compare if destination is greater than the source using an unsigned comparison.

This updates SR.T based on the result of the comparison.
For the 2R form, if E.i is Set, SR.S is updated and SR.T is unchanged.
For the 2RI form, if W.m is Set, SR.S is updated and SR.T is unchanged.
The values of the other bits are unchanged by this operation.

This instruction is only allowed in Lane 1.


=== CMPQHS ===

* F0nm_1GFC  CMPQHS		Rm, Rn			//E.i=0
* F0nm_1GFC  CMPQHSS	Rm, Rn			//(PRED_S) E.i=1, Update SR.S
* F2n6_CGjj  CMPQHS		Imm9u, Rn
* F2n7_CGjj  CMPQHS		Imm9n, Rn

* FE F2n6_CGjj  CMPQHS	Imm33s, Rn		//(Jumbo)

Unsigned (Rn GE DLR)

Compare if destination is greater than or equal to the source using an unsigned comparison.

This updates SR.T based on the result of the comparison.
For the 2R form, if E.i is Set, SR.S is updated and SR.T is unchanged.
For the 2RI form, if W.m is Set, SR.S is updated and SR.T is unchanged.
The values of the other bits are unchanged by this operation.

This instruction is only allowed in Lane 1.


=== CMPTAxx (TTAG) ===

* F0nm_1gC8  CMPTAEQ	Rm, Rn			//(TTAG) Zx(Rn[59:48]) EQ Rm
* F0nm_1gD8  CMPTAHI	Rm, Rn			//(TTAG) Zx(Rn[59:48]) HI Rm
* F0nm_1gE8  CMPTAHS	Rm, Rn			//(TTAG) Zx(Rn[59:48]) HS Rm

Compare the Tag Attribute Bits for the value in Rn against the reference value in Rm.

The Tag Attribute will be compared as-if it had been zero-extended to 32 bits.


=== CMPTTEQ (TTAG) ===

* F0nm_1GB9  CMPTTEQ	RmImm6u, Rn		//(TTAG)

Compare the Tag-Bits in the immediate with those in Rn:
* 0zzzz0: Rn(63:60)==Imm(4:1)
* 0zzz01: Rn(63:61)==Imm(4:2)
* 0zz011: Rn(63:62)==Imm(4:3)
* 0z0111: Rn(63   )==Imm(4  )
* 1zzzz0: Rn( 3: 0)==Imm(4:1)
* 1zzz01: Rn( 2: 0)==Imm(4:2)
* 1zz011: Rn( 1: 0)==Imm(4:3)
* 1z0111: Rn( 0   )==Imm(4  )


=== CONVFXI / CONVFLI (TTAG) ===

* F0nm_1gF8  CONVFXI	Rm, Rn				//Convert Int64 -> Fixint
* F0nm_1GF8  CONVFLI	Rm, Rn				//Convert Binary64 -> Flonum

The CONVFXI instruction sets the high bits to 01 and copies the low order bits from Rm(61:0).
The CONVFLI instruction sets the high bits to 10 and copies the low order bits from Rm(63:2).


=== CPUID ===

* 36jA       CPUID	Imm4
* F06A_3en0  CPUID	Imm5

Load CPUID bits into DLR.
The immediate gives an index into a table of CPUID bits.

Index 0:
* (63:0): Gives an identifier for the processor.
** 'BJX2xxyy', Identifies core as BJX2.
** 'xx' gives the Baseline ISA Profile and Profile Version.
** 'yy' May give an additional version, or '  ' if absent.

ISA Profiles:
* 'A': Full Profile, 48-bit Address Space, Has WEX3W and Jumbo
* 'B': Fix32 Profile, Has FPU, Optional MMU
* 'C': Basic, 32-bit Address Space.
* 'D': Fix32 Profile, No FPU, Optional MMU
* 'E': Lite Profile, No FPU, Optional MMU, 32-bit Address Space
* 'F': Lite Profile, Has FPU, Optional MMU, 32-bit Address Space
* 'G': Full Profile, 96-bit Address Space, Has WEX3W and Jumbo

This instruction is only allowed in Lane 1.

Index 1:
* (7:0): Logical Core ID
** Gives a logical ID number for the current CPU core.
* (  8): WEX
* (  9): WEX-3W
* ( 10): Jumbo
* ( 11): MMU
* ( 12): Addr48
* ( 13): GSV (64-bit, Packed Integer Ops)
* ( 14): PMORT.x
* ( 15): FPU (GFP)
* ( 16): GSVX (128-Bit FP-SIMD)
* ( 17): GFPX (128-Bit Long-Double Ops)
* ( 18): FMAC (FPU Multiply-Accumulate)
* ( 19): ALUX (128-bit ALU Ops)
* ( 20): XGPR (Set if R32..R63 exist)
* ( 21): PRED_S (Allow SR.S Predicates)
* ( 22): FPU Lane 2 (Allow FPU ops in Lane 2)
* ( 23): Conv FP16 (Packed FP8 and FP16 conversion Ops)
* ( 24): RISC-V Mode
* ( 25): TTAG (Type Tag)
* ( 26): XMOV Addressing
* ( 27): XMOV Quadrant Add Mode
* ( 28): DMAC (Integer Multiply-Accumulate)
* ( 29): Op64 RiMOV
* ( 30): MULQ (64-bit MUL and DIV/MOD instructions)
* ( 31): FDIV (Adds FDIV and FSQRT)

* (60:56): TLB Row Count, Log2
** 6=64
** 8=256
** 10=1024
* (63:61): TLB Set Associativity, Log2
** 0..7=1/2/4/8/16/32/64/128

Index 28:
* 1MHz Clock Counter
* Clock count that increments at 1MHz.

Index 30:
* Clock Cycle Counter
* Gives the total clock-cycle count for the current core.

Index 31:
* Hardware Random Number
* Gives a random number generated internally by the CPU.
** Note that this RNG is fully dynamic rather than sequential.
** This RNG is meant to approximate a non-deterministic entropy source.



The TLB Size and Associativity will be valid if using a software managed TLB with a model that can be deterministically modeled.
* TLB rows will be modulo addressed.
* Entries within a set will leave in the same order they were added.
** Adding a new entry will evict the oldest entry from the set.
* The TLB size will be relative to the active base page size.

So, for example, a 256 row, 4-way TLB, with a 16-page size:
* Will have 1024 entries in total.
* Row will be: Index=(Addr>>14)&255;
* Which will map to a set of 4 entries.

If the TLB Row Count is given as 0, the TLB size or algorithm will be unknown.


=== CSELT ===

* F0nm_5go0  CSELT		Rm, Ro, Rn

Select between Rm or Ro based on the value of SR.T, storing the result in Rn.
If SR.T is set, the value from Rm is used, otherwise the value from Ro is used.


=== DIV0 (DIV, Drop) ===

* 30D0      / DIV0
* F000_30D0 / DIV0

Initializes state for unsigned division.

 SR.S <= 0
 SR.T <= 0


=== DIV1 (DIV, Drop) ===

* 30E0      / DIV1
* F000_30E0 / DIV1

Perform a division step, with SR.S and SR.T holding internal state.

 q0 = SR.S;
 t0 = SR.T;
 q1 = DLR[31];
 dn1 = (DLR SHL 1) | t0;
 if (!q0)
   dn2 = dn1 - DHR;
 else
   dn2 = dn1 + DHR;
 c0 = dn2[32];
 q2 = q1 ^ c0;
 t2 = ! q2;
 DLR = dn2;
 SR.S = q2;
 SR.T = t2;
 
 
=== DIVx.Q (3R) ===

* F0nm_6go4 DIVS.Q	Rm, Ro, Rn	//(MULQ) 64-bit Signed Divide
* F0nm_6Go4 DIVU.Q	Rm, Ro, Rn	//(MULQ) 64-bit Unsigned Divide
* F0nm_7go4 DIVS.L	Rm, Ro, Rn	//(MULQ) 32-bit Signed Divide
* F0nm_7Go4 DIVU.L	Rm, Ro, Rn	//(MULQ) 32-bit Unsigned Divide

Perform an integer division.
Divides Rm by Ro and stores the result in Rn.

The Q form performs a 64-bit division, whereas the L form performs a 32-bit division.

Values for 32-bit divide will be required to be in-range, the result of a division with out-of-range values is undefined.

This instruction will be part of the MULQ extension.

This operation only exists in Lane 1.


=== EXTS.B ===

* 32n6       EXTS.B	Rn
* F0nm_1g8C  EXTS.B	Rm, Rn				//Q=0, I=0

Sign extend the value in the low 8 bits of the register to the width of the register.


=== EXTS.L ===

* 36n5       EXTS.L	Rn
* F0nm_1g5C  EXTS.L	Rm, Rn				//Q=0, I=0

Sign extend the value in the low 32 bits of the register to the width of the register.


=== EXTS.W ===

* 32n7       EXTS.W	Rn
* F0nm_1g9C  EXTS.W	Rm, Rn				//Q=0, I=0

Sign extend the value in the low 16 bits of the register to the width of the register.


=== EXTU.B ===

* 32n4       EXTU.B	Rn
* F0nm_1G8C  EXTU.B	Rm, Rn				//Q=1, I=0

Zero extend the value in the low 8 bits of the register to the width of the register.


=== EXTU.L ===

* 36n4       EXTU.L	Rn
* F0nm_1G5C  EXTU.L	Rm, Rn				//Q=1, I=0

Zero extend the value in the low 32 bits of the register to the width of the register.


=== EXTU.W ===

* 32n5       EXTU.W	Rn
* F0nm_1G9C  EXTU.W	Rm, Rn				//Q=1, I=0

Zero extend the value in the low 16 bits of the register to the width of the register.


=== INVxx ===

* 30F2       INVTLB				//Flush the TLB
* 31nC       INVIC	Rn			//Flush L1 I-Cache for Address
* 31nD       INVDC	Rn			//Flush L1 D-Cache for Address
* F002_30F0  INVTLB
* F01C_3en0  INVIC	Rn
* F01D_3en0  INVDC	Rn

Flush or Invalidate parts of the cache.

The INVIC instruction effects the Instruction Cache, whereas INVDC effects the Data Cache.

Invalidating the instruction cache causes any subsequent execution of instructions within the given cache line to be reloaded from memory. This instruction only effects the L1 and its associated state.

The INVIC instruction, as such, is primarily intended for self-modifying code, or to prepare for execution when new code has been loaded into memory (potentially over the top of existing code).


The INVDC instruction will mark cache lines as "flushed" within the L1 data-cache. On subsequent access, they will be written back to L2 if internally marked as dirty, otherwise the corresponding cache lines will be reloaded from the L2 cache.

Doing a proper flush of an address, as such, will involve a process:
  Invalidate the address (to mark it as flushed);
  Load from the address (causing it to be stored back to L2);
  Invalidate the address again (to discard anything just loaded).


If INVIC or INVDC is given an address in the MMIO range, additional special behaviors may be triggered. Using these with FFFFFFFF will cause the whole L1 cache to be flushed.

Passing -2 to INVDC will trigger an L2 Flush. Note that getting something written back from L2 will require first flushing it from the L1.


The INVTLB instruction will flush the TLB. This would be done as part of changing the active page tables.

=== JCMPxx ===

* F1zz_Czzz
** F1nm_Cpdd ? JTSTT	Rm, Rn, Disp8s	//(JCMP)
** F1nm_CPdd ? JTSTQT	Rm, Rn, Disp8s	//(JCMP)
** F1nm_Cqdd ? JTSTF	Rm, Rn, Disp8s	//(JCMP)
** F1nm_CQdd ? JTSTQF	Rm, Rn, Disp8s	//(JCMP)
* F1zz_Dzzz
** F1nm_Dpdd ? JCMPGT	Rm, Rn, Disp8s	//(JCMP)
** F1nm_DPdd ? JCMPQGT	Rm, Rn, Disp8s	//(JCMP)
** F1nm_Dqdd ? JCMPLE	Rm, Rn, Disp8s	//(JCMP)
** F1nm_DQdd ? JCMPQLE	Rm, Rn, Disp8s	//(JCMP)
* F1zz_Ezzz
** F1nm_Epdd ? JCMPHI	Rm, Rn, Disp8s	//(JCMP)
** F1nm_EPdd ? JCMPQHI	Rm, Rn, Disp8s	//(JCMP)
** F1nm_Eqdd ? JCMPLS	Rm, Rn, Disp8s	//(JCMP)
** F1nm_EQdd ? JCMPQLS	Rm, Rn, Disp8s	//(JCMP)
* F1zz_Fzzz
** F1nm_Fpdd ? JCMPEQ	Rm, Rn, Disp8s	//(JCMP)
** F1nm_FPdd ? JCMPQEQ	Rm, Rn, Disp8s	//(JCMP)
** F1nm_Fqdd ? JCMPNE	Rm, Rn, Disp8s	//(JCMP)
** F1nm_FQdd ? JCMPQNE	Rm, Rn, Disp8s	//(JCMP)

Compare (Rn op Rm) and branch based on the result.
This operation will only be present if JCMP is present.

Note that LT and GE comparisons may be created by switching the arguments.
Alternately 

TSTT will perform a logical AND, and Branch if the result is Zero.
TSTF will perform a logical AND, and Branch if the result is Not Zero.

These will also be exposed under alternate names:
* BREQ / BREQ.L / BREQ.Q
* BRNE / BRNE.L / BRNE.Q
* BRGT / BRGT.L / BRGT.Q
* BRLE / BRLE.L / BRLE.Q
* BRLT / BRLT.L / BRLT.Q
* BRGE / BRGE.L / BRGE.Q


=== JCMPZxx ===

* F2n6_Dfjj /? JCMPZ{Q}xx		Rn, Disp8s	//(JCMP)
* F2n7_Dfjj /? JCMPZ{Q}xx		Rn, Disp8s	//(JCMP)

* F2n6_Dfjj /? BRxx.{L/Q}		Rn, Disp8s	//(JCMP)
* F2n7_Dfjj /? BRxx.{L/Q}		Rn, Disp8s	//(JCMP)


Compare (Rn op 0) and branch based on the result.

The (Rm(0),Em,Ei) bits will encode the operator:
* 000: EQ (Rn==0)
* 001: NE (Rn!=0)
* 010: LE (Rn<=0), Signed
* 011: GT (Rn> 0), Signed
* 100: LT (Rn< 0), Signed
* 101: GE (Rn>=0), Signed
* 110: -
* 111: -

This operation will only be present if JCMP is present.


* F2n4_Efjj ? BREQ.{L/Q}	Rn, Disp10u			//(JCMPZ) JCmp (Rn EQ 0)
* F2n5_Efjj ? BREQ.{L/Q}	Rn, Disp10n			//(JCMPZ) JCmp (Rn EQ 0)
* F2n6_Efjj ? BRNE.{L/Q}	Rn, Disp10u			//(JCMPZ) JCmp (Rn NE 0)
* F2n7_Efjj ? BRNE.{L/Q}	Rn, Disp10n			//(JCMPZ) JCmp (Rn NE 0)

* F2n8_Efjj ? BRLT.{L/Q}	Rn, Disp10u			//(JCMPZ) JCmp (Rn LT 0)
* F2n9_Efjj ? BRLT.{L/Q}	Rn, Disp10n			//(JCMPZ) JCmp (Rn LT 0)
* F2nA_Efjj ? BRGE.{L/Q}	Rn, Disp10u			//(JCMPZ) JCmp (Rn GE 0)
* F2nB_Efjj ? BRGE.{L/Q}	Rn, Disp10n			//(JCMPZ) JCmp (Rn GE 0)

* F2nC_Efjj ? BRLE.{L/Q}	Rn, Disp10u			//(JCMPZ) JCmp (Rn LE 0)
* F2nD_Efjj ? BRLE.{L/Q}	Rn, Disp10n			//(JCMPZ) JCmp (Rn LE 0)
* F2nE_Efjj ? BRGT.{L/Q}	Rn, Disp10u			//(JCMPZ) JCmp (Rn GT 0)
* F2nF_Efjj ? BRGT.{L/Q}	Rn, Disp10n			//(JCMPZ) JCmp (Rn GT 0)

These cases allow for a slightly larger displacement.


=== LDACL ===

* 3042       LDACL
* F002_3040  LDACL

Load DLR into the ACL Cache.

This entry associates a set of access modes with a pair of VUGID keys.
* (15: 0): TLB VUGID	//VUGID from the TLB (ACLID).
* (31:16): KRR VUGID	//VUGID from the KRR.
* (43:32): Access Mode	//Same format as in TLBE.
* (63:44): Reserved

Note that unlike with normal TLBE VUGID matching, the ACL entries will require an exact match for both the TLBE and KRR VUGIDs. In this case, only the User and Mode flags in the Access Mode will be used.

Modes:
* 000: Ignore
* 001: Use 'User' flags if an exact match is found.
* 010: Reserved
* ...
* 111: Reserved

When ACL Checking is used in the TLBE, then the contents of the ACL cache will be matched with the keys in the KRR, and if a match is found, the flags from the matching entry will be used. Failure to find a valid match will result in an ACL Check exception.


=== LDTLB ===

* 30F0       LDTLB
* 3052       LDXTLB			//(XMOV)
* F000_30F0  LDTLB
* F002_3050  LDXTLB			//(XMOV)

Copy DHR:DLR into the TLB.

This instruction form is intended for use in a TLB Miss handler.

This instruction is invalid outside of an ISR. Its behavior is undefined, but should behave like a BREAK.

LDXTLB will be an XMOV variant of LDTLB.
The registers R7:R6:R5:R4 will be loaded as a 256-bit Extended TLB (or XTLB).



=== LDEKRR / LDEKEY / LDEENC / SVEKRR ===

* F002_30A0  SVEKRR		//Save Encoded Keyring
* F002_30C0  LDEKRR		//Load Encoded Keyring
* F002_30D0  LDEKEY		//Load Encoded Key
* F002_30E0  LDEENC		//Encode Key

Load Encoded Keyring Register, or the Key for decoding the Keyring.
Support for these instructions will be Optional, dependent on their ability to be implemented securely.

In both LDEKRR and LDEKEY, DHR:DLR will contain the value to be loaded.

The LDEKRR instruction will decode and load a keyring value into KRR.
This instruction will also check the integrity of the decoded key, raising a fault if the provided key is invalid.

The LDEKEY instruction will load a seed value representing the encryption key into an internal register (which will otherwise be invisible). The key will be a random number to be generated by a sufficiently strong random number generator, with its initial state being undefined.

Loading a key value with the low order bits set to 0 will effectively disable the LDEKRR instruction. The use of LDEKRR will be disabled until LDEKEY is used.


The LDEENC will encode a keyring value from DLR and store the result in DHR:DLR. The value may then be loaded via LDEKRR.

The SVEKRR instruction will encode the current KRR and store it into DHR:DLR.


Of these, only LDEKRR and SVEKRR are to be allowed in user-mode. These instructions are to allow controlled transition between keyrings, and to allow saving and restoring keyrings without exposing their contents to the application, or to allow the application to forge its own keyrings.


Both LDEKEY and LDEENC will be supervisor only.

These instructions may not be used in a WEX sequence.
The specific encoding used will be depend on the implementation.
A key encoded on one machine will not necessarily be valid on a different machine.

Encoded keys should not be placed in memory which is readable from user-mode, as this increases the likelihood of code being able to break the encoding and forge its own keys.


=== LEA.x ===

* 4Cnm       LEA.B		(Rm, DLR), Rn
* 4Dnm       LEA.W		(Rm, DLR), Rn
* 4Enm       LEA.L		(Rm, DLR), Rn
* 4Fnm       LEA.Q		(Rm, DLR), Rn

* F0nm_0Go0  LEA.B		(Rm, Disp5u), Rn
* F0nm_0Go1  LEA.W		(Rm, Disp5u), Rn
* F0nm_0Go2  LEA.L		(Rm, Disp5u), Rn
* F0nm_0Go3  LEA.Q		(Rm, Disp5u), Rn

* F0nm_0Go4  LEA.B		(Rm, Ro), Rn
* F0nm_0Go5  LEA.W		(Rm, Ro), Rn
* F0nm_0Go6  LEA.L		(Rm, Ro), Rn
* F0nm_0Go7  LEA.Q		(Rm, Ro), Rn

* F1nm_0Gdd  LEA.B		(Rm, Disp9u), Rn
* F1nm_1Gdd  LEA.W		(Rm, Disp9u), Rn
* F1nm_2Gdd  LEA.L		(Rm, Disp9u), Rn
* F1nm_3Gdd  LEA.Q		(Rm, Disp9u), Rn

* F8An_dddd ? LEA.Q		(GBR, Disp16u), Rn	//(?)
* F8Bn_dddd ? LEA.Q		(GBR, Disp16u), Rk	//(?)

* FE F1nm_0Gdd  LEA.B	(Rm, Disp33s), Rn
* FE F1nm_1Gdd  LEA.W	(Rm, Disp33s), Rn
* FE F1nm_2Gdd  LEA.L	(Rm, Disp33s), Rn
* FE F1nm_3Gdd  LEA.Q	(Rm, Disp33s), Rn


Load the effective address of the Base register added to the displacement and store the result into the destination register.

This will use a scale factor equivalent to the size element type.

In XG2 Mode, the WI flag will cause Disp9u to be one-extended (and thus negative).


Note that the effective precision of LEA will be relative to the size of the address space. In a 32-bit address space, it will only update the low 32 bits, and in a 48-bit address space will adjust the low 48 bits.

The basic form of the LEA.x instruction will zero the tag bits.
In a 32 bit address mode, the high 32 bits of the result will be zeroed.


The LEA.Q instruction will allow composing an address relative to GBR, adressing a region of up to 512K. This is intended mostly for global arrays.


=== LEAT.x ===

* F0nm_4Go0  LEAT.B		(Rm, Disp5), Rn		//(TTAG)
* F0nm_4Go1  LEAT.W		(Rm, Disp5), Rn		//(TTAG)
* F0nm_4Go2  LEAT.L		(Rm, Disp5), Rn		//(TTAG)
* F0nm_4Go3  LEAT.Q		(Rm, Disp5), Rn		//(TTAG)
* F0nm_4Go4  LEAT.B		(Rm, Ro), Rn		//(TTAG)
* F0nm_4Go5  LEAT.W		(Rm, Ro), Rn		//(TTAG)
* F0nm_4Go6  LEAT.L		(Rm, Ro), Rn		//(TTAG)
* F0nm_4Go7  LEAT.Q		(Rm, Ro), Rn		//(TTAG)

* FE F0nm_4Go0  LEAT.B	(Rm, Disp29s), Rn	//(TTAG)
* FE F0nm_4Go1  LEAT.W	(Rm, Disp29s), Rn	//(TTAG)
* FE F0nm_4Go2  LEAT.L	(Rm, Disp29s), Rn	//(TTAG)
* FE F0nm_4Go3  LEAT.Q	(Rm, Disp29s), Rn	//(TTAG)

The LEAT.x instruction will have similar behavior to a normal LEA.

The primary difference, howerver, will be that LEAT will also adjust the tag bits for bounds-checked pointers.

If given a pointer with an unhandled tag type, or if the pointer goes outside the representable range for the bounds, then the tag bits will be zeroed.

If the high 4 bits hold 0xF, the tag will be passed through as-is.


These instructions will be part of the TTAG extension.


As a special case, LEAT.x encoding PC as the base register will generate an address tagged in the same way as the Link Register (LR). This tagging will reflect the current operating mode.

This instruction will, however, Zero the U bits, S/T bits, and WXE bit.
This will be done so that reloading the same function pointer in different contexts (albeit within the same operating mode) will give the same pointer value.


=== LDIN / LDIZ ===

Targeting R0/DLR:
* Ajjj       LDIZ	Imm12u, DLR
* Bjjj       LDIN	Imm12n, DLR
* FAjj_jjjj	 LDIZ	Imm24u, DLR		//Zero Extend
* FBjj_jjjj	 LDIN	Imm24n, DLR		//One Extend

Targeting GPRs:
* Dnii       LDI		Imm8s, Rn		//Rn=Imm8s
* F2n0_Chjj  LDIZ		Imm10u, Rn		//(WEX2), Rn=Imm10u
* F2n1_Chjj  LDIN		Imm10n, Rn		//(WEX2), Rn=Imm10n
* F80n_iiii  LDIZ		Imm16u, Rn
* F81n_iiii  LDIZ		Imm16u, Rk
* F82n_iiii  LDIN		Imm16n, Rn
* F83n_iiii  LDIN		Imm16n, Rk

* FE-FE F80n_iiii  LDI	Imm64, Rn		//R0 ..R15
* FE-FE F81n_iiii  LDI	Imm64, Rk		//R16..R31
* FE-FE F82n_iiii  LDI	Imm64, Rk		//R32..R47
* FE-FE F83n_iiii  LDI	Imm64, Rk		//R48..R63

Load an 8, 12, or 24 bit value into DLR.
Load an 8, 10, 16, or 32 bit value into a GPR.

LDIZ will zero extend the value up to the size of the register.

LDIN will extend the value with ones up to the size of the register.


=== LDIMIZ/LDIMIN ===

* F2n0_CHjj  LDIMIZ	Imm10u, Rn
* F2n1_CHjj  LDIMIN	Imm10n, Rn

These will load an immediate into a register, but with the value shifted left by 16 bits.

LDIMIZ will load a shifted zero-extended immediate.
LDIMIN will load a shifted one-extended immediate.


=== LDISH ===

* 26jj       LDISH	Imm8u, DLR
* F2n2_Chjj  LDISH	Imm8u, Rn

Load Immediate with Shift.

The value in DLR is shifted left 8 bits, and the immediate value is coppied into the low bits.

The 32-bit encoding is mostly intended for use with WEX2. The high 2 bits of the imm10 field are reserved and must be zero.


=== LDIHI{Q} ===

* F2n3_Chjj  LDIHI	Imm10u, Rn
* F2n3_CHjj  LDIHIQ	Imm10u, Rn

Load High Immediate.

LDIHI will load the immediate into bits (31:22), with the result zero extended.
LDIHIQ will load the immediate into bits (63:54).

If used with a Jumbo Prefix, LDIHI will load a 32 bit immediate into (47:16) and LDIHIQ into (63:32).



=== LDISH16 ===

* F86n_iiii  LDISH16	Imm16u, Rn
* F87n_iiii  LDISH16	Imm16u, Rk

Load Immediate with Shift.

The value in DLR is shifted left 16 bits, and the low bits are set according to the pattern value given.


=== LDIROz{Q} (Possible) ===

* F2n4_Chjj  LDIROZ		Imm10u, Rn
* F2n4_CHjj  LDIROZQ	Imm10u, Rn
* F2n5_Chjj  LDIRON		Imm10n, Rn
* F2n5_CHjj  LDIRONQ	Imm10n, Rn

Load Rotated Immediate.

The high-order bits of the immediate will be rotated-left by a shift given in the low 4 bits.

If E.Q is Clear, then a 32-bit rotate will be used and the value will be shifted by a multiple of 2 bits.

If E.Q is Set, then a 64-bit rotate will be used and the value will be shifted by a multiple of 4 bits.

In a Jumbo64 form, the 33-bit immediate will be interpreted as a 25 bit immediate with an 8 bit unscaled shift. Behavior will otherwise be similar to a normal rotate instruction.

This operation may be limited to Lane 1.


=== LDOP/x (RiMOV+LdOp) ===

* FFw0_Uvii-F0nm_0giZ  LDOP/x.x	Rn, (Rm, Disp17s)			//(RiMOV+LdOp)
* FFw0_Uvii-F0nm_0goZ  LDOP/x.x	Rn, (Rm, Ro, Disp11u)		//(RiMOV+LdOp)
* FFw0_UVii-F0nm_0goZ  LDOP/x.x	Rn, (Rm, Ro*Sc, Disp9u)		//(RiMOV+LdOp)
* FFw0_Uvii-F0nm_0giZ  LDOP/x.x	(Rm, Disp17s), Rn			//(RiMOV+LdOp)
* FFw0_Uvii-F0nm_0goZ  LDOP/x.x	(Rm, Ro, Disp11u), Rn		//(RiMOV+LdOp)
* FFw0_UVii-F0nm_0goZ  LDOP/x.x	(Rm, Ro*Sc, Disp9u), Rn		//(RiMOV+LdOp)

* FFw0_Uvii-F0nm_0giZ  LDOP/x.x	Imm6u, (Rm, Disp17s)		//(RiMOV+LdOp)
* FFw0_Uvii-F0nm_0goZ  LDOP/x.x	Imm6u, (Rm, Ro, Disp11u)	//(RiMOV+LdOp)
* FFw0_UVii-F0nm_0goZ  LDOP/x.x	Imm6u, (Rm, Ro*Sc, Disp9u)	//(RiMOV+LdOp)

LdOp will be an extension of RiMOV which adds the ability to encode LoadOp and StoreOp instructions. These will follow the same basic encoding as RiMOV.

LoadOp will combine a Memory Load with an ALU operation, with the result going into Rn. StoreOp will perform an ALU operation against a value in memory, with the result remaining in memory.

The Z field will follow the pattern:
* 0..3: StoreOp (Rm, Disp)
* 4..7: StoreOp (Rm, Ro, Disp)
* 8..B: LoadOp (Rm, Disp)
* C..F: LoadOp (Rm, Ro, Disp)

If U is not 0, then these will encode an LDOP instruction.
* U (LoadOp/StoreOp):
** 0=MOV, 1=XCHG, 2=ADD, 3=SUB
** 4=RSUB, 5=AND, 6=OR, 7=XOR
* / U (StoreOp Only):
** / 8=MOV(Imm6u), 9=MOV(Imm6n), A=ADD(Imm6u), B=SUB(Imm6u)
** / C=RSUB(Imm6u), D=AND(Imm6u), E=OR(Imm6u), F=XOR(Imm6u)
* (Change) 8..F are equivalent to 0..7, but are Volatile.


Volatile operations will perform a Load or Store, but will indicate the cache should immediately flush the cache line.


=== LDTEX (LDTEX) ===

* F0nm_0GdB				LDTEX	(Rm, Ro), Rn				//(LDTEX)
* FFw0_0vii-F0nm_0GoB	LDTEX	(Rm, Ro, Disp11u), Rn		//(LDTEX+RiMOV)
* FFw0_0Vii-F0nm_0GoB	LDTEX	(Rm, Ro*Sc, Disp9u), Rn		//(LDTEX+RiMOV)

Load from Texture.

The Rm gives the base address and format of the texture.

Unlike normal Load/Store instructions, Rm will require a 64-bit alignment.
* Note that alignment will still be required even with 16 or 32-bit texels.

The Ro field gives the ST coords, as a pair of 16.16 fixed-point values.
The ST coordinate values will be interpreted as a texel coordinate.

The result texel will be unpacked into RGBA64 form (4x Int16).

The Rm field will be interpreted as:
* (47: 0): Texture Base Address
* (51:48): Texture X Size
* (56:52): Texture Combined Size (X+Y)
* (59:57): Block Mode
* (63:60): Pointer Tag

Texture X Size:
* 0: Interpret texture as Morton Order
** This will interleave the S and T bits, using this as the index.
* 1..12: X Stride of 2 .. 4096 pixels.
** Y will be shifted left by this many bits, with X in the low bits.
** Note that the low 2-bits of S and T will be cut off for block textures.

Texture Combined Size:
* Interpreted as the log2 of a bit-mask for the texture index (in texels).
* Note that the mask effectively discards the low 4 bits for block coordinates.
** The complete mask would be applicable for RGBA textures.
** However, LDTEX is not required to mask the low 4 bits of the coordinate.

Block Mode:
* 000: UTX2
* 001: RGBA64 / RGBA64F
* 010: UTX3 (LDR)
* 011: UTX3 (HDR)
* 100: RGB15F / RGB4_E4 (?)
* 101: RGB555A
* 110: RGBA8 / RGBA32
* 111: RGBA32_FP8U

If a displacement is used, it will be interpreted as additional parameters:
* Disp(2:0): Pixel Selector
** 000: Truncare (Default)
** 001: Round Nearest
** 010: Reserved
** 011: Reserved
** 100: Round S and T downward (Truncate)
** 101: Round S upward, T downward
** 110: Round S downward, T upward
** 111: Round S and T upward


For UTX2, See [BLKUTX2 (RGB)]

For RGB555A, See [RGB Pack/Unpack]

RGB555A:
* 0rrrrrgggggbbbbb, Opaque
* 1rrrraggggabbbba, Translucent (3 bit Alpha)


RGB4_E4:
* (15:12): Exponent
* (11: 8): R
* ( 7: 4): G
* ( 3: 0): B
* This is a joint exponent format with denormal components.


=== MACx.L ===

* F0nm_6go0  MACS.L		Rm, Ro, Rn		//(DMAC) Rn=Sx(Rn+(Rm*Ro))
* F0nm_6Go0  MACS.L		Rm, Imm5u, Rn	//(DMAC) Rn=Sx(Rn+(Rm*Imm))
* F0nm_6go1  MACU.L		Rm, Ro, Rn		//(DMAC) Rn=Zx(Rn+(Rm*Ro))
* F0nm_6Go1  MACU.L		Rm, Imm5u, Rn	//(DMAC) Rn=Zx(Rn+(Rm*Imm))
* F0nm_6go2  DMACS.L	Rm, Ro, Rn		//(DMAC) Rn=Rn+(Rm*Ro)
* F0nm_6Go2  DMACS.L	Rm, Imm5u, Rn	//(DMAC) Rn=Rn+(Rm*Imm)
* F0nm_6go3  DMACU.L	Rm, Ro, Rn		//(DMAC) Rn=Rn+(Rm*Ro)
* F0nm_6Go3  DMACU.L	Rm, Imm5u, Rn	//(DMAC) Rn=Rn+(Rm*Imm)

* FEii_iiii-F0nm_6Gi0  MACS.L	Rm, Imm29s, Rn		//(DMAC) Rn=Sx(Ro+(Rm*Imm))
* FEii_iiii-F0nm_6Gi1  MACU.L	Rm, Imm29s, Rn		//(DMAC) Rn=Zx(Ro+(Rm*Imm))
* FEii_iiii-F0nm_6Gi2  DMACS.L	Rm, Imm29s, Rn		//(DMAC) Rn=Ro+(Rm*Imm)
* FEii_iiii-F0nm_6Gi3  DMACU.L	Rm, Imm29s, Rn		//(DMAC) Rn=Ro+(Rm*Imm)

* FFw0_0Vpp-F0nm_6go0  MACS.L	Rm, Ro, Rp, Rn		//(DMAC) Rn=Sx(Rp+(Rm*Ro))
* FFw0_0Vpp-F0nm_6go1  MACU.L	Rm, Ro, Rp, Rn		//(DMAC) Rn=Zx(Rp+(Rm*Ro))
* FFw0_0Vpp-F0nm_6go2  DMACS.L	Rm, Ro, Rp, Rn		//(DMAC) Rn=Rp+(Rm*Ro)
* FFw0_0Vpp-F0nm_6go3  DMACU.L	Rm, Ro, Rp, Rn		//(DMAC) Rn=Rp+(Rm*Ro)

* FFw0_0vii-F0nm_6Go0  MACS.L	Rm, Imm17s, Rn		//(DMAC) Rn=Sx(Rn+(Rm*Imm))
* FFw0_0vii-F0nm_6Go1  MACU.L	Rm, Imm17s, Rn		//(DMAC) Rn=Zx(Rn+(Rm*Imm))
* FFw0_0vii-F0nm_6Go2  DMACS.L	Rm, Imm17s, Rn		//(DMAC) Rn=Rn+(Rm*Imm)
* FFw0_0vii-F0nm_6Go3  DMACU.L	Rm, Imm17s, Rn		//(DMAC) Rn=Rn+(Rm*Imm)

* FFw0_0Vii-F0nm_6Go0  MACS.L	Rm, Imm11s, Ro, Rn	//(DMAC) Rn=Sx(Ro+(Rm*Imm))
* FFw0_0Vii-F0nm_6Go1  MACU.L	Rm, Imm11s, Ro, Rn	//(DMAC) Rn=Zx(Ro+(Rm*Imm))
* FFw0_0Vii-F0nm_6Go2  DMACS.L	Rm, Imm11s, Ro, Rn	//(DMAC) Rn=Ro+(Rm*Imm)
* FFw0_0Vii-F0nm_6Go3  DMACU.L	Rm, Imm11s, Ro, Rn	//(DMAC) Rn=Ro+(Rm*Imm)


Multiply Accumulate.

The MACS and MACU instructions will do a multiply-accumulate with a 32-bit result being sign or zero extended to 64 bits.

The DMACS and DMACU instructions will multiply two 32-bit quantities and accumulate using a 64-bit result.


=== MIN/MAX ===

* F0nm_1go4  MIN		Rm, Ro, Rn			//(MINMAX) Minimum Value
* F0nm_1Go4  MAX		Rm, Ro, Rn			//(MINMAX) Maximum Value
* F0nm_7Go0  FMIN		Rm, Ro, Rn			//(MINMAX) Binary64 Min Value
* F0nm_7Go1  FMAX		Rm, Ro, Rn			//(MINMAX) Binary64 Max Value

Select the Minimum or Maximum value between a pair of registers.

The MIN/MAX instructions will interpret the value as a signed 64-bit value.

The FMIN/FMAX instructions will interpret the value as a Binary64 value.


=== MODx.Q (3R) ===

* F0nm_6go5  MODS.Q	Rm, Ro, Rn		//(MULQ) 64-bit Signed Modulo
* F0nm_6Go5  MODU.Q	Rm, Ro, Rn		//(MULQ) 64-bit Unsigned Modulo
* F0nm_7go5  MODS.L	Rm, Ro, Rn		//(MULQ) 32-bit Signed Modulo
* F0nm_7Go5  MODU.L	Rm, Ro, Rn		//(MULQ) 32-bit Unsigned Modulo

Perform an integer modulo.
Find the modulo of dividing Rm by Ro and stores the result in Rn.

This instruction will be part of the MULQ extension.

This operation only exists in Lane 1.


=== MOV ===

MOV, GPR
* 18nm       MOV		Rm, Rn
* 19zz       MOV		Rj, Rn
* 1Azz       MOV		Rm, Rk
* 1Bzz       MOV		Rj, Rk
* F0nm_1g89  MOV		Rm, Rn				//Rn=Rm
* F0nm_1G89  MOVX		Xm, Xn				//(MOVX2)
* F0nm_1G5C  MOVD		Rm, Rn				//(Pseudo) EXTU.L

MOV, Control Register
* 48nm       MOV		Rm, Cn
* 49nm       MOV		Cm, Rn
* 4Anm       MOV		Rm, Sn
* 4Bnm       MOV		Sm, Rn
* F0nm_1eA9  MOV		Rm, Cn				//Cn=Rm
* F0nm_1eB9  MOV		Cm, Rn				//Rn=Cm
* F0nm_1eAC  MOV		Rm, Sn				//Cn=Rm
* F0nm_1eBC  MOV		Sm, Rn				//Rn=Cm

Move a value from the source to the destination register.

MOVX will move a 128 bit register pair, and will reuse the old MOVD encoding.

The MOVD instruction is now an alias for the EXTU.L instruction.


=== MOV.B ===

* 00nm       MOV.B		Rn, (Rm)
* 04nm       MOV.B		Rn, (Rm, DLR)		//(Rn==0): PC-Rel
* 08nm       MOV.B		(Rm), Rn
* 0Cnm       MOV.B		(Rm, DLR), Rn		//(Rm==0): PC-Rel
* F0nm_0go4  MOV.B		Rn, (Rm, Ro)		//Q=0	//(Rn==0): PC-Rel
* F0nm_0goC  MOV.B		(Rm, Ro), Rn		//Q=0	//(Rm==0): PC-Rel
* F1nm_0gdd  MOV.B		Rn, (Rm, Disp9u)
* F1nm_8gdd  MOV.B		(Rm, Disp9u), Rn

* F0nm_0go4  MOV.B		Rn, (Rm, Ro)		//Q=0	//(Rn==0): PC-Rel
* F0nm_0goC  MOV.B		(Rm, Ro), Rn		//Q=0	//(Rm==0): PC-Rel

* FEdd_dddd-F1nm_0gdd  MOV.B	Rn, (Rm, Disp33s)			//(Jumbo)
* FEdd_dddd-F1nm_8gdd  MOV.B	(Rm, Disp33s), Rn			//(Jumbo)

* FFw0_0vii-F0nm_0gi0  MOV.B	Rn, (Rm, Disp17s)			//(RiMOV)
* FFw0_0vii-F0nm_0go4  MOV.B	Rn, (Rm, Ro, Disp11u)		//(RiMOV)
* FFw0_0Vii-F0nm_0go4  MOV.B	Rn, (Rm, Ro*Sc, Disp9u)		//(RiMOV)
* FFw0_0vii-F0nm_0gi8  MOV.B	(Rm, Disp17s), Rn			//(RiMOV)
* FFw0_0vii-F0nm_0goC  MOV.B	(Rm, Ro, Disp11u), Rn		//(RiMOV)
* FFw0_0Vii-F0nm_0goC  MOV.B	(Rm, Ro*Sc, Disp9u), Rn		//(RiMOV)

Load or store a Byte to/from memory.
The byte is sign extended to the size of the register.

In Baseline Mode, Disp9u will always be zero extended.
In XG2 Mode, using the WI flag will encode a one-extended (negative) displacement.

The behavior of the address for MOV.x will depend on the address space size.
In a 32 bit address mode, the high 32 bits of the address will be ignored (treated as if they were always zero).

In both 32 and 48 bit modes, the high 16 bits for Load/Store operations will be ignoed by default (with a partial exception for an optional extended 60-bit addressing mode).

In targets with a 96-bit virtual address space, GBH will be used for the high order bits of the virtual address when using these instruction forms.


Note that for Load/Store, R0 and R1 will be special as Base and Index registers.

For Displacement Forms:
* (R0, Disp) => (PC, Disp)
* (R1, Disp) => (GBR, Disp)

For Indexed Forms with R0 or R1:
* (R0, R0) => (PC, DLR)
* (R0, R1) => (DLR)
* (R1, R0) => (GBR, DLR)
* (R1, R1) => (TBR, DLR)

For other Indexed forms:
* (R0, Ri) => (PC, Ri)
* (R1, Ri) => (GBR, Ri)
* (Rm, R1) => (Rm, R0)		//Unscaled
* (Rm, R1) => (Rm, R0)		//Unscaled


=== MOV.C ===

* F0nm_4go5  MOV.C		Cn, (Rm, Ro)
* F0nm_4goD  MOV.C		(Rm, Ro), Cn
* F1nm_5gdd  MOV.C		Cn, (Rm, Disp9u)
* F1nm_7gdd  MOV.C		(Rm, Disp9u), Cn

* FEdd_dddd-F1nm_5gdd  MOV.C	Rn, (Rm, Disp33s)			//(Jumbo)
* FEdd_dddd-F1nm_7gdd  MOV.C	(Rm, Disp33s), Rn			//(Jumbo)

* FFw0_0vii-F0nm_4go3  MOV.C	Rn, (Rm, Ro, Disp11u)		//(RiMOV)
* FFw0_0Vii-F0nm_4go3  MOV.C	Rn, (Rm, Ro*Sc, Disp9u)		//(RiMOV)
* FFw0_0vii-F0nm_4goB  MOV.C	(Rm, Ro, Disp11u), Rn		//(RiMOV)
* FFw0_0Vii-F0nm_4goB  MOV.C	(Rm, Ro*Sc, Disp9u), Rn		//(RiMOV)

Load or store a Control Register to/from memory.
The behavior of this operation will be similar to a MOV.Q, just applying to control registers.

See MOV.B for general Load/Store semantics.


=== MOV.L ===

* 02nm       MOV.L		Rn, (Rm)
* 06nm       MOV.L		Rn, (Rm, DLR)		//(Rn==0): PC-Rel
* 0Anm       MOV.L		(Rm), Rn
* 0Enm       MOV.L		(Rm, DLR), Rn		//(Rm==0): PC-Rel
* 40nd       MOV.L		Rn, (SP, Disp4u)	//Stack-Relative Store
* 42nd       MOV.L		Rk, (SP, Disp4u)	//Stack-Relative Store
* 44nd       MOV.L		(SP, Disp4u), Rn	//Stack-Relative Load
* 46nd       MOV.L		(SP, Disp4u), Rk	//Stack-Relative Load
* F0nm_0go6  MOV.L		Rn, (Rm, Ro)		//(Rn==0): PC-Rel
* F0nm_0goE  MOV.L		(Rm, Ro), Rn		//(Rm==0): PC-Rel
* F1nm_2gdd  MOV.L		Rn, (Rm, Disp9u)
* F1nm_Agdd  MOV.L		(Rm, Disp9u), Rn

* F2n8_Dhjj  MOV.L		Rn, (GBR, Disp10u*4)
* F2nA_Dhjj  MOV.L		(GBR, Disp10u*4), Rn

* FEdd_dddd-F1nm_2gdd  MOV.L	Rn, (Rm, Disp33s)			//(Jumbo)
* FEdd_dddd-F1nm_Agdd  MOV.L	(Rm, Disp33s), Rn			//(Jumbo)

* FFw0_0vii-F0nm_0gi2  MOV.L	Rn, (Rm, Disp17s)			//(RiMOV)
* FFw0_0Vii-F0nm_0gi2  MOV.L	Rn, (Rm, Disp17s*1)			//(RiMOV)
* FFw0_0vii-F0nm_0go6  MOV.L	Rn, (Rm, Ro, Disp11u)		//(RiMOV)
* FFw0_0Vii-F0nm_0go6  MOV.L	Rn, (Rm, Ro*Sc, Disp9u)		//(RiMOV)
* FFw0_0vii-F0nm_0giA  MOV.L	(Rm, Disp17s), Rn			//(RiMOV)
* FFw0_0Vii-F0nm_0giA  MOV.L	(Rm, Disp17s*1), Rn			//(RiMOV)
* FFw0_0vii-F0nm_0goE  MOV.L	(Rm, Ro, Disp11u), Rn		//(RiMOV)
* FFw0_0Vii-F0nm_0goE  MOV.L	(Rm, Ro*Sc, Disp9u), Rn		//(RiMOV)

Load or store a DWord to/from memory.

See MOV.B for general Load/Store semantics.


=== MOV.Q ===

* 03nm       MOV.Q		Rn, (Rm)
* 07nm       MOV.Q		Rn, (Rm, DLR)
* 0Bnm       MOV.Q		(Rm), Rn
* 0Fnm       MOV.Q		(Rm, DLR), Rn
* 41nd       MOV.Q		Rn, (SP, Disp4u)
* 43nd       MOV.Q		Rk, (SP, Disp4u)
* 45nd       MOV.Q		(SP, Disp4u), Rn
* 47nd       MOV.Q		(SP, Disp4u), Rk
* F0nm_0go7  MOV.Q		Rn, (Rm, Ro)
* F0nm_0goF  MOV.Q		(Rm, Ro), Rn
* F1nm_3gdd  MOV.Q		Rn, (Rm, Disp9u)
* F1nm_Bgdd  MOV.Q		(Rm, Disp9u), Rn

* F2n9_Dhjj  MOV.Q		Rn, (GBR, Disp10u*8)
* F2nB_Dhjj  MOV.Q		(GBR, Disp10u*8), Rn

* FEdd_dddd-F1nm_3gdd  MOV.Q	Rn, (Rm, Disp33s)			//(Jumbo)
* FEdd_dddd-F1nm_Bgdd  MOV.Q	(Rm, Disp33s), Rn			//(Jumbo)

* FFw0_0vii-F0nm_0gi3  MOV.Q	Rn, (Rm, Disp17s)			//(RiMOV)
* FFw0_0Vii-F0nm_0gi3  MOV.Q	Rn, (Rm, Disp17s*1)			//(RiMOV)
* FFw0_0vii-F0nm_0go7  MOV.Q	Rn, (Rm, Ro, Disp11u)		//(RiMOV)
* FFw0_0Vii-F0nm_0go7  MOV.Q	Rn, (Rm, Ro*Sc, Disp9u)		//(RiMOV)
* FFw0_0vii-F0nm_0giB  MOV.Q	(Rm, Disp17s), Rn			//(RiMOV)
* FFw0_0Vii-F0nm_0giB  MOV.Q	(Rm, Disp17s*1), Rn			//(RiMOV)
* FFw0_0vii-F0nm_0goF  MOV.Q	(Rm, Ro, Disp11u), Rn		//(RiMOV)
* FFw0_0Vii-F0nm_0goF  MOV.Q	(Rm, Ro*Sc, Disp9u), Rn		//(RiMOV)

Load or store a QWord to/from memory.

See MOV.B for general Load/Store semantics.


=== MOV.W ===

* 01nm       MOV.W		Rn, (Rm)
* 05nm       MOV.W		Rn, (Rm, DLR)
* 09nm       MOV.W		(Rm), Rn
* 0Dnm       MOV.W		(Rm, DLR), Rn
* F0nm_0go5  MOV.W		Rn, (Rm, Ro)
* F0nm_0goD  MOV.W		(Rm, Ro), Rn
* F1nm_1gdd  MOV.W		Rn, (Rm, Disp9u)
* F1nm_9gdd  MOV.W		(Rm, Disp9u), Rn

* FEdd_dddd-F1nm_1gdd  MOV.W	Rn, (Rm, Disp33s)			//(Jumbo)
* FEdd_dddd-F1nm_9gdd  MOV.W	(Rm, Disp33s), Rn			//(Jumbo)

* FFw0_0vii-F0nm_0gi1  MOV.W	Rn, (Rm, Disp17s)			//(RiMOV)
* FFw0_0Vii-F0nm_0gi1  MOV.W	Rn, (Rm, Disp17s*1)			//(RiMOV)
* FFw0_0vii-F0nm_0go5  MOV.W	Rn, (Rm, Ro, Disp11u)		//(RiMOV)
* FFw0_0Vii-F0nm_0go5  MOV.W	Rn, (Rm, Ro*Sc, Disp9u)		//(RiMOV)
* FFw0_0vii-F0nm_0gi9  MOV.W	(Rm, Disp17s), Rn			//(RiMOV)
* FFw0_0Vii-F0nm_0gi9  MOV.W	(Rm, Disp17s*1), Rn			//(RiMOV)
* FFw0_0vii-F0nm_0goD  MOV.W	(Rm, Ro, Disp11u), Rn		//(RiMOV)
* FFw0_0Vii-F0nm_0goD  MOV.W	(Rm, Ro*Sc, Disp9u), Rn		//(RiMOV)

Load or store a Word to/from memory.
The Word value is sign extended to the size of the register.

See MOV.B for general Load/Store semantics.


=== MOVU.B ===

* 50nm       MOVU.B	(Rm), Rn
* 52nm       MOVU.B	(Rm, DLR), Rn
* F0nm_0GoC  MOVU.B	(Rm, Ro), Rn
* F1nm_8Gdd  MOVU.B	(Rm, Disp9u), Rn

* FEdd_dddd-F1nm_8Gdd  MOVU.B	(Rm, Disp33s), Rn			//(Jumbo)

* FFw0_0vii-F0nm_0Gi8  MOVU.B	(Rm, Disp17s), Rn			//(RiMOV)
* FFw0_0vii-F0nm_0GoC  MOVU.B	(Rm, Ro, Disp11u), Rn		//(RiMOV)
* FFw0_0Vii-F0nm_0GoC  MOVU.B	(Rm, Ro*Sc, Disp9u), Rn		//(RiMOV)

Load or store a Byte to/from memory.
The byte is zero extended to the size of the register.

See MOV.B for general Load/Store semantics.


=== MOVU.L ===

* 28nd       MOVU.L	(SP, Disp4u), Rn
* 2And       MOVU.L	(SP, Disp4u), Rk
* 80nm       MOVU.L	(Rm), Rn
* 88nm       MOVU.L	(Rm, DLR), Rn
* F0nm_0GoE  MOVU.L	(Rm, Ro), Rn
* F1nm_AGdd  MOVU.L	(Rm, Disp9u), Rn

* F2nA_DHjj  MOVU.L	(GBR, Disp10u*4), Rn

* FEdd_dddd-F1nm_AGdd  MOVU.L	(Rm, Disp33s), Rn			//(Jumbo)

* FFw0_0vii-F0nm_0GiA  MOVU.L	(Rm, Disp17s), Rn			//(RiMOV)
* FFw0_0Vii-F0nm_0GiA  MOVU.L	(Rm, Disp17s*1), Rn			//(RiMOV)
* FFw0_0vii-F0nm_0GoE  MOVU.L	(Rm, Ro, Disp11u), Rn		//(RiMOV)
* FFw0_0Vii-F0nm_0GoE  MOVU.L	(Rm, Ro*Sc, Disp9u), Rn		//(RiMOV)

Load or store a DWord to/from memory.
The byte is zero extended to the size of the register.

See MOV.B for general Load/Store semantics.


=== MOVU.W ===

* 51nm       MOVU.W	(Rm), Rn
* 53nm       MOVU.W	(Rm, DLR), Rn
* F0nm_0GoD  MOVU.W	(Rm, Ro), Rn
* F1nm_9Gdd  MOVU.W	(Rm, Disp9u), Rn

* FEdd_dddd-F1nm_9Gdd  MOVU.W	(Rm, Disp33s), Rn			//(Jumbo)

* FFw0_0vii-F0nm_0Gi9  MOVU.W	(Rm, Disp17s), Rn			//(RiMOV)
* FFw0_0Vii-F0nm_0Gi9  MOVU.W	(Rm, Disp17s*1), Rn			//(RiMOV)
* FFw0_0vii-F0nm_0GoD  MOVU.W	(Rm, Ro, Disp11u), Rn		//(RiMOV)
* FFw0_0Vii-F0nm_0GoD  MOVU.W	(Rm, Ro*Sc, Disp9u), Rn		//(RiMOV)

Load or store a Word to/from memory.
The word is zero extended to the size of the register.

See MOV.B for general Load/Store semantics.


=== MOV.X (MOVX2) ===

* 29nd       MOV.X		(SP, Disp4u), Rx
* 2Bnd       MOV.X		Rx, (SP, Disp4u)
* F0nm_4eo0 ? MOV.X		Xn, (Rm, Disp5)
* F0nm_4eo4  MOV.X		Xn, (Rm, Ro)
* F0nm_4eo8 ? MOV.X		(Rm, Disp5), Xn
* F0nm_4eoC  MOV.X		(Rm, Ro), Xn
* F1nm_5Gdd  MOV.X		Xn, (Rm, Disp9u)
* F1nm_7Gdd  MOV.X		(Rm, Disp9u), Xn

* F2n9_DHjj  MOV.X		Xn, (GBR, Disp10u*8)
* F2nB_DHjj  MOV.X		(GBR, Disp10u*8), Xn

* FEdd_dddd-F1nm_5gdd  MOV.X	Xn, (Rm, Disp33s)			//(Jumbo)
* FEdd_dddd-F1nm_7gdd  MOV.X	(Rm, Disp33s), Xn			//(Jumbo)

* FFw0_0vii-F0nm_4gi0  MOV.X	Xn, (Rm, Disp17s)			//(RiMOV)
* FFw0_0vii-F0nm_4go4  MOV.X	Xn, (Rm, Ro, Disp11u)		//(RiMOV)
* FFw0_0Vii-F0nm_4go4  MOV.X	Xn, (Rm, Ro*Sc, Disp9u)		//(RiMOV)
* FFw0_0vii-F0nm_4gi8  MOV.X	(Rm, Disp17s), Xn			//(RiMOV)
* FFw0_0vii-F0nm_4goC  MOV.X	(Rm, Ro, Disp11u), Xn		//(RiMOV)
* FFw0_0Vii-F0nm_4goC  MOV.X	(Rm, Ro*Sc, Disp9u), Xn		//(RiMOV)

Load or Store a 128-bit Register Pair.
Register pair is Even/Odd, and the target memory address is required to have an 8 byte alignment.

Note that MOV.X will scale the index by 8, rather than by 16. In this sense, it can be thought of more as a pair of QWORD values than as a combined 128-bit value.

As with other MOV.x ops, encodings using PC or GBR as a base will use a byte scale, however, the destination is still required to be aligned.

The results of trying to load or store using a misaligned address are undefined.

See MOV.B for general Load/Store semantics.

Some register pairs will be disallowed with MOV.X, and with 128-bit operations in general: X0, X14, X32, X46.


=== MOV.TW (LDST48) ===

* F0nm_9go0  MOV.TW		Rn, (Rm, Disp5)		//(LDST48) Store TWORD
* F0nm_9go1  MOV.HTW	Rn, (Rm, Disp5)		//(LDST48) Store High TWORD
* F0nm_9go4  MOV.TW		Rn, (Rm, Ro)		//(LDST48) Store TWORD
* F0nm_9go5  MOV.HTW	Rn, (Rm, Ro)		//(LDST48) Store High TWORD
* F0nm_9go8  MOV.TW		(Rm, Disp5), Rn		//(LDST48) Load TWORD
* F0nm_9Go8  MOVU.TW	(Rm, Disp5), Rn		//(LDST48) Load TWORD
* F0nm_9go9  MOV.HTW	(Rm, Disp5), Rn		//(LDST48) Load High TWORD
* F0nm_9goC  MOV.TW		(Rm, Ro), Rn		//(LDST48) Load TWORD
* F0nm_9GoC  MOVU.TW	(Rm, Ro), Rn		//(LDST48) Load TWORD
* F0nm_9goD  MOV.HTW	(Rm, Ro), Rn		//(LDST48) Load High TWORD

Load or Store a TWORD, or Logical 48-bit value.
These operations will have a scale and alignment of 2, so using a TWORD in an array will require a pre-scale by 3 (may be accomplished via a LEA).

There will be MOV.TWV/MOVU.TWV/MOV.HTWV pseudo instructions which will synthesize the pre-scale-by-3.


=== MOVN.L ===

* F0nm_4Go8  MOVN.L	(Rm, Disp5), Rn		//Load DWord, One-Extended
* F0nm_4GoC  MOVN.L	(Rm, Ro), Rn		//Load DWord, One-Extended

Special case instruction, similar to MOVU.L, but differs primarily in that the high-order bits are one-extended.



=== MOVST (TTAG) ===

* F0nm_1gFE  MOVST		Rm, Rn			//MOV, Sign-Extend Tag
* F0nm_1GFE  XMOVST		Xm, Xn			//XMOV, Sign-Extend Tag

Move value from Rm to Rn, but sign extend the tag bits (63:48) from bit 47.
The XMOVST will sign extend both registers in the pair.


=== MOVTT (TTAG) ===

* F0nm_3go1            MOVTT	Rm, Ri, Rn		//(TTAG) Set High 16
* F0nm_3Go1            MOVTT	Rm, Imm5u, Rn	//(TTAG) Set Type-Tag
* FFw0_0Vii_F0nm_3Gi1  MOVTT	Rm, Imm17s, Rn	//(TTAG) Set High 16
* FFw0_iiii_F84n_iiii  XMOVTT	Imm32u, Xn		//(TTAG+XGPR) WMI==11

In the 3R form, this copies the low 48 bits from Rm, and the high 16 from Ri.

In the 3RI Imm5u form, the tag is twiddled:
* zzzz0: Rn(63:60)=Imm(4:1)
* zzz01: Rn(63:61)=Imm(4:2)
* zz011: Rn(63:62)=Imm(4:3)
* z0111: Rn(63   )=Imm(4  )

With the 3RI form, all other bits are copied unchanged from Rm.


XMOVTT will load the 32-bit immediate into the high 16 bits of each target register, with the low half into the low register and the high half into the high register. The remaining 96 bits are kept unchanged.


=== MOVZT (TTAG) ===

* F0nm_1gA8  MOVZT		Rm, Rn			//MOV, Zero Tag
* F0nm_1GA8  XMOVZT		Xm, Xn			//XMOV, Zero Tag

Move value from Rm to Rn, but zero the tag bits (63:48).
The XMOVZT will zero (63:48) in both registers in the pair.


=== MULx (3R) ===

* 5Anm       MULS	Rm, DLR, Rn
* F0nm_1go2  MULS	Rm, Ro, Rn
* F0nm_1go3  MULU	Rm, Ro, Rn
* F2nm_2gjj  MULS	Rm, Imm9u, Rn
* F2nm_2Gjj  MULU	Rm, Imm9u, Rn

Performs a narrow multiply (32 bit).
Only the low 32 bits are used from the input registers, and the resulting value is 32-bit sign or zero extended.

This operation only exists in Lane 1.


=== MULx.Q (3R) ===

* F0nm_1Go2  MULS.Q		Rm, Ro, Rn			//(MULQ) Signed Multiply
* F0nm_1Go3  MULU.Q		Rm, Ro, Rn			//(MULQ) Unsigned Multiply

Perform a 64-bit multiply, either signed or unsigned.

This instruction will be part of the MULQ extension.

This operation only exists in Lane 1.


=== MULHx.x (3R) ===

* F0nm_3go4  MULHS.Q	Rm, Ro, Rn		//(MULQ) MUL, High Results Signed
* F0nm_3go5  MULHU.Q	Rm, Ro, Rn		//(MULQ) MUL, High Results Unsigned

Perform a 64-bit multiply, keeping the high-order results (bits 127:64 of a widening multiply).

This instruction will be part of the MULQ extension.

This operation only exists in Lane 1.


=== MULx.x (3R) ===

* F0nm_5go2  DMULS.L	Rm, Ro, Rn			//(MULL) Sx 32b Mul (32*32->64)
* F0nm_5go3  DMULU.L	Rm, Ro, Rn			//(MULL) Zx 32b Mul (32*32->64)
* F0nm_5goE  MULS.W		Rm, Ro, Rn			//(MULW) Sx 16b Mul (16*16->32)
* F0nm_5GoE  MULS.W		Rm, Imm5u, Rn		//(MULW) Sx 16b Mul (16*16->32)
* F0nm_5goF  MULU.W		Rm, Ro, Rn			//(MULW) Zx 16b Mul (16*16->32)
* F0nm_5GoF  MULU.W		Rm, Imm5u, Rn		//(MULW) Zx 16b Mul (16*16->32)

These produce widening multiplies of a given input size.
The values in Rm and Ro are multiplied, and the result is stored in Rn.

The WORD multiplies will exist in all lanes.


=== MULx.X (2R) ===

* F0nm_1G48  MULU.X		Xm, Xn				//(MULX) Xn=Xn*Xm
* F0nm_1G58  MULHU.X	Xm, Xn				//(MULX) Xn=(Xn*Xm)>>128


Multiply two 128 bit values, producing a 128-bit result.
* MULU.X will produce a result containing the low-order bits.
* MULHU.X will produce a result containing the high-order bits.



=== NEG ===

* 33n1       NEG	Rn
* 33nC       NEG	Rn, DLR
* F0nm_1e1C  NEG	Rm, Rn				//Rn=(~Rn)+1

Rn=(~Rn)+1

Negate the value in Rn.


=== NEGC ===

* 33n2	NEGC	Rn

Rn=(~Rn)+(!SR.T).

Negate value in Rn and subtract the SR.T flag.
The value of SR.T is updated to reflect the borrow status of the bit.

This operation is only valid in Lane 1.


=== NOP ===

* 3000       NOP
* F000_3000  NOP		//Fix32

Does Nothing.


=== NOT ===

* 33n0       NOT	Rn
* F0nm_1e0C  NOT	Rm, Rn				//Rn=~Rn

Rn=~Rn

Perform a bitwise NOT of the value in Rn.


=== NOTS ===

* 3090       NOTS
* F000_3090  NOTS		//Fix32

SR.S=!SR.S

Invert the SR.S flag.

This operation is only valid in Lane 1.


=== NOTT ===

* 3080       NOTT
* F000_3080  NOTT		//Fix32

SR.T=!SR.T

Invert the SR.T flag.

This operation is only valid in Lane 1.


=== OR ===

* 16nm       OR		Rm, Rn
* 5Bnm       OR		Rm, DLR, Rn
* F0nm_1go6  OR		Rm, Ro, Rn
* F0nm_1e69 /? OR		Rm, Rn		//Deprecated
* F2nm_6gjj  OR		Rm, Imm9u, Rn

*    FE F2nm_6gjj  OR		Rm, Imm33s, Rn		//(Jumbo)
* FE-FE F2nm_6gjj  OR		Rm, Imm57s, Rn		//(FPIMM)

Perform a bitwise OR of the source and destination, storing the result in the destination.


=== PMORT.x ===

* F0nm_1g1E ? PMORT.L	Rm, Rn		//(GSV)
* F0nm_1G1E ? PMORT.Q	Rm, Rn		//(GSV)

Packed Morton Shuffle.

Interprets a pair of packed DWORD values as indices and performs a bit shuffle, leading to an index in Morton Order.

The PMORT.Q operation will produce a 64-bit value containing all 32 bits from each index. PMORT.L will produce a 32-bit output which only retains the high 16 bits of each input (essentially the high 32 bits of the 64-bit output).

As a simplified example, if we have two 8 bit values:
* u7,u6,u5,u4,u3,u2,u1,u0 and v7,v6,v5,v4,v3,v2,v1,v0
Then this will produce as output:
* u7,v7,u6,v6,u5,v5,u4,v4,u3,v3,u2,v2,u1,v1,u0,v0


=== PxxxSH.x (3R, NNX, Possible) ===

* FFw0_0Vii_F0nm_2Go5  PADDSHX.X	Xm, Xo, Imm11u, Xn
* FFw0_0Vii_F0nm_2Go6  PSUBSHX.X	Xm, Xo, Imm11u, Xn
* FFw0_0Vii_F0nm_2Go7  PMULSHX.X	Xm, Xo, Imm11u, Xn

* FFw0_0Vii_F0nm_2goD  PADDSH.H		Rm, Ro, Imm11u, Rn
* FFw0_0Vii_F0nm_2goE  PADDSH.H		Rm, Ro, Imm11u, Rn
* FFw0_0Vii_F0nm_2goF  PADDSH.H		Rm, Ro, Imm11u, Rn

These would be part of a Neural Net extension.
These instructions would combine a SIMD shuffle with a vector operation.

The immediate consists of a low and high part:
* ( 7:0): Shuffle Mask
** (1:0): First Element
** (3:2): Second Element
** (5:4): Third Element
** (7:6): Fourth Element
* (   8): MBZ
* (10:9): Shuffle Mode
** 00: Non-Shuffle, Rounding Mode
** 01: Shuffle Rm
** 10: Shuffle Ro
** 11: Shuffle both Rm and Ro

In the Non-Shuffle Mode, these overlap with the Rounding Mode encodings, thus the Shuffle form of the instruction will be defined by the Shuffle Mode being non-zero.



=== PxxxSH.x (3RI, NNX, Possible) ===

* FE-FE F2nm_7Pjj  PMACSH.H		Rm, Imm48fv8sh, Rn		//(NNX)
* FE-FE F2nm_7Qjj  PMACSH.F		Xm, Imm48fv8sh, Xn		//(NNX)
* FE-FE F2nm_8pjj  PADD.H		Rm, Imm56fv, Rn			//(NNX)
* FE-FE F2nm_8Pjj  PADD.F		Xm, Imm56fv, Xn			//(NNX)
* FE-FE F2nm_8qjj  PMUL.H		Rm, Imm56fv, Rn			//(NNX)
* FE-FE F2nm_8Qjj  PMUL.F		Xm, Imm56fv, Xn			//(NNX)
* FE-FE F2nm_9pjj  PADDSH.H		Rm, Imm48fv8sh, Rn		//(NNX)
* FE-FE F2nm_9Pjj  PADDSH.F		Xm, Imm48fv8sh, Xn		//(NNX)
* FE-FE F2nm_9qjj  PMULSH.H		Rm, Imm48fv8sh, Rn		//(NNX)
* FE-FE F2nm_9Qjj  PMULSH.F		Xm, Imm48fv8sh, Xn		//(NNX)

These would be part of a Neural Net extension.
These instructions would combine a SIMD shuffle with a vector operation against a vector immediate.

In this case, an Imm56 field would be interpreted as:
* Imm56fv:		4x S.E5.F8		(No Shuffle)
* Imm48fv8sh:	4x S.E5.F6.I2	(Shuffle)
** The I2 gives the source element within Rm. 


=== PMUL.F8H (3RI, NNX, Possible) ===

* F0nm_7Go6  PMUL.F8H	Rm, Ro, Rn			//(NNX) PMUL FP8 to FP16

Packed multiply of two 4 element FP8 vectors widening to a 4 element Binary16 vector.


=== PMULT.x (3RI, ?, Possible) ===

* F2nm_7Rjj  PMULTX.L 	Rm, Imm8u, Rn		//Multiply Elements, 2b
* F2nm_8Rjj  PMULT.W	Rm, Imm8u, Rn		//Multiply Elements, 2b

Multiply each element (16 or 32 bit) with a 2 bit value:
* 00:  0 (Zeroes Bits)
* 01:  1 (Keeps Bits)
* 10: ~1 (Invert All Bits)
* 11: -1 (Invert Sign Bit)

The W variant will operate on 16 bit elements whereas L will operate on 32-bit.



=== RBSLD / RBSST (RBS) ===

* F2n2_Ehjj  RBSLD.Q	Imm10u, Rn				//(RBS) RBS Load
* F2n2_EHjj  RBSLD.X	Imm10u, Xn				//(RBS) RBS Load X
* F2n3_Ehjj  RBSST.Q	Imm10u, Rn				//(RBS) RBS Store
* F2n3_EHjj  RBSST.X	Imm10u, Xn				//(RBS) RBS Store X

Load From or Store To Banked Register.

RBS Registers will give a set of banked registers, and these instructions will allow moving between the banked registers and GPRs.

Bits:
* (5:0), Register ID
* (7:6), Bank
* (8+), MBZ



=== ROTCL ===

* F036_3gn0  ROTCL.L	Rn
* F036_3Gn0  ROTCL.Q	Rn

Rn'=(Rn SHL 1)|SR.T; SR.T=MSB

Rotate the value in Rn left by 1 bit, pulling SR.T into the LSB, and putting the shifted out bit into SR.T.

This operation is only valid in Lane 1.


=== ROTCR ===

* F037_3gn0  ROTCR.L	Rn
* F037_3Gn0  ROTCR.Q	Rn

Rotate the value in Rn right by 1 bit, pulling SR.T into the MSB, and putting the shifted out bit into SR.T.

This operation is only valid in Lane 1.


=== ROTL ===

* F034_3en0 ? ROTL		Rn

Rn=(Rn SHL 1)|(Rn SHR 31)

Rotate the value in Rn left by 1 bit.

This operation is only valid in Lane 1.


=== ROTR ===

* F035_3en0 ? ROTR		Rn

Rn=(Rn SHR 1)|(Rn SHL 31)

Rotate the value in Rn right by 1 bit.

This operation is only valid in Lane 1.


=== ROTL (3R) ===

* F0nm_3go2  ROTL.Q		Rm, Ro, Rn		//(ALU) Rotate Left (64b)
* F0nm_3go6  ROTL.L		Rm, Ro, Rn		//(ALU) Rotate Left (32b)
* F0nm_3Go6  ROTLX		Xm, Ro, Xn		//(ALUX) Rotate Left (128b)

Rotate the value in Rn left by Ro.

This operation is only valid in Lane 1.


=== ROTR (3R) ===

** F0nm_3go3  ROTR.Q	Rm, Ro, Rn		//(ALU) Rotate Right (64b)

Rotate the value in Rn right by Ro.

This operation is only valid in Lane 1.


=== RTE ===

* 30C0       RTE
* F000_30C0  RTE	//Fix32

Return from exception.

Initiates the behavior for returning from an exception handler (described elsewhere).


=== RTS / RTSU ===

* 3010       RTS
* 3012       RTSU
* F000_3010  RTS	//Fix32
* F002_3010  RTSU	//Fix32

Return from Subroutine.
This effectively restores the value from LR into PC, transferring control back to LR.

RTSU will be functionally equivalent to RTS with the main exception that LR may not have been modified within an implementation-dependent number of instructions (based on the number of instructions which may be "in-flight" in the processor's execution pipeline). This is to allow the instruction to be treated like a direct unconditional branch. The result of modifying LR within the pipeline is undefined.

With RTS, the branch predictor may treat it as a predicted branch.

The high order 16 bits of LR will contain saved user flag states:
* LR(63:52) will be copied to SR(15: 4)
* LR(51:50) will be copied to SR(27:26)
* LR(49:48) will be copied to SR( 1: 0)


=== SBB ===

* 13nm       SBB	Rm, Rn
* F0nm_1e39  SBB	Rm, Rn

Rn=Rn+(~Rm)+(!SR.T)

Subtract with Borrow.
SR.T is updated with the borrow result of this operation.

WEX: SBB will only update SR.T in Lane 1. If an SBB is in another lane, the state of SR.T will be undefined following the operation.



=== SETS ===

* 3070       SETS
* F000_3070  SETS		//Fix32

Set the SR.S flag.


=== SETT ===

* 3050       SETT
* F000_3050  SETT		//Fix32

Set the SR.T flag.


=== SETTRAP ===

* F0nm_1GA9  SETTRAP	Rn, (Rm)

Optional Debug Feature: Set Tripwire Mode
This modifies the status of the pointed-to cache line, such as setting it to be temporarily write protected.

The Rm pattern will encode the desired mode:
* (3:1): Properties to modify (RXW)
* (  0): Set (Disable Access) or Clear (Enable Access)

Accessing the cache line in a way inconsistent with the current mode may cause a fault.

Setting a tripwire may also optionally trap if an attempt is made to address across the tripwire for sake of a given access type, treating it as a memory boundary.


Note that this feature is optional and intended for debugging, as such:
* It may be treated as NO-OP;
* The effects may be transient and forgotten at any point.
* It may only be used disallow access to a piece of memory.


=== SHAD ===

* F0nm_1g6C  SHAD	Rm, Rn
* F0nm_5go4  SHAD	Rm, Ro, Rn
* F2nm_8pjj  SHAD	Rm, Imm9, Rn

Barrel Shift, Arithmetic.
Operates on a 32-bit value.
The input is 32-bit sign extended prior to performing the shift.

The Shift Value will behave as if it were a sign-extended byte. Positive values will encode left-shifts whereas negative values will encode right shifts.

For SHAD, the shift will be 32-bit modular.

This is allowed in Lanes 1 and 2, Support in Lane 3 is Optional.


=== SHADQ ===

* F0nm_1G6C  SHADQ		Rm, Rn
* F0nm_5Go4  SHADQ		Rm, Ro, Rn
* F2nm_8Pjj  SHADQ		Rm, Imm9, Rn

Barrel Shift, Arithmetic.
Operates on a 64-bit value.

The Shift Value will behave as if it were a sign-extended byte. Positive values will encode left-shifts whereas negative values will encode right shifts.

For SHADQ, the shift will be 64-bit modular.

This is allowed in Lanes 1 and 2, Support in Lane 3 is Optional.


=== SHADX ===

* F0nm_3Go4  SHADX		Xm, Ro, Xn		//(ALUX) Shift Arithmetic (128b)

Barrel Shift, Arithmetic.
Operates on a 128-bit value.

The Shift Value will behave as if it were a sign-extended byte. Positive values will encode left-shifts whereas negative values will encode right shifts.


=== SHLD ===

* F0nm_1g7C  SHLD	Rm, Rn
* F0nm_5go5  SHLD	Rm, Ro, Rn
* F2nm_9pjj  SHLD	Rm, Imm9u, Rn

Barrel Shift, Logical.
Operates on a 32-bit value.
The input is 32-bit zero extended prior to performing the shift.

The Shift Value will behave as if it were a sign-extended byte. Positive values will encode left-shifts whereas negative values will encode right shifts.

For SHLD, the shift will be 32-bit modular.

This is allowed in Lanes 1 and 2, Support in Lane 3 is Optional.


=== SHLDQ ===

* F0nm_1G7C  SHLDQ		Rm, Rn
* F0nm_5Go5  SHLDQ		Rm, Ro, Rn
* F2nm_9Pjj  SHLDQ		Rm, Imm9u, Rn

Barrel Shift, Logical.
Operates on a 64-bit value.

The Shift Value will behave as if it were a sign-extended byte. Positive values will encode left-shifts whereas negative values will encode right shifts.

For SHLD.Q, the shift will be 64-bit modular.

This is allowed in Lanes 1 and 2, Support in Lane 3 is Optional.


=== SHLDX ===

* F0nm_3Go5  SHLDX		Xm, Ro, Xn		//(ALUX) Shift Logical (128b)

Barrel Shift, Logical.
Operates on a 128-bit value.

The Shift Value will behave as if it were a sign-extended byte. Positive values will encode left-shifts whereas negative values will encode right shifts.


=== SHAR ===

* F0nm_2go2  SHAR	Rm, Ro, Rn
* F2nm_8pjj  SHAD	Rm, -Imm9, Rn

Barrel Shift, Arithmetic Right.
Operates on a 32-bit value.
The input is 32-bit sign extended prior to performing the shift.

Positive values will correspond to a right shift, whereas negative values will be undefined. The immediate form will be encoded by inverting the sign.

For SHAR, the shift will be 32-bit modular.

This is allowed in Lanes 1 and 2, Support in Lane 3 is Optional.


=== SHARQ ===

* F0nm_2Go2  SHARQ		Rm, Ro, Rn
* F2nm_8Pjj  SHADQ		Rm, -Imm9, Rn

Barrel Shift, Arithmetic Right.
Operates on a 64-bit value.

The Shift Value will behave as if it were a sign-extended byte. Positive values will encode left-shifts whereas negative values will encode right shifts.

Positive values will correspond to a right shift, whereas negative values will be undefined. The immediate form will be encoded by inverting the sign.

For SHADQ, the shift will be 64-bit modular.

This is allowed in Lanes 1 and 2, Support in Lane 3 is Optional.


=== SHARX ===

* F0nm_3Go2  SHARX		Xm, Ro, Xn		//(ALUX) Shift Arithmetic (128b)

Barrel Shift, Arithmetic Right.
Operates on a 128-bit value.

The Shift Value will behave as if it were a sign-extended byte. Positive values will encode left-shifts whereas negative values will encode right shifts.

Positive values will correspond to a right shift, whereas negative values will be undefined. The immediate form will be encoded by inverting the sign.


=== SHLR ===

* F0nm_2go3  SHLR	Rm, Ro, Rn
* F2nm_9pjj  SHLD	Rm, -Imm9, Rn

Barrel Shift, Logical Right.
Operates on a 32-bit value.
The input is 32-bit zero extended prior to performing the shift.

Positive values will correspond to a right shift, whereas negative values will be undefined. The immediate form will be encoded by inverting the sign.

For SHLR, the shift will be 32-bit modular.

This is allowed in Lanes 1 and 2, Support in Lane 3 is Optional.


=== SHLRQ ===

* F0nm_2Go3  SHLRQ		Rm, Ro, Rn
* F2nm_9Pjj  SHLDQ		Rm, -Imm9, Rn

Barrel Shift, Logical Right.
Operates on a 64-bit value.

Positive values will correspond to a right shift, whereas negative values will be undefined. The immediate form will be encoded by inverting the sign.

For SHLRQ, the shift will be 64-bit modular.

This is allowed in Lanes 1 and 2, Support in Lane 3 is Optional.


=== SHLRX ===

* F0nm_3Go3  SHLRX		Xm, Ro, Xn		//(ALUX) Shift Logical (128b)

Barrel Shift, Logical Right.
Operates on a 128-bit value.

Positive values will correspond to a right shift, whereas negative values will be undefined. The immediate form will be encoded by inverting the sign.


=== SLEEP ===

* 3020       SLEEP
* F000_3020  SLEEP		//Fix32

Causes the processor to sleep until the next interrupt.

May generate an fault if used in a context where this would halt the processor indefinitely.


=== SNIPExx ===

* F0nm_1gB8  SNIPEDC	Rm, Rn		//Calculate L1 D$ Snipe Address
* F0nm_1GB8  SNIPEIC	Rm, Rn		//Calculate L1 I$ Snipe Address

For the address given in Rm, calculate a "sniper" address within the ZERO or RTS pages. These instructions assume either physical memory mapping, or that these pages are identity mapped.

Accessing the address returned by these SNIPEDC will evict whatever was in the cache-line pointed to by Rm from the L1 D-Cache.

Calling to the address returned by SNIPEIC will evict whatever was in this location in the L1 I-Cache.

These operations will only work for direct-mapped caches.


The SNIPExx instructions will generate a NULL result if SNIPExx is not valid (such as due to using an associative cache). In this case, an INVDC or INVIC cache-flush process will be required instead.


=== SRTPROP (?) ===

* F06C_3en0  SRTPROP	Imm5u

Perform an operation between SR.T and U0..U7.
* (4:2): Bit Index
* (1:0): Op
** 00: U(x) = SR.T
** 01: SR.T = U(x)
** 10: SR.T = SR.T & U(x)
** 11: SR.T = SR.T | U(x)


=== SRTTWID (?) ===

* 36nB       SRTTWID	Imm4
* F06B_3en0  SRTTWID	Imm5

Twiddle SR.T status bits, using SR.U0 .. SR.U7 as a predicate stack.

The Immed functions as a sub-command:
* 00: PUSH SR.T: Shift SR.U Left 1 bit, SR.U0=SR.T;
* 01: POP  SR.T: Shift SR.U Right 1 bit, SR.T=SR.U0;
* 02: POP  SR.T: Shift SR.U Right 2 bits, SR.T=SR.U1;
* 03: POP  SR.T: Shift SR.U Right 3 bits, SR.T=SR.U2;
* 04: AND      : SR.T = SR.T AND SR.U0;
* 05: OR       : SR.T = SR.T OR SR.U0;
* 06: AND+PUSH : AND followed by a PUSH;
* 07: OR +PUSH : OR followed by a PUSH;
* 08: CLEAR    : SR.U=0

An alternate set of ops may use U0..U3 as a THEN counter, and U4..U7 as an ELSE counter.

Alternate IF/ELSE Block:
* 0E: ELSE
* 0F: ENDIF
* 10: IF0T
* 11: IF0F
* 12: IF1T
* 13: IF1F
* 14: IFAAT
* 15: IFAAF
* 16: IFOOT
* 17: IFOOF

IF0T/IF0F will enter an IF block while also resetting the counters. This will function as a top-level IF block.

IF1T/IF1F will reuse the existing counters, entering a nested block.

IFAAT/IFAAF will combine T with the existing block via a Logical AND relation.
IFOOT/IFOOF will combine T with the existing block via a Logical OR relation.

ELSE will transition from a THEN to an ELSE block.

ENDIF will exit the block.

The True/False state of these ops will reflect whether SR.T or !SR.T will be the condition for entering the THEN block.

Following these ops, SR.T will be set to indicate which block is to be executed:
* SR.T=1: Execute the THEN block.
* SR.T=0: Execute the ELSE block.


=== SUB ===

* 11nm       SUB		Rm, Rn
* 59nm       SUB		Rm, DLR, Rn
* F0nm_1go1  SUB		Rm, Ro, Rn			//Rn=Rm-Ro, Q=0
* F0nm_1e19 /? SUB		Rm, Rn				//Rn=Rn-Rm (Deprecated)

Rn=Rn-Rm
Rn=Rm-Ro
Rn=Rm-Imm

Subtract the source from the destination, storing the result in the destination register.

There will not be immediate forms of SUB as this will be handled by ADD.


=== SUBx.L ===

* F0nm_5goD  SUBS.L	Rm, Ro, Rn
* F0nm_5GoD  SUBU.L	Rm, Ro, Rn

Subtract the source from the destination, storing the result in the destination register.

This variant sign or zero extends the 32 bit result to 64 bits.

There will not be immediate forms of SUBx.L as this will be handled by ADDx.L.


=== SUBx.P ===

* F0nm_7go7  SUB.P	Rm, Ro, Rn
* F0nm_7Go7  SUBX.P	Xm, Xo, Xn

Subtract the source from the destination, storing the result in the destination register.

This instruction performs an 48-bit subtraction, sign-extending the result to 64 bits. This instruction will only be valid if ALUPTR is Enabled.

The SUBX.P instruction will subtract produce a 128-bit result, behaving as if each 48-bit piece had been zero-extended prior to the operation. This instruction will also require ALUX.


=== SXENTR / SXEXIT / SVENTR ===

* 3082       SXENTR		//Enter Superuser Mode
* 3092       SUENTR		//Enter User Mode
* 30B2       SVENTR		//Enter Supervisor Mode
* F002_3080  SXENTR		//Enter Superuser Mode
* F002_3090  SUENTR		//Enter User Mode
* F002_30B0  SVENTR		//Enter Supervisor Mode

Local Mode transitions.
* The SXENTR instruction transitions to Superuser Mode.
** Valid from User Mode or Supervisor Mode.
* The SUENTR instruction transitions to User Mode (Not from SX or SV).
** Valid from Superuser Mode or Supervisor Mode.
* The SVENTR instruction transitions to Supervisor Mode.
** Valid from Superuser Mode.

These instructions are only valid within Secure-Execute Pages or from Supervisor mode.

These instructions will be specific to certain modes.


=== TRAP ===

* 36j3       TRAP	Imm4u
* 36n8       TRAP	Rn
* F068_3gn0  TRAP	Rn
* F068_3Gn0  TRAP	Xn

Generate an Interrupt.

Trap with an Imm4 uses the bits from the immediate as part of the bit-pattern for EXSR (0xC08j).


The register trap gives the full EXSR bit pattern (in the low 16 bits).
The high 48 bits of the register may give an address which will be visible to the exception handler by being copied into TEA.

In user mode, only SysCall patterns will be allowed, other patterns will result either in a fault or some other implementation-defined behavior. In supervisor mode, patterns representing a valid EXSR pattern will behave as-if this exception had occured.

Patterns which do not represent a valid EXSR pattern may result in a fault or some other implementation-defined behavior.

In a VM, an EXSR with the pattern 0xFzzz will correspond to a VM trap call. This will call a handler outside of the emulated environment. Use of a VM trap within a bare-metal implementation is undefined and should not be used.

Within a multi-core setup, bits (11:8) may be used to route the interrupt to a different core.


=== TST ===

* 14nm       TST	Rm, Rn
* F0nm_1g49  TST	Rm, Rn				//SR.T=!(Rm&Rn)
* F2n4_Cgjj  TST	Imm9u, Rn			//SR.T=!(Rm&Rn)
* F2n5_Cgjj  TST	Imm9n, Rn			//SR.T=!(Rm&Rn)

SR.T=!(Rm AND Rn)

Perform a bitwise AND of the source and destination registers, updating SR.T based on whether the result of this operation is equal to zero.

A zero result will cause SR.T to be set, and a nonzero result will cause SR.T to be cleared.

This operation will only test the low 32 bits.

For the 2R form, if E.i is Set, SR.S is updated and SR.T is unchanged.
For the 2RI form, if W.m is Set, SR.S is updated and SR.T is unchanged.
The values of the other bits are unchanged by this operation.


=== TSTQ ===

* 54nm       TSTQ	Rm, Rn
* F0nm_1G49  TSTQ	Rm, Rn				//SR.T=!(Rm&Rn), E.i=0
* F0nm_1G49  TSTQS	Rm, Rn				//SR.S=!(Rm&Rn), E.i=1
* F2n4_CGjj  TSTQ	Imm9u, Rn			//SR.T=!(Rm&Rn)
* F2n5_CGjj  TSTQ	Imm9n, Rn			//SR.T=!(Rm&Rn)

SR.T=!(Rm AND Rn)

Perform a bitwise AND of the source and destination registers, updating SR.T based on whether the result of this operation is equal to zero.

A zero result will cause SR.T to be set, and a nonzero result will cause SR.T to be cleared.

This operation will test all 64 bits.

For the 2R form, if E.i is Set, SR.S is updated and SR.T is unchanged.
For the 2RI form, if W.m is Set, SR.S is updated and SR.T is unchanged.
The values of the other bits are unchanged by this operation.


=== VSKx ===

* F0nm_1g68 VSKG		Rm, Rn				//Canary Generate
* F0nm_1g78 VSKC		Rm, Rn				//Canary Check

These instructions will Generate or Check a Canary value.
The Generate operation will transform a reference value into an internal form via combining it with a randomized number. The contents of the generated number will be implementation defined.

The Check operation will verify that the values match, and will raise a Canary Check exception if the values don't match. The reference value supplied to the check will be required to be the same as that supplied to the generate instruction.

The Rm value will supply a reference value (magic number), with Rn holding the Canary value.


=== WEXMD (WEX) ===

* 36n9       WEXMD	Imm4 //( 0..15)
* 3En9       WEXMD	Imm4 //(16..31)
* F069_3en0  WEXMD	Imm5u

Sets the Active WEX Profile.
This instruction is No-Op if the core does not support WEX.

This instruction will enable or disable WEX based on whether or not the given profile ID is supported by the core. This will be achieved by updating the value for SR.WXE based on the profile ID given as an immediate.

If WEX is disabled, any WEX sequences encountered will be treated as if they were scalar instructions.


If the mode is 31 (Supervisor Mode Only), WEX will be disabled and the Canary Seed will be randomized. Otherwise, behavior is NOP or undefined.
Note that any Canary values generated before executing this instruction will become invalid.


=== XBLESS (BCE) ===

* F0nm_1GEE  XBLESS		Xm, Xn


Bless a Capability.

This instruction has behavior similar to a MOVX instruction, except that while moving the value in the registers, it will also set the tag bits to mark the register contents as a capability.

This is a priveleged instruction within the Bounds Check Enforcing extension.

If not Bounds Check Enforce is not set, or the processor is in Supervisor Mode, the BLESS operation will always succeed.

If in User Mode and BCE is enabled, this will fault if Xm is not already blessed as a capability (otherwise, behavior will be equivalent to MOVX).


=== XMOV.x (XMOV, Possible) ===

* F0nm_4Go0  XMOV.B		Rn, (Xm, Disp5)		//(XMOV)
* F0nm_4go1  XMOV.X		Xn, (Xm, Disp5)		//(XMOV)
* F0nm_4Go1  XMOV.W		Rn, (Xm, Disp5)		//(XMOV)
* F0nm_4Go2  XMOV.L		Rn, (Xm, Disp5)		//(XMOV)
* F0nm_4Go3  XMOV.Q		Rn, (Xm, Disp5)		//(XMOV)
* F0nm_4Go4  XMOV.B		Rn, (Xm, Ro)		//(XMOV)
* F0nm_4go5  XMOV.X		Xn, (Xm, Ro)		//(XMOV)
* F0nm_4Go5  XMOV.W		Rn, (Xm, Ro)		//(XMOV)
* F0nm_4Go6  XMOV.L		Rn, (Xm, Ro)		//(XMOV)
* F0nm_4Go7  XMOV.Q		Rn, (Xm, Ro)		//(XMOV)
* F0nm_4Go8  XMOV.B		(Xm, Disp5), Rn		//(XMOV)
* F0nm_4go9  XMOV.X		(Xm, Disp5), Xn		//(XMOV)
* F0nm_4Go9  XMOV.W		(Xm, Disp5), Rn		//(XMOV)
* F0nm_4GoA  XMOV.L		(Xm, Disp5), Rn		//(XMOV)
* F0nm_4GoB  XMOV.Q		(Xm, Disp5), Rn		//(XMOV)
* F0nm_4GoC  XMOV.B		(Xm, Ro), Rn		//(XMOV)
* F0nm_4goD  XMOV.X		(Xm, Ro), Xn		//(XMOV)
* F0nm_4GoD  XMOV.W		(Xm, Ro), Rn		//(XMOV)
* F0nm_4GoE  XMOV.L		(Xm, Ro), Rn		//(XMOV)
* F0nm_4GoF  XMOV.Q		(Xm, Ro), Rn		//(XMOV)

These are XMOV Load/Store Ops.

If XMOV is supported, it will allow expanding the logical address space up to 96 bits with a 33-bit sign-extended displacement.

The 96-bit address will be composed from the low 48 bits of each register in the pair. The remaining upper 16 bits of each register will be used as tag bits.

Note that the XMOV instruction may enforce Bounds Checks depending on the high-order bits of the pointer.

Tagged Object Pointer
* Bits (127:112)=Ignored
* Bits (111: 64)=Address(95:48)
* Bits ( 63: 60)=Tag (0000)
* Bits ( 59: 48)=Object Tag (Ignored)
* Bits ( 47:  0)=Address(47:0)

Bounded Array (Extended)
* Bits (127:112)=ArrayBound(27:12)
* Bits (111: 64)=Address(95:48)
* Bits ( 63: 60)=Tag (0010)
* Bits ( 59: 48)=ArrayBound(11:0)
* Bits ( 47:  0)=Address(47:0)

Double Bounded Array (Extended)
* Bits (127:124)=Element Size
* Bits (123:112)=Array Lower Bound
* Bits (111: 64)=Address(95:48)
* Bits ( 63: 60)=Tag (0011)
* Bits ( 59: 48)=Array Upper Bound
* Bits ( 47:  0)=Address(47:0)

The Tag 0001 will be defined here as a non-pointer, and trying to access memory through it will raise a fault if bounds checks are supported and enabled.

Other tags here will be reserved for future extension.



=== XMOVTT (BCE) ===

* F0nm_8Go6            XMOVTT	Xm, Ro, Xn		//(TTAG+XGPR)
* FFw0_iiii_F84n_iiii  XMOVTT	Imm32u, Xn		//(TTAG+XGPR) WMI==11


Set Type Tag.

In the 3R form, the Capability in Rn will be set to the bounds given in Ro, if valid.

On success, the behavior will be to copy the low 16 bits of Ro to bits (63:48) of Xn, and (31:16) of Ro to (127:112) of Xn, with other bits being copied from Rm unchanged.

The Imm32 form is similar, except using Imm32 in place of Ro, and Xn as the Xm input.


If given an unbounded bare value, it will set any bounds. If the bounds are valid, it may also BLESS the capability. This may trap if XBLESS would trap.

If given a bounded capability, the bounds may only be set if they fall within the prior bounds, otherwise it will trap.

This instruction will have special behavior as part of the Bounds Check Enforcing extension. If this extension is not present, only the basic behavior will be performed, and no traps will occur. If this extension is present, but the CPU is not in Bounds Enforcing mode, it will auto bless the capability, and will not trap.




=== XOR ===

* 17nm       XOR		Rm, Rn
* 5Cnm       XOR		Rm, DLR, Rn
* F0nm_1go7  XOR		Rm, Ro, Rn				//Rn=Rm XOR Ro, Q=0
* F0nm_1e79 /? XOR		Rm, Rn				//Deprecated
* F2nm_7gjj  XOR		Rm, Imm9u, Rn		//


*    FE F2nm_7gjj  XOR		Rm, Imm33s, Rn	//(Jumbo)
* FE-FE F2nm_7gjj  XOR		Rm, Imm57s, Rn	//(FPIMM)

Perform a bitwise XOR of the source and destination registers, storing the result in the destination.


== GPR SIMD Extension (GSV, Optional) ==

These instructions will be considered part of the GPR SIMD Vector Extension (GSV). These instruction forms will be considered optional, and exist mostly to add rudimentary SIMD support to BJX2.

The GSVF variant will also support packed half and single precision floating-point values, whereas GSV by itself will only support packed Word and DWord operations.

Packed byte operations will not be directly supported by GSV.

Special subsets will be defined:
* GSVF: Present if both GSV and GFP exist, adds FP-SIMD
* GSVFX: Present if both GSV and GFPX exist; Supports 128-bit Vectors.


=== MOVxD (GSV) (Dropped) ===

* / F0nm_1g8A  MOVHD		Rm, Rn			//(GSV) Move High DWord
* / F0nm_1G8A  MOVLD		Rm, Rn			//(GSV) Move Low DWord
* / F0nm_1gAA  MOVHLD		Rm, Rn			//(GSV) Rn=Rm (MOV High to Low)
* / F0nm_1GAA  MOVLHD		Rm, Rn			//(GSV) Rn=Rm (MOV Low to High)

Partial Register Move. These MOV part of the bits from Rm into Rn, while leaving the other bits in Rn as their original value.


=== MOVxD 3R (GSV) ===

* F0nm_2go8  MOVHD		Rm, Ro, Rn		//(GSV) MOV, High DWords
* F0nm_2Go8  MOVLD		Rm, Ro, Rn		//(GSV) MOV, Low DWords
* F0nm_2go9  MOVHLD		Rm, Ro, Rn		//(GSV) MOV, High and Low DWords
* F0nm_2Go9  MOVLHD		Rm, Ro, Rn		//(GSV) MOV, Low and High DWords

The 3R forms will merge the values from two source registers into a destination register.

The high 32 bits will come from Rm, and the low 32 bits will come from Ro.

MOVHD will select the high-order bits from both registers, and MOVLD the low order bits from both registers.

MOVHLD will select the high bits from Rm, and the low bits from Ro.
MOVLHD will select the low bits from Rm, and the high bits from Ro.


=== MOVxW 3R (GSV) ===

* F0nm_7go3  MOVHW		Rm, Ro, Rn		//(GSV) MOV, High Words
* F0nm_7Go3  MOVLW		Rm, Ro, Rn		//(GSV) MOV, Low Words

The 3R forms will merge the values from two source registers into a destination register.

This will compose a 32-bit DWORD result from Rm and Ro.
The high 16 bits will come from Rm, and the low 16 bits will come from Ro.
The high 32 bits of the destination register will be zeroed.

MOVHW will select the (31:16) from both registers, and MOVLW will select (15:0) from both registers.


=== PADD.x 3R (GSV) ===

* F0nm_2go0  PADD.W		Rm, Ro, Rn		//(GSV) Packed ADD Word
* F0nm_2Go0  PADD.L		Rm, Ro, Rn		//(GSV) Packed ADD DWord
* F0nm_2go5  PADD.F		Rm, Ro, Rn		//(GSVF) Packed FADD (2x Float)
* F0nm_2Go5  PADDX.F	Xm, Xo, Xn		//(GSVFX) Packed FADD (4x Float)
* F0nm_2goD  PADD.H 	Rm, Ro, Rn		//(GSVF) Packed FADD (4x Half)
* F0nm_2GoD  PADDX.D	Xm, Xo, Xn		//(GSVFX) Packed FADD (2x Double)
* FFw0_08ii_F0nm_2go5  PADD.X		Rm, Ro, Rn, Imm8		//(GSVFX)
* FFw0_08ii_F0nm_2Go5  PADDX.X		Xm, Xo, Xn, Imm8		//(GSVFX)

Packed ADD, but in 3-register form.
PADDX.x will operate on a 128-bit vector.

The Op64 encoding will add a Rounding Mode.
* The Single Precision rounding modes will be special.
** This will encode a request to go faster at the cost of precision.

PzzzX.z variants will be Lane-1 only.

Pzzz.W and Pzzz.L will be allowed in Lanes 1 and 2.
* These ops may be co-issued in any combination.

Pzzz.H / Pzzz.F:
* In WEX Mode 2, these will be Lane 1 only.
* In modes with Dual Lane FPU (FP2) these may be issued from Lane 2.
* Normally, FPU or FP-SIMD ops may not be co-issued with other FPU/FP-SIMD ops.
* In WEX-FP2 Modes, Pzzz.F may be co-issued with itself.
** Matching pairs will be allowed for 2x Single.
** These co-issue cases will be functionally similar to PzzzX.F.
** However, the co-issue case will allow free selection of registers.
** It will be undefined if the paired instructions do not perform the same operation.

Note that FPU SIMD ops will nominally defined as using truncate rounding.
* However, minimal guerantees will be made as to rounding behavior or accuracy.


=== PCMPx.x (GSV) ===

* F0nm_1gAA  PCMPEQ.H	Rm, Rn		//Packed Compare Half, Equal
* F0nm_1GAA  PCMPEQ.F	Rm, Rn		//Packed Compare Single, Equal
* F0nm_1gBA  PCMPGT.H	Rm, Rn		//Packed Compare Half, Greater
* F0nm_1GBA  PCMPGT.F	Rm, Rn		//Packed Compare Single, Greater

* F0nm_1gCA  PCMPEQ.W	Rm, Rn		//Pack Compare Word, Equal
* F0nm_1GCA  PCMPEQ.L	Rm, Rn		//Pack Compare DWord, Equal
* F0nm_1gDA  PCMPHI.W	Rm, Rn		//Pack Compare Word, Above
* F0nm_1GDA  PCMPHI.L	Rm, Rn		//Pack Compare DWord, Above
* F0nm_1gEA  PCMPGT.W	Rm, Rn		//Pack Compare Word, Greater
* F0nm_1GEA  PCMPGT.L	Rm, Rn		//Pack Compare DWord, Greater

Packed Compare.

With Word and Half will update the P/Q/R/O bits in SR depending on the result of each comparison. These will compare each element Rn with the corresponding element in Rm. The SR.P bit will correspond with the low-order element, and SR.O will correspond with the high order element.

Half of Single comparisons will be similar to Word or DWord comparisons, but will differ in that values are sign magnitude.

With Packed DWord and Float32, The T and S bits will be updated based on the comparison (T for the low element, and S for the high element).

Note that bits other than those defined for the comparison will be undefined following the operation. So, If Q=0 then S and T are undefined, and if Q=1 then P/Q/R/O are undefined. An implementation may or may not ignore Q and update both sets of bits at the same time.


=== PCSELT.x (GSV) ===

* F0nm_2go4  PCSELT.W	Rm, Ro, Rn
* F0nm_2Go4  PCSELT.L	Rm, Ro, Rn

PCSELT.L will select elements between Rm or Ro based on the value of SR.S and SR.T, storing the result in Rn. If SR.T is set, the low 32 bits from Rm is used, otherwise the low 32 bits from Ro are used. If SR.S is set, the high 32 bits from Rm is used, otherwise the high 32 bits from Ro are used.

PCSELT.W will select each Word element from Rm or Rn depending on the corresponding P/Q/R/O bits in SR. If set, the word from Rm will be selected, otherwise the value from Ro will be used.


=== PDOTT.x (GSVF, Possible) ===

* F0ni_1g9A  PDOTT.H		Imm5u, Rn	//(GSVF? Predict Dot Predicate)
* F0ni_1G9A  PDOTT.F		Imm5u, Rn	//(GSVF? Predict Dot Predicate)

Make a prediction about a Dot Vector.
These predictions will be fairly loose in terms of accuracy requirements.
In particular, the operation need not actually perform a floating point horizontal add, but may instead use lookup tables or similar to infer whether or not the predicate is likely to be true.

For inexact cases, this operation will be biased in favor of a false positive than a false negative.

The Imm5u value will encode the requested predicate (Half):
* 0: (Rx+Ry+Rz+Rw) >= 0
* 1: (Rx+Ry+Rz+Rw) <= 0
* 2: (Rx+Ry+Rz+Rw) >= 1
* 3: (Rx+Ry+Rz+Rw) <= 1

The Imm5u value will encode the requested predicate (Single):
* 0: (Rx+Ry) >= 0
* 1: (Rx+Ry) <= 0
* 2: (Rx+Ry) >= 1
* 3: (Rx+Ry) <= 1


=== PLDCF8H / PSTCF8H (GSVF) ===

* F0nm_1g0A  PSTCF8H		Rm, Rn			//(GSVF) Packed FP16 to FP8 (4x)
* F0nm_1g1A  PLDCF8H		Rm, Rn			//(GSVF) Packed FP8 to FP16 (4x)

Packed convert between FP8 and Half.

The FP8 format will be defined as S.E4.F3 with a Bias of 7.
* Saturates to 0x7F or 0xFF on overflow.
* 0x00 will be defined as Zero, clamps to 0x00 on underflow.



=== PLDCH / PSTCH (GSVF) ===

* F0nm_1gCE  PLDCH		Rm, Rn		//(GSVF) Packed Half to Single (2x)
* F0nm_1GCE  PLDCHH		Rm, Rn		//(GSVF) Packed Half to Single (Hi)
* F0nm_1gDE  PLDCEHL	Rm, Rn		//(GSVF) Packed Ext-Half to Single (Lo)
* F0nm_1GDE  PLDCEHH	Rm, Rn		//(GSVF) Packed Ext-Half to Single (Hi)
* F0nm_1gEE  PSTCH		Rm, Rn		//(GSVF) Packed Single to Half (2x)

* FFw0_iiii_F88n_iiii  PLDCH	Imm32u, Rn		//(Wm=0, Wi=1) (GSVF,Op64)

Convert between Packed Half and Packed Single.

These operations will convert between Packed Half and the Packed Single format.

The Unpacking conversion will pad the low-order bits with zeroes.
The Packing conversion will truncate the low order bits.

The PLDCH instruction will unpack two Half Precision values from the low order
bits of the source register into two single-precision values in the destination.

The PLDCHH instruction will instead pull the two values from the high order bits of the source register.

The PLDCEHL and PLDCEHH will support a 3x Extended-Half format.
* The first 3 elements will be treated as Half Float numbers.
* The last element will consist of "Extension Bits" for the other elements.
* When each element is unpacked, its fraction will be extended by 5 bits.
** (52:48): Extension for First Element
** (57:53): Extension for Second Element
** (62:58): Extension for Third Element
* PLDCEHH: Will decode the Third Element.
** The Fourth element will be decoded as Zero.


=== PLDCM8x / PSTCM8x (GSVF) ===

* F0nm_1g8E  PLDCM8SH	Rm, Rn		//(GSVF) RGB32SF Unpack to RGB64F
* F0nm_1G8E  PLDCM8UH	Rm, Rn		//(GSVF) RGB32UF Unpack to RGB64F
* F0nm_1gAE  PSTCM8SH	Rm, Rn		//(GSVF) RGB32SF Pack from RGB64F
* F0nm_1GAE  PSTCM8UH	Rm, Rn		//(GSVF) RGB32UF Pack from RGB64F

** FFw0_iiii_F88n_iiii  PLDCM8SH	Imm32u, Rn	//(Wm=1, Wi=0) (GSVF,Op64)
** FFw0_iiii_F88n_iiii  PLDCM8UH	Imm32u, Rn	//(Wm=1, Wi=1) (GSVF,Op64)

These perform Packed Format Conversions for 8-bit microfloats.

These operations will convert between signed and unsigned 8-bit microfloats and the Packed Binary16 format.

The Unpacking conversion will pad the low-order bits with zeroes.
The Packing conversion will truncate the low order bits.

Formats here:
* RGB32SF: 4x FP8S (E4.F3.S), 32 bits
* RGB32UF: 4x FP8U (E4.F4), 32 bits
* RGB64F: 4x Binary16 (Packed Half), 64 bits

The FP8S and FP8U formats will consist a microfloat with a bias of 7.
* Includes Zero, does not include Inf or NaN.


=== PLDCM30AH / PSTCM30AH (RGBF) ===

* F0nm_1g9E  PLDCM30AH	Rm, Rn		//(RGBF) RGB30A Unpack to RGB64F
* F0nm_1gBE  PSTCM30AH	Rm, Rn		//(RGBF) RGB30A Pack from RGB64F

Pack or Unpack the RGB30A format.

The RGB30A Format will consist of three 10 bit fields:
* (31): Selects RGB/RGA
* (30): Selects Unsigned and Signed
* (29: 0): Depends on Selector

If Bit(31) is 0:
* (29:20): Red
* (19:10): Green
* ( 9: 0): Blue
* Values are truncated Binary16
** Unsigned, E5.F5
** Signed, E5.F4.S
* Alpha is decoded as 1.0 (3C00)

If Bit(31) is 1:
* (29:22): Alpha
* (21:15): Red
* (14: 7): Green
* ( 6: 0): Blue
** Unsigned is E4.F4 / E4.F3
** Signed is E4.F3.S / E4.F2.S

If Bit(30) is 0:
* Values are Unsigned
* Values are Signed


=== PMUL.F 3R (GSVF) ===

* F0nm_2go7  PMUL.F		Rm, Ro, Rn			//(GSVF) Packed FMUL (2x Float)
* F0nm_2Go7  PMULX.F	Xm, Xo, Xn			//(GSVFX) Packed FMUL (4x Float)
* F0nm_2goF  PMUL.H		Rm, Ro, Rn			//(GSVF) Packed FMUL (4x Half)
* F0nm_2GoF  PMULX.D	Xm, Xo, Xn			//(GSVFX) Packed FMUL (2x Double)
* FFw0_08ii_F0nm_2go7  PMUL.X		Rm, Ro, Rn, Imm8
* FFw0_08ii_F0nm_2Go7  PMULX.X		Xm, Xo, Xn, Imm8


Floating point packed multiply.
PMULX.x will operate on a 128-bit vector.

These will multiply the vectors in Rm and Ro, storing the result in Rn.
The vectors are available in 2x or 4x Single, 4x Half, or 2x Double.

The Op64 encoding will add a Rounding Mode.
* The Single Precision rounding modes will be special.
** This will encode a request to go faster at the cost of precision.

See [PADD.x 3R (GSV)] for Lane usage rules.


=== PMULx.W 3R (GSV) ===

* F0nm_5go1  PMULS.W	Rm, Ro, Rn		//(GSV) Packed Multiply (2x16->2x32)
* F0nm_5Go1  PMULU.W	Rm, Ro, Rn		//(GSV) Packed Multiply (2x16->2x32)
* F0nm_5go6  PMULS.LW	Rm, Ro, Rn		//(GSV) Packed Mul (Low Word)
* F0nm_5Go6  PMULU.LW	Rm, Ro, Rn		//(GSV) Packed Mul (Low Word)
* F0nm_5go7  PMULS.HW	Rm, Ro, Rn		//(GSV) Packed Mul (High Word)
* F0nm_5Go7  PMULU.HW	Rm, Ro, Rn		//(GSV) Packed Mul (High Word)

The basic 3R PMULx.W will accept vectors with two 16 bit inputs, and produce a vactor with two 32-bit products.

The LW and HW variants will consume 4-wide input vectors and produce a 4-wide output. The LW variant will keep the low order bits from the result, whereas the HW variant will keep the high-order bits.

See [PADD.x 3R (GSV)] for Lane usage rules.


=== PRCPA.x 2R (GSV) ===

* F0nm_1gAF  PRCPA.H		Rm, Rn		//(GSVF) RCPA (4x FP16)
* F0nm_1gBF  PRCPA.F		Rm, Rn		//(GSVF) RCPA (2x FP32)

Calculate an approximate reciprocal of an FP-SIMD vector.

For each element, this can be seen as:
* (~elem) + 0x7800 (Binary16)
* (~elem) + 0x7F000000 (Binary32)



=== PRELU.x 2R (GSV) ===

* F0nm_1GAF  PRELU.H		Rm, Rn		//(GSVF) RELU (4x FP16)
* F0nm_1GBF  PRELU.F		Rm, Rn		//(GSVF) RELU (2x FP32)

Perform a RELU operator.
In this case, positive values are passed through as-is, whereas negative values are zeroed.


=== PSHxx.W 2R (GSV) ===

* F0nm_1g6E  PSHAL.W		Rm, Rn	//(GSV) Arithmetic Left
* F0nm_1G6E  PSHLL.W		Rm, Rn	//(GSV) Logical Left
* F0nm_1g7E  PSHAR.W		Rm, Rn	//(GSV) Arithmetic Right
* F0nm_1G7E  PSHLR.W		Rm, Rn	//(GSV) Logical Right

Packed Shift by 1 bit.
These shifts will shift each element by 1 bit to the left or right.

Unlike with normal integer shift, the packed shift left will be saturating.
If the shift would have overflowed, signed will clamp the result to 0x8000 or 0x7FFF, whereas unsigned will clamp to 0xFFFF.


=== PSQRTA.x 2R (GSV) ===

* F0nm_1g8F  PSQRTA.H		Rm, Rn		//(GSVF) SQRTA (4x FP16)
* F0nm_1G8F  PSQRTUA.H		Rm, Rn		//(GSVF) SQRTA (4x FP16)
* F0nm_1g9F  PSQRTA.F		Rm, Rn		//(GSVF) SQRTA (2x FP32)
* F0nm_1G9F  PSQRTUA.F		Rm, Rn		//(GSVF) SQRTA (2x FP32)

Perform an approximate Square-Root.

For Binary16, the operation is effectively:
* (Val&0x8000)|(((Val&0x7FFF)>>1)+0x1E00)

The PSQRTUA variants differ in that they will set negative values to zero.


=== PSUB.x 3R (GSV) ===

* F0nm_2go1  PSUB.W		Rm, Ro, Rn			//(GSV) Packed SUB Word
* F0nm_2Go1  PSUB.L		Rm, Ro, Rn			//(GSV) Packed SUB DWord
* F0nm_2go6  PSUB.F		Rm, Ro, Rn			//(GSVF) Packed FSUB (2x Float)
* F0nm_2Go6  PSUBX.F	Rm, Ro, Rn			//(GSVFX) Packed FSUB (4x Float)
* F0nm_2goE  PSUB.H		Rm, Ro, Rn			//(GSVF) Packed FSUB (4x Half)
* F0nm_2GoE  PSUBX.D	Xm, Xo, Xn			//(GSVFX) Packed FSUB (2x Double)

Subtract Packed Elements.

Each element from Ro is subtracted from the corresponding element of Rm.

The PSUBX variant will operate on 128-bit vectors (GPR pair).

See [PADD.x 3R (GSV)] for Lane usage rules.


=== PSELx.x (GSV) (Drop) ===

* F0nm_1pBA / PSELT.W	Rm, Rn		//Packed Select Word
* F0nm_1qBA / PSELF.W	Rm, Rn		//Packed Select Word
* F0nm_1PBA / PSELT.L	Rm, Rn		//Packed Select DWord
* F0nm_1QBA / PSELF.L	Rm, Rn		//Packed Select DWord

Select each Word element from Rm or Rn depending on the corresponding P/Q/R/O bits in SR, or each DWord element depending on the corresponding T or S bit.

PSELT will select the Word from Rm if the corresponding bit is Set, selecting the Rn otherwise. PSELF will instead select from Rn if the bit is set, selecting from Rm otherwise.


=== PSHUF.x ===

* F2nm_8qjj  PSHUF.B	Rm, Imm8, Rn		//(GSV)
* F2nm_8Qjj  PSHUF.W	Rm, Imm8, Rn		//(GSV)
* FE F2nm_8Qjj  PSHUF.W	Rm, Imm33s, Rn		//(NNX)
* FF F2nm_8Qjj  PSHUF.W	Rm, Imm17s, Rn		//(NNX)

Packed Shuffle of Byte or Word elements.

The elements from Rm will be reordered using the mask given in the immediate, and the results will be stored in Rn.
In the case of a Byte shuffle, the high 32 bits will be copied unchanged.

Each element will be given as a 2-bit value, with 00 mapping to the low-order bits, and 11 to the high order bits. The elements within the mask will have the same relative order (in terms of bit-positions) as the elements in the register, and will serve to select an element at each position from the source register.


If a jumbo prefix is used with PSHUF.W, it will function as a combined shuffle and tristate multiply.
* imm( 7: 0): Shuffle Mask
* imm(15: 8): Tristate Multiply
* imm(31:16): Reserved (if not all 1s, sign-extended).
* imm(   32): 1


=== PSHUFX.L ===

*  PSHUFX.L	Rm, Imm8, Rn		//Synthetic

This instruction is synthetic.
Packed Shuffle of DWord element, operating on a logical 128-bit register.

This instruction will be built using MOVxxD ops.


=== PSCHxx.W ===

* F0nm_2goA  PSCHEQ.W	Rm, Ro, Rn		//(GSV)
* F0nm_2GoA  PSCHEQ.B	Rm, Ro, Rn		//(GSV)
* F0nm_2goB  PSCHNE.W	Rm, Ro, Rn		//(GSV)
* F0nm_2GoB  PSCHNE.B	Rm, Ro, Rn		//(GSV)

Checks if the elements in Rm match those in Ro, storing the index of the first match into Rn.

SR.T is updated based on whether or not any match was found.
SR.T will be set if the condition is true, and cleared if false.

PSCHEQ will update Rn and SR.T based on the first matching element.
PSCHNE will update Rn and SR.T based on the first non-matching element.

Elements may be either Packed Word or Packed Byte.
Elements will be numbered starting from the low-order bits.

For the Packed Byte operator, it will check up to 8 bytes.
For the Packed Word operator, it will check up to 4 words.


=== RGB Pack/Unpack ===

* F0nm_1g2E  RGB5PCK32		Rm, Rn		//(RGB) RGB555A Pack from RGBA32
* F0nm_1G2E  RGB5PCK64		Rm, Rn		//(RGB) RGB555A Pack from RGBA64
* F0nm_1g3E  RGB5UPCK32		Rm, Rn		//(RGB) RGB555A Unpack to RGBA32
* F0nm_1G3E  RGB5UPCK64		Rm, Rn		//(RGB) RGB555A Unpack to RGBA64
* F0nm_1G4E  RGB32PCK64		Rm, Rn		//(RGB) RGB32 Pack from RGBA64
* F0nm_1G5E  RGB32UPCK64	Rm, Rn		//(RGB) RGB32 Unpack to RGBA64

Pack or Unpack RGB values.
Pack will convert from a wider format to a narrower one.
Unpack will convert from a narrower format to a wider one.

RGB555A:
* 0rrrrrgggggbbbbb, Opaque
* 1rrrraggggabbbba, Translucent (3 bit Alpha)

Unpacking to RGB555A to RGB32 will mirror the high bits of each component into the low bits, so 43210 is expanded to 8 bits as 43210432. Note that the alpha-blended case will unpack RGB the same as in the opaque case, so 4321a becomes 4321a432.

When unpacking Alpha, the low order bits will be filled with zeroes.
In the case of opaque pixels (MSB is zero), all alpha bits will be set to ones.

Unpacking to RGBA64 will repeat each RGBA32 component twice, for all components.

Unpacking RGB555A to RGBA64 will behave as if the value were first unpacked to RGBA32 and then from RGBA32 to RGBA64.

Packing will do the reverse process, keeping only the high order bits and discarding the rest. When packing to RGB555A, if the high 4 bits of Alpha are Set, then the pixel will be treated as opaque (and MSB will be clear), otherwise the MSB will be set and the LSB of each component will contain the one of the high 3 bits of the Alpha channel.


=== RGB5SHR1 (RGB) ===

* F0nm_1g0E  RGB5SHR1		Rm, Rn		//(RGB) RGB555 Shift Right

Shift components in a packed RGB555 vector right by 1 bit.
Effectively, this is a combined 1-bit shift right operation and a bit-mask.


=== RGB5MINMAX (RGB?) ===

* F0nm_1G0E  RGB5MINMAX		Rm, Rn		//(RGB?) RGB555 Min and Max

Selects the Minimum and Maximum values from a vector of RGB555 values based on the approximate luminance of each color.

Within the result:
* (63:32): Reserved / Zeros
* (31:16): Minimum
* (15: 0): Maximum

The luminance function will depend on the implementation, but for example may be defined in terms of bit-interleving, say:
* G4, G3, R4, B4, G2, R3, B3, G1

Where, one may choose bit interleaving due to having less latency than converting 

The selected values will be implementation dependent in cases where multiple pixels have the same luminance.

This operation will be Lane 1 only.


=== RGB5CCENC (RGB?) ===

* F0nm_7go6  RGB5CCENC	Rm, Ro, Rn		//(RGB-CCE)

Color-Cell Encode Helper.
* Rm will give the pixel values to encode.
* Ro will give the Min and Max color endpoints.
* Rn will give both the source and destination for the selector bits.

This operation will classify the pixel colors relative to the endpoints, generating a selector between 00 (Minimum) and 11 (Maximum).

/ The low order 32 bits of Rn will be shifted right by 8 bits, with the new endpoint selectors being added to bits (31:24).

The low order 32 bits of Rn will be shifted left by 8 bits, with the new endpoint selectors being added to bits (7:0).

This operation will be Lane 1 only.


=== RGB5PCKI8 (RGB?) ===

* F0nm_1GCF ? RGB5PCKI8   Rm, Rn		//(RGB?) Pack RGB555 to 8-bit

Optional RGB. Pack four RGB555 endpoints into an indexed color in a fixed color palette.

The color palette will roughly follow the form:
* irgbyyyy (if yyyy>=0100)
* yyyy00cc

Where yyyy gives a luma, and irgb/cc gives chroma.
* irgb: Gives 16 colors, where i gives the saturation.
** 0rgb: High saturation (Hi=Y, Lo=1/4 Y)
** 1rgb: Low saturation (Hi=Y, Lo=5/8 Y)
** 0000: Grayscale
** 0111: Off-White Magenta (Hi=Y, Lo=7/8 Y)
** 1000: Off-White Yellow (Hi=Y, Lo=7/8 Y)
** 1111: Off-White Cyan (Hi=Y, Lo=7/8 Y)
* cc: Gives medium saturation colors:
** 00: RGBI (yyyy interpreted as a traditional 16-color palette)
** 01: Orange	(1/1, 5/8, 3/8)
** 10: Olive	(5/8, 1/1, 3/8)
** 11: Azure	(3/8, 5/8, 1/1)

Change:
* Replace Off-White wiith Orange/Olive/Azure
** 0111: Orange
** 1000: Olive
** 1111: Azure
* With cc now encoding off-white:
** 00: RGBI (yyyy interpreted as a traditional 16-color palette)
** 01: Off-White
*** ( 1.. 5)=Magenta (11..15)
*** ( 6..10)=Yellow (11..15)
*** (11..15)=Cyan (11..15)
** 10: Reserved
** 11: Reserved

=== RGB5SH3 / RGB5USH3 (RGB) ===

* F0nm_1g4E  RGB5SH3		Rm, Rn		//(RGB) RGB555 Shuffle-3
* F0nm_1g5E  RGB5USH3		Rm, Rn		//(RGB) RGB555 UnShuffle-3

The RGB5SH3 instruction shuffles the bits from a RGB555 value into an interleved 3-bit form:
* TRRRRRGGGGGBBBBB
* TGRBGRBGRBGRBGRB

The input register is assumed to contain 4 RGB555 values, which are all shuffled in the same way.

The RGB5USH3 instruction does the inverse process, converting the shuffled form back into the unshuffled form.



=== SWxP.x (GSV, Dropped) ===

* F0nm_3g07  SWAP.B		Rm, Rn		//(1032) Swap E/O Bytes (Low Four Bytes)
* F0nm_3g17  SWAP.W		Rm, Rn		//(1032) Swap E/O Words (All words)
* F0nm_3g27  SWAP.8B	Rm, Rn		//       Swap E/O Bytes (All Eight Bytes)
* F0nm_3g37  SWAP.L		Rm, Rn		//(2301) Swap DWord
* F0nm_3g47  SWAP.LB	Rm, Rn		//(1023) Swap Low Two Bytes (GSV)
* F0nm_3g57  SWAP.LW	Rm, Rn		//(1023) Swap Low Two Words (GSV)
* F0nm_3g67  SWCP.LB	Rm, Rn		//(0023) Copy Low Bytes (GSV)
* F0nm_3g77  SWCP.LW	Rm, Rn		//(0023) Copy Low Words (GSV)
* F0nm_3g87  SWAP.MB	Rm, Rn		//(2103) Swap Low and Middle Byte (GSV)
* F0nm_3g97  SWAP.MW	Rm, Rn		//(2103) Swap Low and Middle Word (GSV)
* F0nm_3gA7  SWCP.MB	Rm, Rn		//(0103) Copy Low To Middle Byte (GSV)
* F0nm_3gB7  SWCP.MW	Rm, Rn		//(0103) Copy Low To Middle Word (GSV)
* F0nm_3gC7  SWAP.HB	Rm, Rn		//(3120) Swap Low and High Byte (GSV)
* F0nm_3gD7  SWAP.HW	Rm, Rn		//(3120) Swap Low and High Word (GSV)
* F0nm_3gE7  SWCP.HB	Rm, Rn		//(0120) Copy Low To High Bytes (GSV)
* F0nm_3gF7  SWCP.HW	Rm, Rn		//(0120) Copy Low To High Word (GSV)

Swap or Copy elements within a register.

SWAP.x will exchange a the low element with another element, whereas SWCP.x will copy the low element into another element.

Other than the first element, the others will be labeled as Low, Middle, and High. Low will correspond to the lower-order bits of the register, and High will correspond with the higher order bits.

These will be deprecated in favor of PSHUF.B / PSHUF.W.


=== BITMOV (BITS) ===

* FEii_iiii_F0nm_2go3  BITMOV		Rm, Ro, Rn, Imm24
* FEii_iiii_F0nm_2Go3  BITMOVX		Xm, Ro, Xn, Imm24

* FFw0_08pp_F0nm_2go3  BITMOV		Rm, Ro, Rp, Rn
* FFw0_08pp_F0nm_2Go3  BITMOVX		Xm, Ro, Xp, Xn


* FEii_iiii_F0nm_2go2  BITMOVS		Rm, Ro, Rn, Imm24
* FEii_iiii_F0nm_2Go2  BITMOVSX		Xm, Ro, Xn, Imm24

* FFw0_08pp_F0nm_2go2  BITMOVS		Rm, Ro, Rp, Rn
* FFw0_08pp_F0nm_2Go2  BITMOVSX		Xm, Ro, Xp, Xn


Move a value from one bitfield to another.
The immediate field from the jumbo prefix is decoded as a single immediate.

The BITMOV instruction will move a bitfield while leaving copying all bits outside the masked region uchanged.

The BITMOVS instruction will selectively invert any bits outside the masked region based on the MSB of the masked value.


Imm24:
* ( 7: 0): Shift to Apply
* (15: 8): Mask Low Bit (L)
* (23:16): Mask High Bit (H)

The value in Rm is shifted and then combined with the value in Ro according to the mask encoded in the L and H fields of the immediate.

In the 4R variant, the value in Rm is shifted and combined with the value from Rp with Ro giving the shift and mask in the same format as the Imm24.

For mask generation:
* L: Generates 1s for bits GE L;
* H: Generates 1s for bits LT H;
* H GT L: Combine using AND
* H LE L: Combine using OR 

The shift will be encoded as a signed 8 bit shift from -63 to 63 for BITMOV or -127 to 127 for BITMOVX. Positive values encode left shifts and negative values encode right shifts.


=== BITSEL (BITS) ===

* F0nm_6goB  BITSEL		Rm, Ro, Rn			//(?) Rn = (Rm & Ro) | (Rn & ~Ro)
* F0nm_6GoB  BITSELX	Xm, Xo, Xn			//(?) Xn = (Xm & Xo) | (Xn & ~Xo)

Generate a value combining bits from Rm and Rn using a mask given ib Ro.


=== BLERP (RGBX) ===

* F0nm_6go9            BLERP	Rm, Ro, Rn		//(RGBX) Linear Interpolate
* FFw0_00pp_F0nm_6go9  BLERPx	Rm, Ro, Rp, Rn	//(RGBX) Linear Interpolate

Linearly interpolate between two vectors.
* The vectors will be interpreted as RGBA64 or Packed UInt16.
* Rm and Ro will give the vectors to interpolate.
* Rn (3R) or Rp (4R) will give the interpolated value.
** The 3R form will use bits (15:0) as the interplator.
** The 4R will use bits (7:6) of the pp field to select an interpolator:
*** 00: Bits (15: 0) (BLERPS)
*** 01: Bits (31:16) (BLERPSH)
*** 10: Bits (47:32) (BLERPT)
*** 11: Bits (63:48) (BLERPTH)

The LERP of each component will be interpreted as:
* ValRm*(~Interp) + ValRo*Interp
* With both components treated as Unsigned.


=== BLKUAB (UAB) ===

* F0nm_6goC  BLKUAB1	Rm, Ro, Rn
* F0nm_6GoC  BLKUAB2	Rm, Ro, Rn

Extract an audio sample from a block.

These blocks will come in two formats, UAB1 and UAB2.
The UAB1 format will store 16 samples in 32 bits.
The UAB2 format will store 16 samples in 64 bits.

The block is given in Rm, with a sample index in Ro.

The result is unpacked to 16-bit linear PCM and stored in Rn.

For UAB1, the output will be encoded as:
* (15: 0): Sample Value
* (31:16): Sample Value (Repeat)

For UAB2, the output will be encoded as:
* (15: 0): Sample Value (Left)
* (31:16): Sample Value (Right)


UAB1:
* ( 5: 0): Line Start (S.E3.F2)
* (11: 6): Line End   (S.E3.F2)
* (15:12): Line Sigma (  E3.F1)
* (31:16): Sample Selector Bits

UAB2:
* ( 7: 0): Line Start (S.E3.F4)
* (15: 8): Line End   (S.E3.F4)
* (23:16): Line Sigma (Z.E3.F4)
* (31:24): Left/Right Bias (Zero for Mono)
* (63:32): Sample Selector Bits

The block will represent a line segment given as a start and end endpoint.
To decode a sample, linearly interpolate between the start and end value via the sample index.

The Sigma will represent a distance above or below this line segment, which will be either added to or subtracted from the interpolated value (based on 
the selector bits) to get the result.

The line sigma will always be positive for a UAB2 block. The use of a negative Sigma will be reserved, and may be used to escape to another block format.

The Left/Right bias will represent a constant bias which will be added to the left channel and subtracted from the right channel.


Selector bits are organized with the first sample in the low order bits, and the last sample in the high bits.

For each selector, 1 Bit:
* 0: V=Pred-Sigma
* 1: V=Pred+Sigma

For each selector, 2 Bits:
* 00: Small Negative (-0.333 * Sigma)
* 01: Large Negative (-1.000 * Sigma)
* 10: Small Positive (+0.333 * Sigma)
* 11: Large Positive (+1.000 * Sigma)

Endpoints are encoded as a microfloat value which expands into a 12-bit integer format (PCM):
0: 00000000xxxx
1: 00000001xxxx
2: 0000001xxxx0
3: 000001xxxx00
4: 00001xxxx000
5: 0001xxxx0000
6: 001xxxx00000
7: 01xxxx000000

Negative values will invert the value of the decoded sample.

Interpolation and other calculations will be performed in terms of PCM values.
An encoder should ensure that decoded values will not go out of range.


=== BLKUTX1 (RGB) ===

* F0nm_2goC  BLKUTX1	Rm, Ro, Rn		//(RGB)

Extract pixels from blocks encoded in UTX1.

UTX1 will encode 16 pixels in 32-bits (2bpp).

The Rm field will give the pixel block, and Ro will give an index into this block. Only the low order bits of Ro will be used, with any other bits being ignored.

The output given in Rn will be in unpacked RGBA64 form, with the alpha channel set as FFFF.

For UTX1, the colors endpoints will be given as an RGB444 center, and a 4-bit Delta (YMax-YMin), with 16-bits for pixel data: PPPPDRGB.

The RGB components and delta will be expanded to 8 bits by repeating the nybble.

The Delta will be used to calculate the minimum and maximum colors, and will need to be chosen such that the RGB components do not overflow or underflow.

Where:
* Cm=Cc-(D>>1);		//Minimum
* Cn=Cm+D;			//Maximum
* Repeated for all 3 components.

Within the pixel bits, 0 will select the minimum color, and 1 will select the maximum color.


=== BLKUTX2 (RGB) ===

* F0nm_2GoC  BLKUTX2	Rm, Ro, Rn		//(RGB)

Extract pixels from blocks encoded in UTX2.

UTX2 will encode 16 pixels in 64-bits (4bpp).

The Rm field will give the pixel block, and Ro will give an index into this block. Only the low order bits of Ro will be used, with any other bits being ignored.

The output given in Rn will be in unpacked RGBA64 form.

For UTX2, a pair of RGB555A endpoints will be given:
* (15: 0): Color A
* (31:16): Color B
* (63:32): Pixel Bits (4x4, 2bpp)

The alpha bits of the first and second endpoint will encode a mode (A,B):
* 00: Opaque Block, Linear Interpolation.
** Both endpoints will be interpreted as being Opaque.
* 01: Bit-Select Alpha
** Both endpoints will be interpreted as having Alpha.
* 10: Reserved
* 11: Translucent Block, Linear Interpolation.
** Both endpoints will be interpreted as having Alpha.

With linear interpolation, for each 2 bit pixel:
* 00: ColorB
* 01: 2/3*ColorB + 1/3*ColorA
* 10: 1/3*ColorB + 2/3*ColorA
* 11: ColorA

For interpolated translucent blocks, the alpha will be interpolated along with the color.


For bit select alpha, the high bit will be interpreted as selecting the color, with the low bit selecting the alpha. No interpolation will be performed.

With bit-selection, for each 2 bit pixel:
* 00: ColorB, AlphaB
* 01: ColorB, AlphaA
* 10: ColorA, AlphaB
* 11: ColorA, AlphaA

Pixel ordering will be such that 0 refers to the pixel at the low order bit position, and 15 at the high order bit position. These operations will not care about the internal pixel ordering within the block (which may be either raster or morton).


=== BLKUTX3 (RGBX) ===

* F0nm_6Go8  BLKUTX3H	Xm, Ro, Rn			//(RGBX) Extract Pixel, UTX3 HDR
* F0nm_6Go9  BLKUTX3L	Xm, Ro, Rn			//(RGBX) Extract Pixel, UTX3 LDR

Extract a texel from UTX3.

The UTX format will come in two sub-variants:
** LDR, with RGBA32 endpoints.
** HDR, with with FP8U endpoints in place of linear 8-bit values.

Basic Format:
* ( 31: 0): ColorA
* ( 63:32): ColorB
* ( 95:64): Selectors (RGB)
* (127:96): Selectors (Alpha)

Selectors for RGB and Alpha will be 4x4x2:
* 00: ColorB
* 01: 2/3*ColorB + 1/3*ColorA
* 10: 1/3*ColorB + 2/3*ColorA
* 11: ColorA


=== BLKUVF1 (?) ===

* FFw0_0040_F0nm_7eoC  BLKUVF1VL	Rm, Ro, Rn
* FFw0_0041_F0nm_7eoC  BLKUVF1VH	Rm, Ro, Rn
* FFw0_0042_F0nm_7eoC  BLKUVF1SL	Rm, Ro, Rn
* FFw0_0043_F0nm_7eoC  BLKUVF1SH	Rm, Ro, Rn


Basic Format:
* (  7: 0): ValueA
* ( 15: 8): ValueB
* ( 31:16): Sign Bits
* ( 63:32): Selectors

Selectors will be 4x4x2:
* 00: ValueB
* 01: 2/3*ValueB + 1/3*ValueA
* 10: 1/3*ValueB + 2/3*ValueA
* 11: ValueA

Ops 'VL'/'SL' will interpret the value as a byte.
Ops 'VH'/'SH' will interpret the value as FP8U.

Ops 'VL'/'VH' will interpret the block as four 4-element vectors.
Ops 'SL'/'SH' will interpret the block as 16 scalar values.



== GFP Instructions (GPR FPU Ops, Optional) ==

GFP will perform FPU operations with GPRs.

The BJX2 FPU aims more to be "sufficient" and "reasonably cheap" than strict adherence to IEEE 754. An FPU is allowed, but not required, to be IEEE 754 compliant.

These will reuse the existing IEEE formats, with some noted divergences:
* Rounding should be within +/- 2 ULP (rather than 0.5 ULP).
** No user-defined rounding behavior is provided for.
* The exact handling of denormals is undefined.
** Zero is to behave as Zero.
** Non-Zero denormals may behave either as zero or some tiny non-zero value.
* The results of operations involving NaN or Inf values are undefined.
** Operations on these values giving garbage results will be acceptable.
* Inf vs NaN need only check the high 4 bits of the mantissa.
** If these bits are 0, Inf may be assumed regardless of the other bits.
* Divide and Square Root are omitted.
* Handling of range overflow or underflow is required:
** If the operation exceeds the valid range, an Inf is produced.
** If the value underflows, a zero will be produced.

The nominal format of the BJX2 FPU is 64-bit Double.


There will be a second level of FPU instructions under the name GFPX:
* Adds LongDouble / Binary128
** LongDouble will nominally be defined as truncated Binary128.
** Most FPU ops are ignore the low-order bits.
** Operations which ignore the low-order bits are to zero them in results.
* Rounding is tightened to 0.51 ULP for base ops.

Another Extension, GFP_MAC, will add FMAC:
* Adds FMAC


For now, the "baseline" Binary128 support will define (Old):
* 72-bit mantissa (for the portion treated as significant).
* Though, 80-bit mantissa will be considered nominal.
* Full 15 bit exponent range.

Will define the GFPX instructions as using LongDouble (New):
* Basic representation is the same as Binary128.
** The mantissa will nominally be 80 bits.
** The actual number of mantissa bits will be implementation dependent.
* Nominally, the low bits for normal FPU operations will be ignored.
* The 32-bits of the result are to be set to zero.
* Some GFPX operations may use the full width mantissa.
** An implementation may, optionally, implement full Binary128.
** FADD, FSUB, and FMUL should use the same number of bits.


=== FABS (GFP) ===

* F0nm_1e9D  FABS		Rm, Rn		//Absolute

Get the absolute value of the double precision value in Rm and store the result in Rn. This effectively amounts to clearing bit 63 and leaving the other bits unchanged.


=== FADD (GFP) ===

* 60nm                 FADD		Rm, Rn				//(GFP)
* F0nm_5go8            FADD		Rm, Ro, Rn			//(GFP)
* F0nm_5Go8            FADDX	Xm, Xo, Xn			//(GFPX)
* F0nm_6goD            FADDG	Rm, Ro, Rn			//(GFP)
* F0nm_7go0          ? FADDA	Rm, Ro, Rn			//(GFP)
* F0nm_6GoD          ? FADD		Rm, Imm5fp, Rn		//(FpImm)
* F2nE_DHjj          ? FADD		Imm10fp, Rn			//(FpImm) (E.q=1) FADD
* FEjj_jjjj_F2nm_1Pjj  FADD		Rm, Imm32f, Rn		//(FPIMM)
* FEjj_jjjj_F2nm_1Qjj  FADDA	Rm, Imm32f, Rn		//(FPIMM)
* FFw0_08ii_F0nm_5go8  FADD		Rm, Ro, Rn, Imm8	//(GFP)
* FFw0_08ii_F0nm_5Go8  FADDX	Xm, Xo, Xn, Imm8	//(GFPX)

* FE-FE F2nm_1Gjj      FADD		Rm, Imm56f, Rn		//(FPIMM)
* FE-FE F2nm_3Gjj   /? FADD		Rm, Imm56f, Rn		//(FPIMM) Deprecated

Add the double precision value in Ro to the value from Rm and store the resulting value in Rn.

FADDX will operate instead on a LongDouble value.
This may be partial precision, with the range of mantissa bits supported being implementation defined. If supported, the minimum will be 52 bits.


For FADD vs FADDG vs FADDA:
* FADD will be hard-wired for Round to Nearest (RM=00).
* FADDG will use a rounding mode from the FPSCR sub-register.
* FADDA may be hard-wired for Single (RM = 08 or 09)
** The intention for FADDA is to be a speed hint for 'float'.
** Not to give accurate single-precison rounding.
* These rules also apply to FSUB and FMUL variants.



In the Op64 case, the Imm8 may encode a rounding mode:
* 00: Round to Nearest
* 01: Truncate / Round towards zero.
* 02: Round toward +Inf
* 03: Round toward -Inf
* 04: Round, ULP+2, Inexact Status

* 08: Round to Nearest (Single)
* 09: Truncate Towards Zero (Single).
* 0A: Round toward +Inf (Single)
* 0B: Round toward -Inf (Single)

In rounding mode 4, the ULP will be moved upwards by 2 bits.
The low 2 bits of inputs and results will be treated as an inexact status.
If these bits hold 0, the result is exact, otherwise the result is inexact. This state will be sticky.


Note that the FPU will not guerantee a strict +/- 0.5 ULP rounding behavior for Binary64, merely a "reasonable effort" rounding.


Behavior of the Single Modes may depend on implementation:
* May be equivalent to the basic modes (if unsupported).
* May round as-if the value were single precision.
* May serve as a hint to use a faster but less precise FPU.
* The FPU is to give roughly similar precision to Binary32 if available.
* Rounding behavior is weakly defined under these modes.
** May either perform rounding as requested.
** Or may always give nearest or truncate rounding.
** Rounding behavior may depend on implementation.


The Imm5fp and Imm10fp encodings will be part of the FpImm extension.

In the Imm5fp case, the value will represent an E3.F2 microfloat.
These will be decoded as-if they were first expanded to Binary16 as:
* 000xx: 3000, 3100, 3200, 3300 ( 0.125,  0.156,  0.188,  0.219)
* 001xx: 3400, 3500, 3600, 3700 ( 0.250,  0.313,  0.375,  0.438)
* 010xx: 3800, 3900, 3A00, 3B00 ( 0.500,  0.625,  0.750,  0.875)
* 011xx: 3C00, 3D00, 3E00, 3F00 ( 1.000,  1.250,  1.500,  1.750)
* 100xx: 4000, 4100, 4200, 4300 ( 2.000,  2.500,  3.000,  3.500)
* 101xx: 4400, 4500, 4600, 4700 ( 4.000,  5.000,  6.000,  7.000)
* 110xx: 4800, 4900, 4A00, 4B00 ( 8.000, 10.000, 12.000, 14.000)
* 111xx: 4C00, 4D00, 4E00, 4F00 (16.000, 20.000, 24.000, 28.000)

In XG2 and XG3, Imm6fp will exist. The format is basically the same as Imm5fp, except that WI will encode a sign bit.

The Imm10fp case will be be encoded as (S.E5.F4).
This is effectively a truncated form of Binary16, and may be converted to the Binary16 format by adding 6 zero bits onto the bottom.

In both the Imm5fp and Imm10fp cases, the value will be decoded as-if it were first converted to Binary16 and then converted to Binary64 using similar behavior as the FLDCH instruction.

Some encodings may encode an Imm32f. This encoding will basically represent the value as a Binary64 with the low 32 bits cut off (S.E11.F20).

In WEX profiles without FP2, FADD/FSUB/FMUL will be Lane 1 only.
* In profiles with FP2, they may be issued from Lane 2.
* FPU ops may not be co-issued with other FPU ops.


=== FCMPEQ (GFP) ===

* 64nm                 FCMPEQ		Rm, Rn			//(GFP) FCMPEQ
* F0nm_1gAD            FCMPEQ		Rm, Rn			//SR.T=(Rn EQ Rm)
* F0nm_1GAD            FCMPXEQ		Rm, Rn			//SR.T=(Rn EQ Rm)
* F2nC_Dhjj            FCMPEQ		Imm10fp, Rn		//(FpImm) (E.q=0) Rn==Imm
* FEjj_jjjj_F2nC_Dhjj  FCMPEQ		Imm32f, Rn		//(FpImm)
* FE   FE   F2nC_Dhjj  FCMPEQ		Imm56f, Rn		//(FpImm)

Compare the double precision values in Rm and Rn for equality, setting SR.T if both values are equal, otherwise clearing SR.T.

If either value encodes a NaN, SR.T will be cleared.

For Imm10fp and Imm32f, see FADD.

=== FCMPGE (GFP) ===

* F2nF_Dhjj            FCMPGE		Imm10fp, Rn		//(FpImm) (E.q=0) FCMPGE
* FEjj_jjjj_F2nF_Dhjj  FCMPGE		Imm32f, Rn		//(FpImm) (E.q=0) FCMPGE
* FE   FE   F2nF_Dhjj  FCMPGE		Imm56f, Rn		//(FpImm)

Compare if the double precision value in Rn is greater than or equal to the value in Rm, setting SR.T if this is the case, and clearing otherwise.

If either value is NaN, the resulting state of SR.T is undefined.

For Imm10fp and Imm32f, see FADD.



=== FCMPGT (GFP) ===

* 65nm                 FCMPGT		Rm, Rn			//(GFP) FCMPGT
* F0nm_1gBD            FCMPGT		Rm, Rn			//SR.T=(Rn GT Rm)
* F0nm_1GBD            FCMPXGT		Rm, Rn			//SR.T=(Rn GT Rm)
* F2nE_Dhjj          ? FCMPGT		Imm10fp, Rn		//(FpImm) (E.q=0) FCMPGT
* FEjj_jjjj_F2nE_Dhjj  FCMPGT		Imm32f, Rn		//(FpImm)
* FE   FE   F2nE_Dhjj  FCMPGT		Imm56f, Rn		//(FpImm)

Compare if the double precision value in Rn is greater than the value in Rm, setting SR.T if this is the case, and clearing otherwise.

If either value is NaN, the resulting state of SR.T is undefined.

For Imm10fp and Imm32f, see FADD.


=== FCONV (GFPX, Drop) ===

* F2nm_8qjj  PCONV		Rm, Imm8, Rn		//E.i=1, Packed Convert
* F2nm_8Qjj  PCONVX		Rm, Imm8, Rn		//E.i=1, Packed Convert

Packed convert vectors.
The E.q parameter selects between the use of 64 and 128 bit vectors.
The vector width will be the minumum of the two vector widths.
Each vector width will be the vector size divided by the element size.
Valid encodings will be limited to a maximum of 4 elements.

Conversion between floating point and integer types will produce results scaled as integers. Conversion to integer types will be modular.

The Unit cases will be scaled to unit range (0.0 to 1.0).

Imm8:
* (3:0)=Source Element Type
* (7:4)=Destination Element Type

Types:
* 0000: Binary16
* 0001: Binary32
* 0010: Binary64
* 0011: Binary128
* 0100: Int16
* 0101: Int32
* 0110: Int64
* 0111: -
* 1000: Unit16
* 1001: Unit32
* 1010: Unit64
* 1011: -
* 1100: UInt16
* 1101: UInt32
* 1110: UInt64
* 1111: -


=== PCVTxx (GSVF) ===

* F0nm_1g0F  PCVTSB2HL	Rm, Rn		//(GSVF) Packed SB to FP16 (Low32)
* F0nm_1G0F  PCVTUB2HL	Rm, Rn		//(GSVF) Packed UB to FP16
* F0nm_1g1F  PCVTSB2HH	Rm, Rn		//(GSVF) Packed SB to FP16 (High32)
* F0nm_1G1F  PCVTUB2HH	Rm, Rn		//(GSVF) Packed UB to FP16
* F0nm_1g2F  PCVTSW2FL	Rm, Rn		//(GSVF) Packed SW to FP32 (Low32)
* F0nm_1G2F  PCVTUW2FL	Rm, Rn		//(GSVF) Packed UW to FP32
* F0nm_1g3F  PCVTSW2FH	Rm, Rn		//(GSVF) Packed SW to FP32 (High32)
* F0nm_1G3F  PCVTUW2FH	Rm, Rn		//(GSVF) Packed UW to FP32

* F0nm_1g4F  PCVTH2SB	Rm, Rn		//(GSVF) Packed FP16 to SB (4x)
* F0nm_1G4F  PCVTH2UB	Rm, Rn		//(GSVF) Packed FP16 to UB (4x)
* F0nm_1g5F  PCVTSW2H	Rm, Rn		//(GSVF) Packed SW to FP16 (4x)
* F0nm_1G5F  PCVTUW2H	Rm, Rn		//(GSVF) Packed UW to FP16 (4x)
* F0nm_1g6F  PCVTF2SW	Rm, Rn		//(GSVF) Packed FP32 to SW (2x)
* F0nm_1G6F  PCVTF2UW	Rm, Rn		//(GSVF) Packed FP32 to UW (2x)
* F0nm_1g7F  PCVTH2SW	Rm, Rn		//(GSVF) Packed FP16 to SW (4x)
* F0nm_1G7F  PCVTH2UW	Rm, Rn		//(GSVF) Packed FP16 to UW (4x)

* F0nm_1gCF ? PCVTH2AL	Rm, Rn		//(GSVF?) Packed FP16 to A-Law (4x)
* F0nm_1gDF ? PCVTAL2H	Rm, Rn		//(GSVF?) Packed A-Law to FP16 (4x)

Perform packed conversion between floating point and integer types.

Note that the packed integer operations will perform a slightly atypical conversion in that they will map the converted value between 1.0000 and 1.9999 for unsigned and 2.0000 and 3.9999 for signed. One may add a bias of 1.0 or 3.0 to get the value back into unit range.


The packed A-Law will represent values in an S.E3.F4 format, and will map over to unit range values (-1.0 to 1.0) on the FP16 side of things. Exponents above 1.0 will be clamped to 0x7F or 0xFF.

While standard A-Law also has the values XOR'ed with 0x55, this will not be performed by this operation.


=== FDIVx (GFP, FDIV) ===

* F0nm_6go6 FDIV		Rm, Ro, Rn			//(FDIV) FDIV
* F0nm_6Go6 FDIVX		Rm, Ro, Rn			//(FDIV+GFPX)
* F0nm_6go7 FDIVA		Rm, Ro, Rn			//(FDIV) FDIV (Approx)
* F0nm_6Go7 FDIVXA		Rm, Ro, Rn			//(FDIV+GFPX) (Approx)

Perform a Floating-Point Divide.
Divides Rm by Ro and store the result in Rn.

The FDIVA and FDIVXA will perform a fast approximation without waiting for the result to converge on the correct answer.

This instruction will be part of the FDIV extension.

This operation will be Lane 1 only.


=== FLDCx (GFP) ===

* 63nm       FLDCF		Rm, Rn		//(GFP) Single->Double
* F0nm_1g0D  FLDCF		Rm, Rn		//Load Convert Float32 (Low Bits, ZX)
* F0nm_1g1D  FLDCHF		Rm, Rn		//Load Convert Float32 (High Bits)
* F0nm_1g2D  FLDCI		Rm, Rn		//Load Convert Int64
* F0nm_1g3D  FLDCH		Rm, Rn		//Load Convert Half (Low16)

* F0nm_1gFD ? FLDCIU	Rm, Rn		//Load Convert UInt64
* F0nm_1gEF ? FLDCIS.L	Rm, Rn		//Load Convert Int32
* F0nm_1gFF ? FLDCIU.L	Rm, Rn		//Load Convert UInt32

Load and convert the value from Rm and store the resulting double precision value in Rn.

FLDCF will pull a single precision value from the low 32 bits of Rm.
FLDCHF will pull a single precision value from the high 32 bits of Rm.

FLDCI will pull a 64-bit integer value from Rm.
FLDCH will pull a 16-bit half-float value from Rm.

When converting a narrower floating point type to a wider one:
* The low order bits will be filled with zeroes.
* The exponent will be widened with appropriate bias adjustment.
* A zero exponent will be copied as 0.
** Mantissa bits will be copied diectly.
** Essentially, denormals are ignored.
* An Inf or NaN exponent will be copied as its equivalent.
** Mantissa bits still copied directly.

For FLDCI variants:
* FLDCIU will interpret Rm as Unsigned.
* FLDCIU.L will interpret Rm as an unsigned 32-bit integer.
* FLDCIS.L will interpret Rm as a signed 32-bit integer.


=== FLDCx Imm (GFP) ===

* F88n_iiii  FLDCH		Imm16u, Rn
* F89n_iiii  FLDCH		Imm16u, Rk

* FFw0_iiii_F88n_iiii  FLDCF	Imm32u, Rn		//(Op64)
* FFw0_iiii_F89n_iiii  FLDCF	Imm32u, Rk		//(Op64)

Load a floating point immediate and convert to Double.
Conversion will be the same as that for the register instruction.


=== FMAC (FMAC/GFPX) ===

* F0nm_5goB            FMAC		Rm, Ro, Rn			//(FMAC) FMAC, Rn+=Rm*Ro
* F0nm_5GoB            FMACX	Xm, Xo, Xn			//(GFPX) FMAC, LongDbl

* FFw0_00pp_F0nm_5goB  FMAC		Rm, Ro, Rp, Rn		//(FMAC) FMAC, Rn=Rm*Ro+Rp
* FFw0_00pp_F0nm_5GoB  FMACX	Xm, Xo, Xp, Xn		//(GFPX) FMAC, LongDbl
* FFw0_01pp_F0nm_5goB  FMAS		Rm, Ro, Rp, Rn		//(FMAC) FMAS, Rn=Rm*Ro-Rp
* FFw0_01pp_F0nm_5GoB  FMASX	Xm, Xo, Xp, Xn		//(GFPX) FMAS, LongDbl
* FFw0_02pp_F0nm_5goB  FMRS		Rm, Ro, Rp, Rn		//(FMAC) FMRS, Rn=Rp-Rm*Ro
* FFw0_02pp_F0nm_5GoB  FMRSX	Xm, Xo, Xp, Xn		//(GFPX) FMRS, LongDbl
* FFw0_03pp_F0nm_5goB  FMRA		Rm, Ro, Rp, Rn		//(FMAC) FMRA, Rn=-Rm*Ro-Rp
* FFw0_03pp_F0nm_5GoB  FMRAX	Xm, Xo, Xp, Xn		//(GFPX) FMRA, LongDbl

* FFw0_04pp_F0nm_5goB  PMAC.H	Rm, Ro, Rp, Rn		//(NNX) PMAC, 4x Fp16
* FFw0_04pp_F0nm_5GoB  PMAC.F	Xm, Xo, Xp, Xn		//(NNX) PMAC, 4x Fp32
* FFw0_05pp_F0nm_5goB  PMAS.H	Rm, Ro, Rp, Rn		//(NNX) PMAS, 4x Fp16
* FFw0_05pp_F0nm_5GoB  PMAS.F	Xm, Xo, Xp, Xn		//(NNX) PMAS, 4x Fp32
* FFw0_06pp_F0nm_5goB  PMRS.H	Rm, Ro, Rp, Rn		//(NNX) PMRS, 4x Fp16
* FFw0_06pp_F0nm_5GoB  PMRS.F	Xm, Xo, Xp, Xn		//(NNX) PMRS, 4x Fp32
* FFw0_07pp_F0nm_5goB  PMRA.H	Rm, Ro, Rp, Rn		//(NNX) PMRA, 4x Fp16
* FFw0_07pp_F0nm_5GoB  PMRA.F	Xm, Xo, Xp, Xn		//(NNX) PMRA, 4x Fp32

Multiply the double precision value in Ro with the value from Rm and add this to the value in Rn, storing the result in Rn.

The four register form will add against the value from Rp, storing the result in Rn.

FMACX will operate instead on a LongDouble value (partial precision).


=== FMUL (GFP) ===

* 62nm                 FMUL		Rm, Rn			//(GFP) FMUL
* F0nm_5goA            FMUL		Rm, Ro, Rn		//(GFP) FMUL
* F0nm_5GoA            FMULX	Xm, Xo, Xn		//(GFPX) FMUL
* F0nm_6goF            FMULG	Rm, Ro, Rn		//(GFP) FMUL
* F0nm_7go2          ? FMULA	Rm, Ro, Rn		//(GFP)
* F0nm_6GoF          ? FMUL		Rm, Imm5fp, Rn	//(FpImm)
* F2nF_DHjj          ? FMUL		Imm10fp, Rn		//(FpImm) (E.q=1) FMUL
* FEjj_jjjj_F2nm_9pjj / FMUL	Rm, Imm32f, Rn		//(NNX)
* FEjj_jjjj_F2nm_4Pjj ? FMUL	Rm, Imm32f, Rn		//(FPIMM)
* FEjj_jjjj_F2nm_4Qjj ? FMULA	Rm, Imm32f, Rn		//(FPIMM)
* FFw0_08ii_F0nm_5goA  FMUL		Rm, Ro, Rn, Imm8	//(GFP)
* FFw0_08ii_F0nm_5GoA  FMULX	Xm, Xo, Xn, Imm8	//(GFPX)

* FE-FE F2nm_2Gjj     / FMUL		Rm, Imm56f, Rn		//(FPIMM)
* FE-FE F2nm_4Gjj      FMUL		Rm, Imm56f, Rn		//(FPIMM)

Multiply the double precision value in Ro with the value from Rm and store the resulting value in Rn.

FMULX will operate instead on a LongDouble value.

The basic forms of FMUL will be fixed at Round-Nearest.
The FMULG form will use a dynamic rounding mode.

For Imm5fp or Imm10fp, see FADD.
For Lane use rules, see FADD.


=== FNEG (GFP) ===

* F0nm_1g8D  FNEG		Rm, Rn		//Negate

Negate the value of the double precision value in Rm and store the result in Rn. This effectively amounts to inverting bit 63 and leaving the other bits unchanged.



=== FSQRTx (GFP, FDIV) ===

* F0nm_1gCD  FSQRT		Rm, Rn		//(FDIV) Square Root
* F0nm_1GCD  FSQRTX		Xm, Xn		//(FDIV) Square Root (GFPX)
* F0nm_1gDD  FSQRTA		Rm, Rn		//(FDIV) Square Root (Approx)
* F0nm_1GDD  FSQRTXA	Xm, Xn		//(FDIV) Square Root (GFPX, Approx)

Perform a Floating-Point Square Root.

The FSQRTA and FSQRTXA forms will perform a fast approximation without waiting for the result to converge on the correct answer.

Note that SQRT will be defined as a Signed Square Root, as-in, the output sign will be equivalent to the input sign.

This instruction will be part of the FDIV extension.

This instruction will be Lane 1 only.


=== FSTCx (GFP) ===

* 66nm       FSTCF		Rm, Rn		//(GFP) Double->Single
* F0nm_1e4D  FSTCF		Rm, Rn		//Store Convert Float32 (Low Bits, ZX)
* F0nm_1e5D  FSTCHF		Rm, Rn		//Store Convert Float32 (High Bits)
* F0nm_1e6D  FSTCI		Rm, Rn		//Store Convert Int
* F0nm_1e7D  FSTCH		Rm, Rn		//Store Convert Half (Low16)

Convert the double precision value from Rm into the destination value and store the result in Rn.

FSTCF will produce a single precision float, which will be stored into the low 32 bits of the destination register. The high 32 bits of the register will be set to zero.

FSTCHF will produce a single precision float, which will be stored into the high 32 bits of the destination register. The low 32 bits of Rn will be kept unmodified.

FSTCI will produce a 64-bit integer output with truncation towards zero.

FSTCH will produce a half precision float, which will be stored into the low 16 bits of the destination register. The high 48 bits of the register will be set to zero.

Conversion to integer will truncate towards zero. Its output will be a 64-bit integer with undefined results if the value overflows the allowed range.


=== FSUB (GFP) ===

* 61nm                 FSUB		Rm, Rn			//(GFP) FSUB
* F0nm_5go9            FSUB		Rm, Ro, Rn		//(GFP) FSUB
* F0nm_5Go9            FSUBX	Rm, Ro, Rn		//(GFPX) FSUB
* F0nm_6goE            FSUBG	Rm, Ro, Rn		//(GFPX) FSUB
* F0nm_7go1          ? FSUBA	Rm, Ro, Rn		//(GFP)
* F0nm_6GoE          ? FSUB		Rm, Imm5fp, Rn		//(FpImm)
* FFw0_08ii_F0nm_5go9  FSUB		Rm, Ro, Rn, Imm8	//(GFP)
* FFw0_08ii_F0nm_5Go9  FSUBX	Xm, Xo, Xn, Imm8	//(GFPX)

Subtract the double precision value in Ro from the value from Rm and store the resulting value in Rn.

FSUBX will operate instead on a LongDouble value (if GFPX is available).

For Imm5fp, see FADD.
For Lane use rules, see FADD.


== Jumbo Instructions ==

Moved to core ISA Spec.


=== JLDI (Jumbo Load) ===

Jumbo Load is an "instruction" which loads a 64-bit constant all at once.

Basic Encoding:
* FEjj_jjjj-F2n0_Cgjj            LDIZ		Imm32u, Rn
* FEjj_jjjj-F2n1_Cgjj            LDIN		Imm32n, Rn
* FEzz_zzzz-FEyy_yyyy-F80n_xxxx  JLDI		Imm64, Rn
* FEjj_jjjj-F2n3_Cgjj            LDIHI.L	Imm32u, Rn		//Rn=Imm32u<<16
* FEjj_jjjj-F2n3_CGjj            LDIHI.Q	Imm32u, Rn		//Rn=Imm32u<<32

The immediate fields for JLDI will be interpreted as a 64-bit value:
* zzzz_zzyy_yyyy_xxxx
* Or, essentially, the three immedite fields glued together.

JLDI will not need one or zero extension given the immediate fills an entire 64-bit register.

Jumbo LDIHI will load a 32-bit immediate with a constant shift. The Q bit will encode whether to shift the immediate left by 16 or 32 bits.

