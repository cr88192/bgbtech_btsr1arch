
== Overview ==

BJX2
* Variable-Length Instructions (16/32/64/96)
* Derived from BSR1 and BJX1-64C
** Little-Endian, Byte Addressed
** Instructions as a sequence of 16-bit words.

BJX2D
* More Cleanup / Prune
* Drop 48-bit ops
* GFP is now the main FPU
* Promote WEX2 to Core
** An implementation which ignores WEX2 is still valid.
* Change encoding for Jumbo
** Jumbo will be promoted to part of the base ISA.
** Jumbo forms will be reclassified as 64/96 bit instructions.

=== Profiles ===

Multiple subset profiles will exist, and some features will be optional.

Several ISA Profiles may exist:
* A: Full, Full Features, 48-bit address space.
* B: Fix32 + GFP, 32-bit physical space
* C: Basic, Partial Features, 32-bit address space.
* D: Fix32 + SoftFPU, 32-bit physical space
* E: Lite + SoftFPU, 32-bit physical space
* F: Lite + GFP, 32-bit physical space
* G: Full Features, 96-bit address space and XGPR.
* H: Full Features, similar to G, but assumes a 48-bit Address Space
* I: Basic, 48-bit Address Space, GFP+GSV+GSVF

The compiler is not to use features which are absent in a given profile, and will not assume the presence of features marked as optional unless specified as part of the current pofile.

Features assumed:
* A: GFP, JUMBO, MOVX, GSV, GSVF, GSFVX, FMOVS, WEX3W
* B: FIX32, GFP
* C: GFP
* D: FIX32
* E: None
* F: GFP
* G: GFP, JUMBO, MOVX, GSV, GSVF, GSFVX, FMOVS, XGPR, XMOV, WEX3W
* H: GFP, JUMBO, MOVX, GSV, GSVF, GSFVX, FMOVS, XGPR, XMOV, WEX3W
* I: GFP, JUMBO, MOVX, GSV, GSVF, GSFVX, WEX2W


==== BJX2 Full Profile ====

BJX2 Full Profile
* Will have MMU and FPU.
* Will use a 48 bit virtual address space.
* Will use a 32 or 48 bit physical address space.
* Will support 16, 32, and 64/96 bit instruction encodings.
* Will support GSV, GFP, and GSVF.
* Will support WEX2 3W, Jumbo, and MOVX2.
* Support MULQ is optional.
* Support for XGPR is optional for A profile.
* Support for XGPR is required for G and H Profiles.

ABI: HardFP (GFP), Base-Relocatable or PBO.


==== BJX2 Basic Profile ====

BJX2 Basic Profile
* Will have MMU and FPU.
* Will use a 32-bit address space (both virtual and physical).
** May support a 48-bit virtual address space.
* Will support 16 and 32 bit instruction encodings.
* Support for GSV and JCMP is optional.
* Will not support MULQ.
* Will support WEX2 2W or 3W.
* Will not support XGPR.

ABI: HardFP (GFP)
* Base-Relocatable for kernel
* PBO for applications and libraries.


==== BJX2 Lite Profile ====

BJX2 Lite Profile
* Will leave MMU and FPU as optional.
* Will use a 32-bit (physical) address space.
* Will not support GSV, GSVF, or JCMP.
* Will support 16 and 32 bit instruction encodings.
* Support for WEX is optional.
* Support for Jumbo is optional.
* Support MULQ is optional.
* Will not support XGPR.

ABI: SoftFP
* Absolute Addressing for ROM
* PBO or Base-Relocatable for kernel.
* PBO for application binaries and libraries.

Optional: GFP
* GFP will implement a basic FPU via GPRs.


==== BJX2 Fix32 Profile ====

BJX2 Fix32
* Will be similar to Lite.
* Will only support 32-bit instruction encodings.
* Will use a 32-bit address space.
* Will have MMU and FPU as optional.


ABI: SoftFP
* Base-Relocatable for boot image
* PBO, Application binaries and libraries.


=== Registers ===

Registers (Baseline):
* R0..R31, 64-bit
** There are 32, 64-bit GPRs.
** R0..R15 will be Primary GPRs (Partially excluding R0, R1, and R15).
** R16..R31 are Extended GPRs.
* R32..R63, 64-bit, XGPR
** Will only be encodable with XGPR or Op64 ops.
** Encodings will only be valid if the core supports XGPR.
* C0..C31, 64-bit.
** Control Registers.
** Not all of these necessarily exist.
* C32..C63, 64-bit, XGPR+XMOV.
** XMOV Control Registers
* S0..S31, 64-bit.
** Reserved.

Registers (Extended):
* X0..X30, 128-bit
** Will map to even numbered pairs of GPRs.
** Encoded as an Xn register with the LSB Clear.
** Currently, the results of accessing X0 or X14 are undefined.
* X32..X62, 128-bit (XGPR)
** Will exist as 128-bit registers within the XGPR space.
** Encoded as an Xn register with the LSB Set.
** Will logically exist as pairs.
** Currently, the results of accessing X32 or X46 are undefined.


Special Purpose Registers:
* R0 / DLR: Displacement/Low Register.
* R1 / DHR: Displacement/High Register.
* R15 / SP: Will be the Stack Pointer.

Control Registers:
* C0 / PC: Program Counter
* C1 / LR: Link Register
* C2 / SR: Status Register.
* C3 / VBR: Vector Base Register.
* C4 / SPC: Saved PC
* C5 / SSP: Saved SP
* C6 / GBR: Global Base Register.
* C7 / TBR: Thread Base Register.
* C8 / TTB: Translation Table Base.
* C9 / TEA: Translation Effective Address.
* C10 / MMCR: MMU Control Register
* C11 / EXSR: Exception Status Register.
* C12 / STTB: System TTB.
* C13 / KRR: MMU Keyring Register.
* C14: Reserved
* C15 / R15U: R15-Unmapped (RVX)
* C16 / PC2: Program Counter (SMT)
* C17 / LR2: Link Register (SMT)
* C18 / SR2: Status Register (SMT)
* C19 / TEAH: Translation Effective Address (High Bits)
* C20 / SPC2: Saved PC (SMT)
* C21 / SSP2: Saved SP (SMT)
* C22 / GBR2: Global Base Register (SMT)
* C23 / TBR2: Thread Base Register (SMT)
* C24: Reserved
* C25: Reserved
* C26 / DCB0: Possible DCB State Register (DCB)
* C27 / DCB1: Possible DCB State Register (DCB)
* C28 / VIPT: Virtual Inverted Page-Table (VIPT)
* C29 / KRR2: MMU Keyring Register (SMT).
* C30 / R0U: R0 Unmapped
* C31 / R1U: R1 Unmapped

Control Registers (XMOV, Possible):
* C32: Reserved
* C33: Reserved
* C34 / PCH: Program Counter (High Bits)
* C35 / VBR_HI: Vector Base Register (High Bits)
* C36 / SPC_HI: Saved PC (High Bits)
* C37: Reserved
* C38 / GBH: Global Base Register (High Bits)
* C39: Reserved
* C40: Reserved
* C41: Reserved
* C42: Reserved
* C43: Reserved
* C44: Reserved
* C45: Reserved
* C46: Reserved
* C47: Reserved
* C48: Reserved
* ...
* C63: Reserved

Access to SPRs will depend on Mode:
* C0: All Modes
* C1: All Modes
* C2: Read-Only in Usermode
** Partial write via special ops.
* C3: Supervisor Only
* C4: Supervisor Only
* C5: Supervisor Only
* C6: All Modes
* C7: Read-Only in Usermode
* C8: Supervisor Only
* C9: Supervisor Only
* C10: Supervisor Only
* C11: Supervisor Only
* C12: Supervisor Only
* C13: Supervisor Only
* C14: Supervisor Only
* C15: Supervisor Only
* C16: Supervisor Only
* ...
* C31: Supervisor Only


SR Bits:
* 0: T, Used as a True/False status, or Carry for ADC/SBB.
* 1: S, Special Status Flag.
** Used as secondary True/False by GSV.
* 3/2: Interrupt Priority Level
** 00: Interrupts Disabled (Reset on Fault).
** 01: High Priority Only (System/Fault).
** 10: Medium Priority Only (Timers, ...).
** 11: All Interrupts Enabled.
* 4..7: P/Q/R/O Bits, Context Dependent
** Used by GSV for Vector Compare (Word/Half).
* 8..15: U0..U7, Context Dependent
** May be used as a Predicate Stack.
* 16: XMOV: Bounds Enforce Mode
* 17: Reserved
* 18: Reserved
* 19: Reserved
* 20: WM2, Reserved
* 21: WM1, Reserved
* 22: WX4, Mode 4
* 23: WX3, Mode 3
* 24: XMOV Copy / Add (0 = Copy, 1 = Add, Possible)
* 25: XMOV User Enable (0 = Supervisor Only, 1 = User/Supervisor)
* 26: WX2, WEX Secondary (RISC-V Enable)
* 27: WXE, WEX Enable
* 28: BL, Block Interrupts
** SMT1 (SR2 Specific)
* 29: RB, Encodes the ISR Status (MD=1).
** SMT2 (SR2 Specific)
** Enables Superuser Mode (MD=0).
** Set when interrupt occurs.
** Cleared when interrupt returns.
* 30: MD, User/Supervisor.
* 31: JQ, Address-Space Size (32/48).
* 32..63: Reserved

Most operations will not change the contents of SR unless noted otherwise.


For application-level code, misaligned access to memory is required to be supported (excluding system structures, for which proper alignment is required). However, whether this is provided by the hardware or via an interrupt handler is left as implementation defined.


The JQ bit will effect the size of the address space:
* JQ=0, Use a 32-bit address space.
** AGU will produce 32-bit zero-extended output.
** (XMOV) Will behave as if high-order bits of the base are zero.
* JQ=1, Use a 48-bit address space.
** AGU will produce 48-bit zero extended output.
** An implementation may produce 64-bit output.
* This will be AND'ed with the bit from MMCR.
* Memory access will ignore high-order address bits.

Clearing the WEX Enable flag will cause WEX instructions to be executed as scalar instructions on cores with WEX support. On other cores this flag will be ignored. This flag may be set of cleared depending on the results of the WEXMD operation.


The Link Register (LR) will have the layout:
* (63:56), Saved SR(15: 8), U0..U7
* (55:52), Saved SR(23:20), WX3, WX4, WM1, WM2
* (51:50), Saved SR(27:26), WXE, WX2
* (49:48), Saved SR( 1: 0), S and T
* (47: 0), Saved PC Address

The Global Base Register (GBR) will have the layout:
* (63:48), FPU Status/Control (FPSCR)
* (47: 0), GBR Address

FPSCR Will be defined as:
* (15: 8), Status Bits
** (11): Sticky, Invalid Operation
** (10): Sticky, Overflow
** ( 9): Sticky, Underflow
** ( 8): Sticky, Inexact
* (7:4), Control Bits
* (3:0), Rounding Mode

The Vector Base Register (VBR) will have the layout:
* (63:56), Reserved (MBZ)
* (55:52), WX3/WX4/WM1/WM2 on ISR Entry
* (51:50), WXE and WX2 on ISR Entry
* (49:48), Reserved (MBZ)
* (47: 0), VBR Base Address

(WX4,WX3,WX2,WXE) will Enocde a 4-bit CPU Mode:
* 0000: BJX2 Scalar (16/32+)
* 0001: BJX2 WEX (16/32+)
* 0010: RISC-V
* 0011: RISC-V (Reserved)
* 0100: BJX2-XG2 Mode (No WEX, 32+)
* 0101: BJX2-XG2 Mode (WEX, 32+)
* 0110: XG2RV Mode (No WEX)
* 0111: XG2RV Mode (WEX)
* 1zzz: Reserved


=== Special Restrictions ===

A few special-case restrictions will be defined as part of this ISA:
* Implicit PC-Relative addressing may not cross a 4GB boundary.
** Loading a program image across 4GB a boundary is undefined.
** This will apply both to branch instructions and PC-relative load/store.
* PC is required to be aligned to a 16-bit boundary.
** Setting the low bit of PC is Undefined.
* SP (and SSP) will also have some restrictions:
** The stack may not cross a 4GB boundary.
** SP will have a minimum alignment of 8 bytes.
** SP may only be modified from Lane 1.

The results of violating these restrictions are undefined.

Explicit addressing via MOV.x instructions or LEA.x will respect the full address space, regardless of the base register used.


Optional restrictions as of BJX2D:
* / An implementation may require that 32-bit instructions be 32-bit aligned.
** Drop: This has too adverse of an effect.
* Change: 32 and 64 bit forms will have a 16-bit alignment.
** 96-bit forms may optionally require a 32-bit alignment.


=== Interrupts ===

A simple interrupt mechanism will be used, whereby some registers are copied around, and the processor will branch to a displacement relative to the VBR control register. This destination will (typically) contain a branch to the entry point to the interrupt.


VBR Displacements:
* VBR+0x00: Reset Vector
* VBR+0x08: General Fault
* VBR+0x10: IRQ, Hardware Interrupt, or TRAP instruction
* VBR+0x18: MMU/TLB Miss/Fault.
* VBR+0x20: Syscall

Note that the location of VBR is not allowed to be in a location where the vector displacement may result in a carry beyond the low 8 bits.

EXSR Codes:
* Holds a numeric status code for the exception.
* Bits 15..12:
** 0x8: General Fault
** 0xA: MMU/TLB
** 0xB: Inter-Processor Hard Interrupt
** 0xC: IRQ or Software Interrupt
** 0xD: Software Interrupt / Inter-Processor Soft Interrupt
** 0xE: Syscall
** 0xF: Processor Internal
* General Faults:
** 0x8000: Invalid Address
** 0x8001: Invalid Read (Address lacks read access)
** 0x8002: Invalid Write (Address lacks write access)
** 0x8003: Invalid Execute (Address lacks execute access)
** 0x8004: BREAK instruction used.
** 0x8005: Invalid use of a SLEEP instruction.
** 0x8006: TRAxx, Compare Fault.
** 0x8007: General Page Fault (Rethrow)
** 0x8008: Misaligned Special
** 0x8009: Misaligned Read
** 0x800A: Misaligned Write
** 0x800B: Misaligned Execute ((PC&1)!=0)
** 0x800C: SMT_Fault (Invalid Operation in SMT Mode)
** 0x800D: Load Keyring Fault (LDEKRR Failed)
** 0x800E: Invalid Opcode
** 0x800F: Bounds Fault
** 0x8010: Canary Fault
** 0x8011: Privleged Instruction Fault
** 0x8012: Floating Point Fault (Needs Emulation)
* Interrupts:
** 0xC000: General Interrupt
** 0xC001: Interval Timer
** 0xC002: Debug UART
** 0xC003: SPI Interrupt
** 0xC08x: TRAP #x
* MMU:
** 0xA000: General Fault
** 0xA001: TLB Miss
** 0xA002: TLB ACL Check
* Inter-Processor Hard:
** 0xB000: Core Boot To Address.
* Syscall
** 0xEnxx: System Call, xxx gives the SysCall ID (12 bits).
* Bits (11:8) will be used for inter-core routing.

EXSR bits (11:8) will be used for inter-core routing:
* 0x0: Will be 'Local'
* 0x1..0xB: will be logical cores IDs 1 to 11.
* 0xC: Reserved.
* 0xD: Reserved.
* 0xE: Interrupt is broadcast to Parent.
* 0xF: Interrupt is broadcast to Current or Child.

Heirarchical Grid Addressing:
* Cores will be organized into a tree structured Grid.
* Core 1 will be located within the parent grid.
** This will be the Parent of the current grid
* Cores 2..B will be within the current or child grid.
** Cnzz: Routed within the current grid.
** Dnzz: Routed within child grid.
* Either the high or low bits of the address may be used for routing.

The relevant Control Registers will be swapped depending on the state of the SR.RB status.

On interrupt Entry:
* (If SR.RB=0) Exchange SP with SSP
* Copy PC to SPC.
* Copy low 32 bits of SR into high 32 bits of EXSR.
* The status code for the interrupt will be loaded into EXSR.
** Set SR.BL, SR.MD, and SR.RB
** Set SR.WXE and SR.WX2 according to VBR.WXE and VBR.WX2
* TEA will be set to the Fault Address (if relevant for the interrupt).
* TEAH will be set to the High Fault Address (if relevant for the interrupt).
* EXSR will be set to the exception code in the low 16 bits.
** The high 32 bits will hold the saved bits from SR.
** The remaining 16 bits may be used by the handler mechanism.
* An displacement will be added to VBR and the result will be loaded into PC.

Interrupt Return:
* (If EXSR.SR.RB=0) Exchange SP with SSP
* Copy SPC back to PC (Branch to SPC)
* Restore bits in SR.
** Copy bits from EXSR(63:32) to SR(31:0)
** SR(63:32) will be left unchanged (Global State)


The interrupt hanlder will be responsible for saving/restoring any registers effected by the ISR. For passing control back into C, this will mean saving any ABI scratch registers.

Instructions which modify DLR may not be used until after DLR has been preserved.


SystemCalls:
* On entry, arguments will be passed they would in the C ABI.
* Unlike other interrupts, SysCall handlers need not preserve scratch registers.
* Roughly 12 bits are available for a function number.
** The specifics of this number are outside the scope of the ISA spec.


=== Memory Map ===

Base Physical Memory Map (32-bit):
* 00000000 .. 00007FFF: Boot ROM
* 00008000 .. 0000BFFF: Extended Boot ROM (Optional)
* 0000C000 .. 0000DFFF: Boot SRAM
* 0000E000 .. 0000FFFF: Extended Boot SRAM (Optional)
* 00010000 .. 000FFFFF: Reserved
* 00100000 .. 00FFFFFF: Reserved
* 01000000 .. 7FFFFFFF: DRAM
* 80000000 .. EFFFFFFF: Reserved
* F0000000 .. FFFFFFFF: MMIO (No MMU, Bypass)
** F0000000 .. FEFFFFFF: MMIO / Chipset
** FF000000 .. FFFFFFFF: Memory Mapped Registers


Logical Memory Map (32-bit):
* 00000000 .. 7FFFFFFF: Userland (MMU)
* 80000000 .. EFFFFFFF: System (MMU)
* F0000000 .. FFFFFFFF: MMIO (No MMU, Bypass)


Addresses in the MMIO ranges will "bypass" the normal memory access mechanisms and go directly to the MMIO Bus.

Addresses in the MMIO range will be masked to 28 bits, which will be used as the corresponding address on the MMIO Bus.

Access to MMIO will require being in Supervisor mode, and will not be subject to address translation via the MMU.

In 48-bit Mode (MMU), the address space will be extended (bits 32..47):
* 0000..7FFF: Userland (MMU).
* 8000..BFFF: System (MMU).
* C000..CFFF: System (No MMU; 44-bit physical address).
* D000..DFFF: System (No MMU; 44-bit physical address; Volatile).
* E000..EFFF: Reserved.
* F000..FFFF: Expanded Processor/Chipset/MMIO (44-bit MMIO Address).
** FFFF_Fzzz_zzzz will identity map to Fzzz_zzzz (28-bit MMIO Address).


If more DRAM exists than fits into the low 2GB, then addresses above the 4GB mark will be used for DRAM. It will be implementation defined how the DRAM region below the 2GB mark maps into this larger space.

Change:
* If JQ is Set, the MMIO range at 0000Fzzzzzzz will be disabled. In this case, the entire 47 bit userland will behave as a contiguous address range.
* The 44-bit Expanded MMIO range will only be enabled if JQ is Set.

If XMOV is supported, the quadrant bits will be used regardless of the JQ setting, however, operation in 32-bit mode will behave as-if Bits (47:32) were set to zero.


=== MMU ===

The MMU will consist of a semi-exposed TLB Mechanism.

During a TLB Miss Exception, the ISR is to fetch the relevant data from the Page Table and load it into the TLB.

The TLB will be loaded via the LDTLB instruction, which will load the 128 bit entry from the DHR:DLR pair.

TLB Entry (TLBE):
* (127:112) = VUGID / ACLID
* (111: 76) = Virtual Address Page
* ( 75: 64) = KR Access
* ( 63: 48) = ASID
* ( 47: 12) = Physical Address Page
* ( 11:  0) = Page Control bits

XTLB Entry / LDXTLB (Drop):
* (255:240) = VUGID / ACLID
* (239:140) = Virtual Address Page
* (139:128) = KR Access
* (127:112) = ASID
* (111: 12) = Physical Address Page
* ( 11:  0) = Page Control bits

X2TLB Entry (High):
* (127:112) = Reserved
* (111: 64) = Virtual Address Page (High 48 Bits)
* ( 63: 16) = Physical Address Page (High 48 Bits)
* ( 11:  0) = Page Control bits

X2TLB Entry (Low):
* Same as Base TLB Entry

X2TLB (Possible):
* Two TLB Entries are used in a pair (Hi and Lo).
* LDTLB first sends Hi; then Lo.


VUGID:
* (15:10) = VGID / VUID(Swap)
* ( 9: 0) = VUID / VGID(Swap)
* May be interpreted as a PID or ACLID.
** In these cases, VUGID is treated as a single 16-bit unit.

ASID:
* (15:10) = AGID (0=Shared, Global)
* ( 9: 0) = APID (0=Shared, Group)

KR Access (VUGID):
* (11): Other X
* (10): Other W
* ( 9): Other R
* ( 8): Group X (VUGID Check Only)
* ( 7): Group W (VUGID Check Only)
* ( 6): Group R (VUGID Check Only)
* ( 5): User X (VUGID Check Only)
* ( 4): User W (VUGID Check Only)
* ( 3): User R (VUGID Check Only)
* ( 2): VUGID Mode 2
* ( 1): VUGID Mode 1
* ( 0): VUGID Mode 0

If Keyring Check is Enabled, VUGID Mode will have several modes:
* 000: Access Check Always Fails (Access Fault)
* 001: Perform VUGID Check, Access Check is Pass or Fail (Access Fault).
* 010: Always Generate ACL Check Exception (on ACL Miss)
* 011: Perform VUGID Check, Generate ACL Check if no keys match (ACL Miss).
* 100: Possible, Restricted Execute Mode
* 101: Same as in 001, But swap the UID and GID fields.
* 110: Same as in 010, Grant Privledge on Execute.
* 111: Same as in 011, But swap the UID and GID fields.

If Keyring Check is Disabled, VUGID access checks are ignored (Always Pass).
* Only the normal page access checks are used.

When executing code within a Privledge on Execute page, the MMU is to behave as if the VUGID for the currently executing page were present within KRR. This may replace whatever is the last entry in the register. Note that the KRR must also grant Execute to this page when interpreting the VUGID as an ACLID.

The Restricted Execute mode would operate as-if the KRR contained only the VUGID from the page being executed.


Page Control:
* (11): PR.U3 (User 3, OS / Virtual Memory)
* (10): PR.U2 (User 2, OS / Virtual Memory)
* ( 9): PR.S1 (Size 1) / PR.U1 (User 1, OS / Virtual Memory)
* ( 8): PR.S0 (Size 0) / PR.U0 (User 0, OS / Virtual Memory)
* ( 7): PR.NU (Not User Accessible)
** Disallow access from User Mode.
* ( 6): PR.NX (Execute Disable)
** Page may not be executed.
* ( 5): PR.NW (Write Disable)
** Page may not be written to.
* ( 4): PR.NR (Read Disable)
** Page may not be read from.
* ( 3): PR.NC, Page Is Volatile / Non-Cachable
** Set to indicate accesses to this page are volatile (Data).
** Contents in this page should not be retained in the L1 cache.
** Indicates Secure-Execute if NW is also Set and NX is Clear.
* ( 2): PR.D, (Dirty)
** Set if page has been modified.
* ( 1): PR.V1, Page Is Valid (Mode)
* ( 0): PR.V0, Page Is Valid (Mode)

Valid 1/0:
* 00: Not Valid (This TLBE will be ignored).
* 01: Valid Exclusive (Normal Page), Low Range (X2TLB)
* 10: High-Range (X2TLB), Addr(95:48)
* 11: Valid Shared (Normal Page), Low Range (X2TLB)

Size 1/0 (Base=4K):
* 00:  4K Page (Single Page)
* 01: 2MB Page (512 Pages, PDE)
* 10: 64K Page (16 Pages)
* 11: -

Size 1/0 (Base=64K):
* 00: 64K Page (1 Page)
* 01: 512M Page (8192 Pages, PDE)
* 10: 64K Page (1 Page)
* 11: -

Size 1/0 (Base=16K):
* 00: 16K Page (1 Page)
* 01: 32M Page (2048 Pages, PDE)
* 10: 64K Page (4 Pages)
* 11: -

If NC is Set and NU is Clear:
* If NX is Clear, This page has Secure-Execute privledge.
** Secure-Execute may also require NW to be Set.
* If NR or NW are Set, these are ignored in Supervisor Mode.
** The NC flag is Ignored if NX is Set.

Valid Shared/Exclusive Mode:
* Valid Exclusive: Assume that page is only mapped once.
* Valid Shared: Assume that this page may be in use multiple places.

Keyring checks are enabled via the Keyring Register (KRR):
* This register is interpreted as four 16-bit word values.
** Each holds a VUGID pair.
* If the low 16 bits are zero, Keyring Check is disabled.
* If the other words are zero, they are ignored.

The keyring values are compared against the VUGID value in the TLB:
* Both Match: Check using User RWX bits.
* VGID Matches: Check using Group RWX bits.
* Otherwise: Check using Other RWX bits.

If any of the four keys will allow the requested access, access will pass.
* Otherwise, if ACL Check is set, generate an ACL Check Exception on ACL Miss.
* Otherwise, the access will generate an Access Fault Exception.

Note that Keyring checks are placed after normal page access checks, so with Keyring access enabled, both the Normal and Keyring checks are required to pass in order to allow access to the page.

Note that if the normal modes, there will be 64 Groups of 1024 Users, but these may be flipped per-page to effectively allow 1024 Groups of 64 Users. 

Note that if both fields are equal, the result will be equivalent regardless of the flip.

The KRR Keys may also be checked against an ACL Cache, with entries of the form:
* (63:44): Reserved / MBZ
* (43:32): KR Access Mode
* (31:16): KRR VUGID / PID
* (15: 0): TLB VUGID / ACLID

The entries in the ACL Cache will be matched against those in the KRR if an ACL Check mode is used. If a matching entry is found, it will be used, otherwise an ACL Check exception will be generated if the TLBE doesn't match.

The ACL Check handler may then upload an ACL Entry via LDACL which may either grant or deny access to the page ACLID in question.

Note that ACL Entries will require an exact match for both IDs, which will be interpreted as a single 16-bit value.


MMCR Bits:
* 0001: Enable MMU/TLB
** Enables the use of page-level translation via the TLB.
** If clear, direct physical addressing is used.
** Ignored if SR.RB is Set.
* 0002: Enable 96-bit Virtual Addressing (If Supported)
* 0004: User-mode uses 48-bit addressing (else, assume 32-bit).
* 0008: System-mode uses 48-bit addressing (else, assume 32-bit).
* 0030: Basic Page Size.
** 0: Use 4K as the Basic Page Size.
** 1: Use 64K as the Basic Page Size.
** 2: Use 16K as the Basic Page Size.
** 3: Reserved
* 0040: Reserved
* 0080: Enable Hardware Page Walk
** Enables use of hardware page walk if the hardware exists.
** May still generate TLB Miss events if/when the page-walk fails.
** May require the use of conventional nested page tables.


==== Page Tables ====

Page Table Entries (32-bit PTE):
* ( 31: 12) = Physical Address Page
* ( 11:  0) = Page Control bits

Page Table Entries (64-bit PTE, U2=0):
* ( 63: 48) = ACLID Table Index
* ( 47: 12) = Physical Address Page
* ( 11:  0) = Page Control bits

Page Table Entries (64-bit PTE, U2=1):
* ( 63: 48) = VUGID
* ( 47: 36) = KR Access Mode
* ( 35: 12) = Physical Address Page
* ( 11:  0) = Page Control bits

Page Table Entries (64K, 128-bit):
* (127:112) = VUGID
* (111:100) = KR Access Mode
* ( 63: 12) = Physical Address Page
* ( 11:  0) = Page Control bits

There will be a User Page Table (TTB) and a System Page Table (STTB).
Whether STTB is used, and what page-table type is assumed, will depend on the low bits in TTB.

TTB Bits:
* ( 1: 0): Will give the main page-table type (32/64 bit PTE).
* ( 3: 2): Will give the type of STTB table.
* ( 7: 4): Will give the nominal page size.
* ( 9: 8): Quadrant Extra Levels
* (11:10): Extendec Table Type
* (47:12): Physical Page Address
* (63:48): Process ASID (0 if ASID is unused)

Nominal page size:
* 0000: Use 4K pages (32/64-bit PTE)
* 0001: Use 4K pages (128-bit PTE)
* 0010: Use 64K pages (64-bit PTE)
* 0011: Use 64K pages (128-bit PTE)
* 0100: Use 16K pages (64-bit PTE)
* 0101: Use 16K pages (128-bit PTE)

* 1000: Use 4K pages, 64-bit PTE, Hashed Top
* 1001: -
* 1010: Use 64K pages, 64-bit PTE, Hashed Top
* 1011: -
* 1100: Use 16K pages, 64-bit PTE, Hashed Top
* 1101: -
* Others: Reserved

Type of STTB table:
* 00: None, Reuse TTB.
* 01: Use Two-Level STTB.
* 10: Use Three-Level STTB.
* 11: Use Four-Level STTB.

The STTB table will need to match the main page table in terms of nominal page size.

Quadrant Extra Levels (Page Table):
* 0: Adds 0 Levels (NA,  2,  3,  4)
* 1: Adds 4 Levels ( 5,  6,  7,  8)
* 2: Adds 8 Levels ( 9, 10, 11, 12)
* 3: Reserved

Quadrant Extra Levels (AVL Tree):
* 0: AVL Tree
* 1: B-Tree
* 2: Hybrid B-Tree
* 3: Hybrid-2 B-Tree


The page table level will be defined as its distance from the final level.
Level 0 will contain individual (leaf) pages, level 1 and above will refer to page directories, up to level N as a top-level directory (depending on the total height of the page table).

Within the Page tables, the Size field will depend on its depth within the table.
* 00: No Skip, Single Page (Level 0)
* 01: Large Page (Level 1), Skip 1 level (Level > 1)
* 10: Large Page (Level 0), Skip 2 levels (Level > 1)
* 11: Large Page (Level 0), Skip 3 levels (Level > 1)

Within skipped levels, the address bits are either 0, or the result of the page walk is returned as 0.


==== Hashed Top Tables ====

Hashed Top Tables will follow the same basic structure as a normal page table, except that it will add an extra top-most level representing a hash of any bits above the top level of the table.


So, a Hashed 2-level table will be understood as an N+2, or 3-level table.
Entries in the hashed table will consist of a pair of a masked virtual address for bits (95:32), followed by the associated Page Directory Entry.

The hash will be based on the masked high-order bits of the address in question, with the mask determined by the depth of the table and the page size.

The hash will consist of every non-masked level of the table being XORed together.


==== 4K Page Tables ====

Page table types for 32/64 bit PTEs:
* 00: None / Invalid
* 01: Two-level, 32-bit PTE; Single Page Table.
* 10: Three-Level, 64-bit PTE, 39-bit address.
* 11: Four-Level, 64-bit PTE, 48-bit address.

The page table types for 128-bit PTEs with 4K pages:
* 00: Four-level, 44-bit address
* 01: Five-level, 52-bit address
* 10: Six-Level, 60-bit address
* 11: AVL Tree


==== 64K Page Tables ====

Alternate page table mode with 64K as the basic page size.
Page table will be 2-4 levels each containing 4096 or 8192 entries.

With 64-bit PTE modes, the physical page address will be shifted right by 4 bits.

Page Table Types (64-bit PTE):
* 00: None / Invalid
* 01: Two-level, 64-bit PTE, 42-bit address.
* 10: Three-Level, 64-bit PTE, 55-bit address.
* 11: -

Page Table Types (128-bit PTE):
* 00: None / Invalid
* 01: Two-level, 128-bit PTE, 40-bit address.
* 10: Three-Level, 128-bit PTE, 52-bit address.
* 11: AVL Tree


==== 16K Page Tables ====

Alternate page table mode with 16K as the basic page size.
Page table will be 2..4 levels each containing 2048 entries.

With 64-bit PTE modes, the physical page address will be shifted right by 2 bits.

Page Table Types (64-bit PTE):
* 00: None / Invalid
* 01: Two-level, 64-bit PTE, 36-bit address.
* 10: Three-Level, 64-bit PTE, 47-bit address.
* 11: Four-Level, 64-bit PTE, 58-bit address.

Page Table Types (128-bit PTE):
* 00: None / Invalid
* 01: Two-level, 128-bit PTE, 34-bit address.
* 10: Three-Level, 128-bit PTE, 44-bit address.
* 11: AVL Tree


=== AVL-Tree Page Table ===

An AVL Tree may be used instead of a page table.

AVL Node:
* (255:172): Virtual Address (95:12)
* (171:160): Optional KR Mode
* (159:152): Node Depth (0=Leaf)
* (151:128): ?
* (127: 64): Page Table Entry (64-bit)
* ( 63: 32): Left Node
* ( 31:  0): Right Node

TTB will hold the Root Node ID of the tree in TTB(43:12).

The Node IDs will be encoded as the Node's physical address shifted right by 4 bits.


=== B-Tree Page Table ===

A B-Tree will be used in place of a typical page table, with each B-Tree node equal in size to a page.

Typical Node Layout:
* u64 ptr_next;			//pointer to next node (same level)
* u64 ptr_prev;			//pointer to previous node (same level)
* u64 ptr_up;			//pointer to parent node
* u16 n_ecnt;			//number of entries in this node
* byte n_level;			//node level (0=leaf)
* byte pad1[5];			//pad to 32B (MBZ)
* byte pad2[NODEPAD];	//pad bytes (MBZ)
* u32 addr_hi[NODESZ];	//high address bits
* u64 addr_lo[NODESZ];	//low address bits
* u64 pte_val[NODESZ];	//page table entry

The address will be represented as a 96-bit linear address.

The NODESZ will depend on page size:
* 4K: 203 (PAD=4)
* 16K: 817 (PAD=12)
* 64K: 3275 (PAD=4)


=== VIPT / Virtual Inverted Page Table ===

This may be used as an extended RAM-Backed TLB.
* The Table base will be a multiple of the page size.

The VIPT may exist as an intermediate between the TLB and a TLB Miss Exception.
If VIPT support is present, and the table is in-use, it may be checked during a TLB Miss before a TLB Miss exception is raised. If a matching entry in the table is found, then this entry will be used in place of raising a TLB Miss Exception.

Note that the Inverted Page Table is intended as an intermediate stage between the TLB and the main Page Table, not as a replacement for the Page Table. Its existence is intended as a an optional feature to accelerate the Software TLB.


VIPT:
* (63:48): Reserved
* (47:12): Base Address
* (11: 5): Reserved
* ( 4: 0): Table Size in Rows, Log2 (0 if unused)

In the current definiton, each row in the VIPT will be 64 bytes and represent 4 TLB entries. In effect, this table will be 4-way set associative.

The virtual page number will be masked by the table size in order to calculate the row index of the table.

So, for example, if the table size is 0C, and the page size is 16K, then bits (25:14) of the address will be used as the table row index.


=== Addressing Modes ===

Addressing Modes (16-bit Instruction Forms):
* (Rn), Register used as an address.
* (Rn, DLR), Register used with scaled Displacement Register.
** MOV.x, Scale is size of base type.
* (PC, DLR), PC used with scaled Displacement Register.
* (DLR), Direct/Absolute Address
* (PC, Disp8s), PC with scaled 8-bit displacement.
* (SP, Disp4u), SP with scaled 4-bit displacement.

Addressing Modes (32-bit Instruction Forms):
* (Rm, Ri), Register with a scaled index.
* (Rm, Disp9u), Register with a 9 bit displacement.

For memory operations, Rm will serve as the Base Register, and Rn will serve as a Source or Destination register.

Additional addressing modes may be faked by the assembler.
* Larger displacements will involve loading a value into DLR.

Displacement Scale:
* MOV.x will depend on the base register:
** If using a GPR, will be scaled based on the size of the type.
** If using PC, a scale of 1 is used for MOV.x forms.
** GBR and TBR, as with PC, will use a scale of 1.
* Branches will use a scale of 2.
* LEA.x will be scaled by the size of the base type.
** This includes if using PC or GBR as a base register.

The Scaled Index forms will be demoted to Optional in Fix32 'D' Profile.


For PC relative addresses, the address will be calculated relative to the start of the following instruction.

Architectural memory access will be unaligned for types less than or equal to 64 bits. Types larger than 64-bits may have an implied 64-bit alignment. 

Integer values will be little endian two's complement.


The use of R0 or R1 will be special as a base register for memory-access ops.
* R0 will encode a PC relative form.
** (R0, R0) will encode (PC, DLR)
** (R0, R1) will encode (DLR)
* R1 will encode GBR or TBR.
** Encoding of GBR or TBR will depend on Index register.
** (R1, R0) will encode (GBR, DLR)
** (R1, R1) will encode (TBR, DLR)

If used with Displacement Forms:
* (R0, Disp) will encode (PC, Disp)
* (R1, Disp) will encode (GBR, Disp)

In the case where the base reegister is interpreted as PC, GBR, or TBR, the scale used for the index or displacement will be 1.


Note that the effective precision of LEA will be relative to the size of the address space. The effective precision of a LEA may be smaller than 64 bits. The value contained in bits above the effective size of the virtual address space will be implementation defined. The default case will be to zero extend the results to the full 64 bits.

R1 and R15 will be special and reserved as Index Registers.
* (Rn, R1) will encode (Rn, DLR), except scale will be 1.
* (Rn, R15) will be Reserved.

R16 and R17 will be reserved as base or index registers, similar to R0 and R1.

Byte and Word loads may not target R15.

Possible: MOV.B load with R15 as the destination will be interpreted as a PREFETCH instruction. This will not perform a load, but will instead request that the processor retrieve the associated cache lines.


Depending on the implementation and mode, LEA may be computed either as 32-bit address, or with a full 48 or 64 bits.

If full 48-bit addressing is used:
* Bits 0..47 will be subject to address calculation;
* Bits 48..63 will be zeroed.

If 32-bit wraparound is used with 48 bit addressing:
* Bits 0..31 will be subject to address calculation;
* Bits 32..47 will be copied unchanged;
* Bits 48..63 will be zeroed.

In an implementation with 32-bit addressing, or in 32-bit address mode:
* Bits 0..31 will contain the computed address;
* Bits 32..63 will be zeroed.

Note that the address mode used is not required to match the pointer width used by the C ABI. However, it may be presumed that the address width matches between the address mode, C ABI, and the width specified in executable images.


For branches and other instructions which directly use PC-relative addressing, only the low 32 bits are required to be computed. These instructions will be undefined if the target address crosses a 4GB boundary.

For other cases, the nominal address calculation will be performed as 36 bits with a 33 bit sign-extended displacement. The remaining high-order address bits will be adjusted based on the results in the low-order bits.

To support objects larger than those which support a 32 or 33 bit displacement, it will be necessary to use explicit ALU instructions.


=== Symmetric Multithreading (SMT, Possible) ===

Multithreading may be provided as an optional feature.
* In this mode, two logical user threads may run in parallel.
* This mode will temporarily suspend itself during interrupts.
** The second thread will not execute while in an interrupt.

Within SR2, the RB and BL flags will be reused as SMT1/SMT2 flags.
* (SMT2,SMT1) = 00: SMT is Disabled
* (SMT2,SMT1) = 01: SMT is Suspended (Interrupts/Etc)
** May return to Active when the interrupt returns.
* (SMT2,SMT1) = 1z: SMT is Active
** SMT1 will be used as an internal SMT state flag in Active mode.
** SMT mode is to be entered in the 10 state.

In most other regards, the secondary set of control registers will mirror the behavior of the primary set during an interrupt. An OS scheduler may schedule a pair of threads in roughly a similar way to a single thread.


In SMT mode, the set of 64 GPRs will effectively be divided in half, giving each thread is own logical set of 32 GPRs.
* R0..R31 will belong to Thread 1.
** R15: SP1
* R32..R63 will belong to Thread 2.
** R47: SP2

Code within the second thread will be decoded as if it were the base ISA with 32 GPRs, and any attempts to use XGPR operations with extended GPRs will be undefined when SMT is Active.

Several additional control registers will be defined as partial duplicates of the primary control registers. Within the second thread, these registers will appear to be equivalent to the base registers.

Note that both threads are to operate within the same virtual address space.

Note that an SMT_Fault exception may be Raised in SMT Mode:
* If an attempt is made to access R32..R63
* If an attempt is made to encode a bundle longer than 3.
* If an attempt is made to encode an interleaved bundle.


Note also that a core may allow WEX3W in SMT mode but 5W or 6W with SMT Disabled or Suspended. In this case the maximum number of pipeline lanes will be effectively doubled when SMT is Disabled or Suspended.

For bundles wider than 3, the lanes will be interleaved by default:
* OP2A | OP1A
* OP3A | OP2A | OP1A
* OP2B | OP2A | OP1B | OP1A
* OP3A | OP2B | OP2A | OP1B | OP1A
* OP3B | OP3A | OP2B | OP2A | OP1B | OP1A

However, interleaved modes may be used for narrower bundles:
* OP1B | OP1A
* OP2A | OP1B | OP1A

If Lane 2 would otherwise contain an operation which is not allowed in Lane 2 in 3W mode (such as an F1 block instruction), or if Lane 3 contains a Jumbo Prefix and Lane 2 contains a non-Jumbo instruction (Most F0 block instructions).

Interleaved decoding will be invalid if Lane 2 contains a Jumbo Prefix. Similarly, bundles longer than 3 will be invalid with a Jumbo prefix in Lane 2.


This interleaving may effect the operation of Jumbo Prefixes.
* J2B | J2A | OP1B | OP1A

For narrow bundles:
*      F4 | Fz: Non Interleave
*      F5 | Fz: Interleave (Invalid In 3W)
*      F6 | Fz: Non Interleave
*      F7 | F0: Reserved
*      FC | F0: Non Interleave
*      FD | F0: Reserved
*      FE | Fz: Non Interleave
*      FF | Fz: Non Interleave
* FE | FE | Fz: Non Interleave (Jumbo96)
* Fz | FE | Fz: Non Interleave
* Fz | FF | Fz: Non Interleave
* FE | F4 | F0: Interleave (Invalid In 3W)
* FE | F4 | F1: Interleave (Invalid In 3W)
* FE | F4 | F2: Interleave (Invalid In 3W)
* FE | F5 | F0: Invalid (Encoding Violation)
* FE | F6 | F0: Non Interleave
* FE | F5 | F1: Interleave (Invalid In 3W, Jumbo combines with Lane 1)
* FE | F6 | F1: Undefined (Ambiguous)
* FF | Fz | Fz: Undefined (Invalid in 5W/6W)

Note that interleaving the jumbo prefixes violates the equivalence between sequential and parallel execution. By extension, profiles which allow the use interleaved jumbo encodings will drop the requirement for sequential equivalence.


=== XMOV ===

Possible Extension, Extended Address Space
* Will extend the address space to 96 bits.
* Addresses in the extended space will be encoded as pairs.
* Note that only the low 48 bits are necessarily linear.
** The high 48 may represent a sparse and non-linear space.
** Operations like LEA will only be defined relative to the low 48 bits.

The Low Register (Even) will encode the low 48 bits of the address.
* This is part is equivalent between XMOV and Non-XMOV Addressing.
* Some encodings will explicitly extend to the full address width.
** Use of PC, GBR, or TBR as a base register.

The High Register (Odd) will encode the high order bits of the address.
* This high part will be referred to as a Quadrant (or Quad).

The control registers which contain addresses will be similarly expanded, however the high-order part may only be accessible via MOVX or XGPR encodings.
* PC, GBR, VBR, ... will be expanded.
* The high halves may also be mapped to C32..C63.

Modifying a Control Register with a 64-bit instruction will only modify the low-order bits, with the high order bits being kept as-is.

Accessing memory using a Non-XMOV encoding with a GPR as a base register will copy the high order bits of the address from GBH. In effect, GBH will serve as a data quadrant.

For Branches, PCH (PC Hi) will serve as a code quadrant.
* Normal branches will only be within a single quadrant.
* Wide branch operations may be used to branch into a different quadrant.
* VBR_HI, SPC_HI


There will be several addressing sub-modes:
* XMOV Disabled, Only 32 or 48 bit addressing will be available.
* Quadrant Copy
** All bits from the Quadrant will be copied over directly.
** This mode will only have the 32 or 48-bit base address.
* Quadrant Add (Possible):
** In 32-bit Mode, Functionally equivalent to Quadrant Copy.
** In 48-bit Mode, Adds bits (59:47) from the base to (15:0) from the quadrant.
** This in effect allows an expanded pseudo linear addressing mode for data access.
** This is done if the high bits of the base are 1111.
** Bit 47 of the generated address is set to 0.

There will also be a flag to select between User/Supervisor and Supervisor Only.
* In U/S Mode, Userland code is allowed to use XMOV ops.
* In SVO Mode, Only supervisor mode may use XMOV ops.


=== Predicated Instructions ===

The instructions encoded in the predicated ranges will be otherwise equivalent to their non-predicated counterparts.

Predicated instructions will be executed if the state of SR.T matches their expected value. If the state of this flag does not match (at the time the instruction is executed), the instruction will function as a NOP.

Instructions whose behavior already depends on SR.T or whose function is to update SR.T may not be encoded as predicated instructions. Branches and other control-flow instructions similarly may not normally be predicated, with some exceptions.

Examples of instructions which will not allow predication:
* BT and BF
* RTS, RTSU, RTE, etc.
* CSELT and similar
* CMPxx, ...
* ADC, SBB
* Etc.

Similarly, instruction forms which are not otherwise encodable with predication may not be predicated.

In the ASM syntax, they will be expressed via an "?T" or "?F" suffix:
* MOV.B?T R3, (R5)	//Store if SR.T is True

If BRA is predicated, its behavior will be analogous to the conditional forms:
* BRA?T will behave the same as BT
* BRA?F will behave the same as BF
* BSR may be predicated.

However:
* BT and BF may not be predicated.


Predicated Ranges will exist:
* E0zz_zzzz (Execute if True, Repeats F0)
* E1zz_zzzz (Execute if True, Repeats F1)
* E2zz_zzzz (Execute if True, Repeats F2)
* E3zz_zzzz (Execute if True, Repeats F3)
* E4zz_zzzz (Execute if False, Repeats F0)
* E5zz_zzzz (Execute if False, Repeats F1)
* E6zz_zzzz (Execute if False, Repeats F2)
* E7zz_zzzz (Execute if False, Repeats F3)
* E8zz_zzzz (Execute if True, Repeats F8)
* E9zz_zzzz (Execute if True, Repeats F9)
* EAzz_zzzz (PrWEX Block, Repeats F0, Execute if True)
* EBzz_zzzz (PrWEX Block, Repeats F2, Execute if True)
* ECzz_zzzz (Execute if False, Repeats F8)
* EDzz_zzzz (Execute if False, Repeats F9)
* EEzz_zzzz (PrWEX Block, Repeats F0, Execute if False)
* EFzz_zzzz (PrWEX Block, Repeats F2, Execute if False)

The ranges FAzz, FBzz, FEzz, and FFzz, may not be encoded in predicated forms.
The encoding of predicated forms will be otherwise equivalent to their Fz counterparts.


=== Extensions List ===

Extensions List:
* ALU: Optional ALU Extensions
* ALUX: 128-bit ALU Instructions
* BCD: Packed Binary Coded Decimal
* CLZ: Count Leading Zeroes
* DMAC: Integer Multiply Accumulate
* FDIV: Floating Point Divide
* GFP: GPR Floating Point
* GSV: GPR SIMD
* GSVF: GPR Floating-Point SIMD (64-bit)
* GSVFX: GPR Floating-Point SIMD (128-bit)
* JCMP: Compare-and-Branch
* Jumbo: Large Immediate Encodings
* LdOp: Load/Store Operations
* LDTEX: Load Texture
* MOVX2: 128-bit Load/Store
* MULL: 32-bit Multiply
* MULQ: 64-bit Multiply and Integer Divide Instructions
* MULW: 16-bit Multiply
* MULX: 128-bit Multiply
* Op24 (Deprecated): 24-bit instruction encodings.
* Op64: Extended 64-bit instruction encodings
* PrWEX: Predicated WEX Encodings
* RGB: Packed RGB Operations
* RGBF: Packed Floating Point RGB Operations
* RGBX: Extended 128-bit RGB Operations
* RiMOV: Register+Displacement Addressing Mode
* TTAG: Type Tag Instructions
* WEX2: Wide-Execute
* XGPR: Extended GPR Set (R0..R63)
* XMOV: Load/Store with 128-bit Addresses.


== Instruction Set ==

Notation:
* 0..9, A..F: Literal hex bits
* n: Destination Register, Typically bits 7..4
* m: Source Register, Typically bits 3..0
* i: Signed immediate bits
* j: Unsigned immediate bits
* d: Displacement bits
* e/f/g/G: The 'E' field.
* w/W: The 'W' field.
* x: First Value Placeholder bits.
* y: Second Value Placeholder bits.
* z: Undefined/Pattern/Third Value, Placeholder bits.
** Will typically be defined later or in a sub-pattern.

Prefixes:
* / Indicates forms which have been dropped.
* ? Indicates forms which may or may not be supported (Optional).
** If unsupported, the encoding space is a placeholder.
** Compiler will be expected to only emit I-Forms appropriate for the target.
** An instruction need not be marked optional if the parent block is optional.
*** Instead, this will denote that the instruction is optional within the block.
* ??, Don't know if it will be implemented.
** Instruction space may likely be reserved but unimplemented.
* ?/ or /?, May drop from ISA.

Register Notation:
* Rm, Source Register
* Rn, Destination Register
* Rj, Source Register (R16..R31)
* Rk, Destination Register (R16..R31)
* Rx, Source/Destination Register (Even, LSB: 0=R0..R6, 1=R16..R30)
* Cm, Source Register (Control Register)
* Cn, Destination Register (Control Register)
* Sm, Source Register (Shadow Register)
* Sn, Destination Register (Shadow Register)
* Xm, Source Register (128-bit pair)
* Xn, Destination Register (128-bit pair)
** Even: 0=(R1:R0), 2=(R3:R2), ..., 30=(R31:R30)
** Odd (Reserved): 1=(R33:R32), ..., 31=(R63:R62)


Immediate Notation
* Imm5, Immediate with 5 bits, unspecified extension.
* Imm5u, Immediate with 5 bits, zero extended (WI=Undef).
* Imm5n, Immediate with 5 bits, one extended (WI=Undef).
* Imm5us, Immediate with 5 bits, zero extended, XGPR/XG2 sign-extend 6-bit.

* Imm9, Immediate with 9 bits, unspecified extension.
* Imm9u, Immediate with 9 bits, zero extended.
* Imm9s, Immediate with 9 bits, sign extended.
* Imm9n, Immediate with 9 bits, one extended.
* Imm9us, Immediate with 9 bits, zero extended, XGPR/XG2 sign-extend 10-bit.

* Imm10, Immediate with 10 bits, unspecified extension.
* Imm10u, Immediate with 10 bits, zero extended.
* Imm10s, Immediate with 10 bits, sign extended.
* Imm10n, Immediate with 10 bits, one extended.
* Imm10us, Immediate with 10 bits, zero extended, XGPR/XG2 sign-extend 11-bit.

* Disp5 / Disp5u: Displacement with 5 bits, zero extended, XGPR/XG2 Disp6s.

* Disp17u, Displacement with 17 bits, zero extended.
* Disp17s, Displacement with 17 bits, sign extended.
* Etc.


Notation for the 'E' field:
* e: qnmi
** q(E.q)=Quadword/Alternate Operation
*** Selects between MOV.L and MOVU.L
*** Selects between CMPxx and CMPQxx
** n(E.n)=Bit 4 of Rn
** m(E.m)=Bit 4 of Rm
** i(E.i)=Bit 4 of Ro/Ri
* f: qnii (If n is in n position)
* f: qiin (If n is in o position)

* G/g: 1nmi / 0nmi
* H/h: 1nii / 0nii (If n is in n position)
* H/h: 1iin / 0iin (If n is in o position)

* P/p: 1nm0 / 0nm0
* Q/q: 1nm1 / 0nm1

Notation for the 'W' field:
* w: wnmi
** w (W.w): WEX (7zzz/9zzz)
** n(W.n)=Bit 5 of Rn
** m(W.m)=Bit 5 of Rm
** i(W.i)=Bit 5 of Ro/Ri

Canonical F0 Block Instruction:
* F0nm_ZeoZ
** n: Nominal position for Rn.
** m: Nominal position for Rm.
** o: Nominal position for Ro.
*** May also be Rn for some forms.
*** May be used as an imm5/disp5.
*** May be used for additional opcode bits.


The base unit of instruction encoding is a 16-bit word, with the high bits of the first word encoding the length of the instruction.

When an immediate is split between multiple words, the preceding word will contain the high-order bits relative to the following word, with the primary exception of Imm32.


Instruction Space:
* 0xxx .. Dxxx: 16-bit Instruction Forms
* E0xx .. EFxx: 32-bit Instruction Forms (Predicated)
* F0xx .. FFxx: 32-bit Instruction Forms (Normal and WEX)

Except where noted otherwise, the 16-bit instruction forms are limited to the first 16 registers.


=== 16-bit Instruction Forms ===


Instruction Space
* 0zzz (Basic MOV.x Block)
** 00nm  MOV.B	Rn, (Rm)
** 01nm  MOV.W	Rn, (Rm)
** 02nm  MOV.L	Rn, (Rm)
** 03nm  MOV.Q	Rn, (Rm)
** 04nm  MOV.B	Rn, (Rm, DLR)
** 05nm  MOV.W	Rn, (Rm, DLR)
** 06nm  MOV.L	Rn, (Rm, DLR)
** 07nm  MOV.Q	Rn, (Rm, DLR)
** 08nm  MOV.B	(Rm), Rn
** 09nm  MOV.W	(Rm), Rn
** 0Anm  MOV.L	(Rm), Rn
** 0Bnm  MOV.Q	(Rm), Rn
** 0Cnm  MOV.B	(Rm, DLR), Rn
** 0Dnm  MOV.W	(Rm, DLR), Rn
** 0Enm  MOV.L	(Rm, DLR), Rn
** 0Fnm  MOV.Q	(Rm, DLR), Rn

* 1zzz (Basic Arith Block)
** 10nm  ADD	Rm, Rn				//Rn=Rn+Rm
** 11nm  SUB	Rm, Rn				//Rn=Rn-Rm
** 12nm  ADC	Rm, Rn				//Add with Carry, Rn=Rn+Rm+SR.T
** 13nm  SBB	Rm, Rn				//Subtract with Borrow, Rn=Rn+(~Rm)+(!SR.T)
** 14nm  TST	Rm, Rn				//SR.T=!(Rm&Rn)
** 15nm  AND	Rm, Rn
** 16nm  OR		Rm, Rn
** 17nm  XOR	Rm, Rn
** 18nm  MOV	Rm, Rn				//Rn=Rm
** 19nm  MOV	Rj, Rn
** 1Anm  MOV	Rm, Rk
** 1Bnm  MOV	Rj, Rk
** 1Cnm  CMPEQ	Rm, Rn				//Rn==Rm (Low 32 Bits)
** 1Dnm  CMPHI	Rm, Rn				//Unsigned Rn GT Rm (Low 32 Bits)
** 1Enm  CMPGT	Rm, Rn				//Signed Rn GT Rm (Low 32 Bits)
** 1Fnm  CMPGE	Rm, Rn				//Signed Rn GE Rm (Low 32 Bits)

* 2zzz
** 20dd  BRA	(PC, disp8s)		//Branch, PC=PC+(disp8s*2)
** 21dd  BSR	(PC, disp8s)		//Branch Subroutine
** 22dd  BT		(PC, disp8s)		//Branch True
** 23dd  BF		(PC, disp8s)		//Branch False
** 24jj  -
** 25jj  -
** 26jj  LDISH	Imm8u, DLR			//DLR=(DLR SHL 8)|Imm8u
** 27nj  -
** 28nd  MOVU.L	(SP, disp4u), Rn	//Stack-Relative ZX Load
** 29nj  MOV.X	Rx, (SP, disp4u)	//(MOVX2) Stack-Relative Store Pair
** 2And  MOVU.L	(SP, disp4u), Rk	//Stack-Relative ZX Load
** 2Bnj  MOV.X	(SP, disp4u), Rx	//(MOVX2) Stack-Relative Load Pair
** 2Cnj  CMPEQ	Imm4u, Rn			//Rn==Imm4, Zero Extend
** 2Dnj  CMPEQ	Imm4n, Rn			//Rn==Imm4, One Extend
** 2Enj  CMPGT	Imm4u, Rn			//Rn==Imm4, Zero Extend
** 2Fnj  CMPGE	Imm4u, Rn			//Rn==Imm4, Zero Extend

* 3zzz
** 3000  NOP						//Do Nothing
** 3010  RTS						//PC=LR
** 3020  SLEEP						//Sleep
** 3030  BREAK						//Breakpoint
** 3040  CLRT						//Clear SR.T
** 3050  SETT						//Set SR.T
** 3060  CLRS						//Clear SR.S
** 3070  SETS						//Set SR.S
** 3080  NOTT						//SR.T=!SR.T
** 3090  NOTS						//SR.S=!SR.S
** 30A0  /
** 30B0  /
** 30C0  RTE						//Return from exception
** 30D0 / DIV0						//Setup SR for divide
** 30E0 / DIV1						//Divide Step (Uses DHR, DLR)
** 30F0  LDTLB						//Load entry into TLB

** 30z1  -

** 3002  ZZOP						//Internal Op, UB
** 3012  RTSU						//PC=LR, Hint
** 3022  SYSCALL					//Throw(DLR)
** 3032  -
** 3042  LDACL
** 3052  LDXTLB						//(XMOV) Load entry into TLB
** 3062  -
** 3072  -
** 3082  SXENTR		//Enter Superuser Mode
** 3092  SUENTR		//Enter User Mode
** 30A2  SVEKRR		//Save Encoded Keyring
** 30B2  SVENTR		//Enter Supervisor Mode (From Superuser Mode)
** 30C2  LDEKRR		//Load Encoded Keyring
** 30D2  LDEKEY		//Load Encoded Key
** 30E2  LDEENC		//Encode Key
** 30F2  INVTLB		//Flush the TLB

** 30z3  -

** 30z4  -
** 30z5  -
** 30z6  -
** 30z7  -
** 30z8  -
** 30z9  -
** 30zA  -
** 30zB  -
** 30zC  -
** 30zD  -
** 30zE  -
** 30zF  -

** 31n0  BRA	(PC, Rn)			//Branch to address given in (PC, Rn)
** 31n1  BSR	(PC, Rn)			//Branch Subroutine given by (PC, Rn)
** 31n2  BT		(PC, Rn)			//Branch if True to (PC, Rn)
** 31n3  BF		(PC, Rn)			//Branch if False to (PC, Rn)
** 31n4  -
** 31n5  -
** 31n6  -
** 31n7  -
** 31n8  -
** 31n9  -
** 31nA  -
** 31nB  -
** 31nC  INVIC	Rn					//Flush I-Cache for Address
** 31nD  INVDC	Rn					//Flush D-Cache for Address
** 31nE  -
** 31nF  -

** 32n0  JMP	Rn					//Branch to address given in Rn
** 32n1  JSR	Rn					//Branch Subroutine given by Rn
** 32n2  JT		Rn
** 32n3  JF		Rn
** 32n4  EXTU.B	Rn
** 32n5  EXTU.W	Rn
** 32n6  EXTS.B	Rn
** 32n7  EXTS.W	Rn

** 32n8  BRA.B	(PC, Rn)			//(Op24) Byte Aligned Branch
** 32n9  BSR.B	(PC, Rn)			//(Op24) Byte Aligned Branch
** 32nA  BT.B	(PC, Rn)			//(Op24) Byte Aligned Branch
** 32nB  BF.B	(PC, Rn)			//(Op24) Byte Aligned Branch
** 32nC  BRA.L	(PC, Rn)			//DWord Aligned Branch
** 32nD  BSR.L	(PC, Rn)			//DWord Aligned Branch
** 32nE  BT.L	(PC, Rn)			//DWord Aligned Branch
** 32nF  BF.L	(PC, Rn)			//DWord Aligned Branch

** 33n0  NOT	Rn					//Rn=~Rn
** 33n1  NEG	Rn					//Rn=(~Rn)+1
** 33n2  NEGC	Rn					//Rn=(~Rn)+(~SR.T)
** 33n3  MOVNT	Rn					//Rn=!SR.T
** 33n4  -
** 33n5  -
** 33n6  -
** 33n7  -
** 33n8  -
** 33n9  -
** 33nA  -
** 33nB  -
** 33nC  NEG	Rn, DLR				//DLR=(~Rn)+1
** 33nD  -
** 33nE  -
** 33nF  -

** 34zz  -

** 35zz  -

** 36n0  -
** 36n1  -
** 36n2  -
** 36j3  TRAP		Imm4u			//Generate an Interrupt
** 36n4  EXTU.L		Rn				//Zero Extend DWord to QWord
** 36n5  EXTS.L		Rn				//Sign Extend DWord to QWord
** 36n6  SHAD		DLR, Rn			//Barrel Shift, Arithmetic
** 36n7  SHLD		DLR, Rn			//Barrel Shift, Logical
** 36n8  TRAP		Rn				//Raise an Exception.
** 36n9  WEXMD		Imm4			//Set WEX Profile
** 36jA  CPUID		Imm4			//Load CPUID bits into DHR:DLR
** 36nB  SRTTWID	Imm4			//Twiddle SR.T
** 36nC  -
** 36nD  CMPHS		DLR, Rn			//Unsigned (Rn GE DLR)
** 36nE /? CMPGE	DLR, Rn			//Signed (Rn GE DLR)
** 36nF  MOVT		Rn				//Rn=SR.T

** 37zz  -

** 38zz (Mirror 30zz, Rn=R16..R31)
** 39zz (Mirror 31zz, Rn=R16..R31)
** 3Azz (Mirror 32zz, Rn=R16..R31)
** 3Bzz (Mirror 33zz, Rn=R16..R31)
** 3Czz (Mirror 34zz, Rn=R16..R31)
** 3Dzz (Mirror 35zz, Rn=R16..R31)
** 3Ezz (Mirror 36zz, Rn=R16..R31)
** 3Fzz (Mirror 37zz, Rn=R16..R31)

* 4zzz
** 40nd  MOV.L		Rn, (SP, disp4u)	//Stack-Relative Store
** 41nd  MOV.Q		Rn, (SP, disp4u)	//Stack-Relative Store
** 42nd  MOV.L		Rk, (SP, disp4u)	//Stack-Relative Store
** 43nd  MOV.Q		Rk, (SP, disp4u)	//Stack-Relative Store
** 44nd  MOV.L		(SP, disp4u), Rn	//Stack-Relative Load
** 45nd  MOV.Q		(SP, disp4u), Rn	//Stack-Relative Load
** 46nd  MOV.L		(SP, disp4u), Rk	//Stack-Relative Load
** 47nd  MOV.Q		(SP, disp4u), Rk	//Stack-Relative Load
** 48nm  MOV		Rm, Cn				//Cn=Rm
** 49nm  MOV		Cm, Rn				//Rn=Cm
** 4Anm ? MOV		Rm, Sn				//Store to Shadow Register
** 4Bnm ? MOV		Sm, Rn				//Load from Shadow Register
** 4Cnm  LEA.B		(Rm, DLR), Rn
** 4Dnm  LEA.W		(Rm, DLR), Rn
** 4Enm  LEA.L		(Rm, DLR), Rn
** 4Fnm  LEA.Q		(Rm, DLR), Rn

* 5zzz (Binary Ops)
** 50zz  MOVU.B		(Rm), Rn
** 51zz  MOVU.W		(Rm), Rn
** 52zz  MOVU.B		(Rm, DLR), Rn
** 53zz  MOVU.W		(Rm, DLR), Rn
** 54zz	 TSTQ		Rm, Rn			//SR.T=!(Rm&Rn)
** 55nm  CMPQEQ		Rm, Rn			//Rn==Rm, Quad
** 56nm  /
** 57nm  /
** 58nm  ADD		Rm, DLR, Rn		//Rn=Rm+DLR
** 59nm  SUB		Rm, DLR, Rn
** 5Anm  MULS		Rm, DLR, Rn		//Rn=Rm*DLR (32-bit, Signed Result)
** 5Bnm  CMPQHI		Rm, Rn			//Unsigned Rn GT Rm, Quad
** 5Cnm  CMPQGT		Rm, Rn			//Signed Rn GT Rm, Quad
** 5Dnm  AND		Rm, DLR, Rn
** 5Enm  OR			Rm, DLR, Rn
** 5Fnm  XOR		Rm, DLR, Rn

* 6zzz  (Additional Ops)
** 60nm  FADD		Rm, Rn			//(GFP) FADD
** 61nm  FSUB		Rm, Rn			//(GFP) FSUB
** 62nm  FMUL		Rm, Rn			//(GFP) FMUL
** 63nm  FLDCF		Rm, Rn			//(GFP) Single->Double
** 64nm  FCMPEQ		Rm, Rn			//(GFP) FCMPEQ
** 65nm  FCMPGT		Rm, Rn			//(GFP) FCMPGT
** 66nm  FSTCF		Rm, Rn			//(GFP) Double->Single
** 67nz  -
** 68nj  ADD		Imm4u, Rk
** 69nj  ADD		Imm4n, Rk
** 6Anj  LDIZ		Imm4u, Rk
** 6Bnj  LDIN		Imm4n, Rk
** 6Cnj  CMPEQ		Imm4u, Rk		//Rn==Imm4, Zero Extend
** 6Dnj  CMPEQ		Imm4n, Rk		//Rn==Imm4, One Extend
** 6Enj  CMPGT		Imm4u, Rk		//Rn==Imm4, Zero Extend
** 6Fnj  CMPGE		Imm4u, Rk		//Rn==Imm4, Zero Extend

* 7zzz  ( XGPR Escape )
** 7wnm-ZeoZ (F0 Block)

* 8zzz ( MOV.L 1000-rddd-nnnn-mmmm, r=Store/Load )
** 80nm  MOVU.L		(Rm), Rn
** 81nm  MOV.L		Rn, (Rm,  4)
** 82nm  MOV.L		Rn, (Rm,  8)
** 83nm  MOV.L		Rn, (Rm, 12)
** 84nm  MOV.L		Rn, (Rm, 16)
** 85nm  MOV.L		Rn, (Rm, 20)
** 86nm  MOV.L		Rn, (Rm, 24)
** 87nm  MOV.L		Rn, (Rm, 28)
** 88nm  MOVU.L		(Rm, DLR), Rn
** 89nm  MOV.L		(Rm,  4), Rn
** 8Anm  MOV.L		(Rm,  8), Rn
** 8Bnm  MOV.L		(Rm, 12), Rn
** 8Cnm  MOV.L		(Rm, 16), Rn
** 8Dnm  MOV.L		(Rm, 20), Rn
** 8Enm  MOV.L		(Rm, 24), Rn
** 8Fnm  MOV.L		(Rm, 28), Rn

* 9zzz  ( XGPR Escape )
** 9wnm-Zeii (F1 and F2 Block)

* Ajjj  LDIZ		Imm12u, DLR	//Load 12 bit value into DLR (Zero Extend)
* Bjjj  LDIN		Imm12u, DLR	//Load 12 bit value into DLR (One Extend)
* Cnii  ADD			Imm8s, Rn		//Rn=Rn+Imm8
* Dnii  LDI			Imm8s, Rn		//Rn=Imm8
* Ezzz  (Escape32, Predicate Block)
* Fzzz  (Escape32, Normal Block)



=== 32-bit Instruction Forms ===


Major Ranges
* F0nm_XeoX (Basic Instructions, Partially mirrors 16-bit space)
* F1nm_Xedd (Load/Store, Disp9)
* F2nm_Xejj (Imm9 / Imm10 ops)
* F3zz_zzzz (Reserved / User Block)
* F4zz_zzzz (Repeat F0zz, WEX2 Hint)
* F5zz_zzzz (Repeat F1zz, WEX2 Hint)
* F6zz_zzzz (Repeat F2zz, WEX2 Hint)
* F7zz_zzzz (Repeat F3zz, WEX2 Hint)
* F8Xn_jjjj (Imm16 Instructions)
* F9nm_XeoX (Reserved, More 3R Space)
* FAjj_jjjj (LDIZ Imm24, DLR)
* FBjj_jjjj (LDIN Imm24, DLR)
* FCzz_zzzz (Repeat F8zz, WEX2 Hint)
* FDzz_zzzz (Repeat F9zz, WEX2 Hint)
* FEzz_zzzz (Jumbo Imm24)
* FFzz_zzzz (Jumbo Op64)

Additionally, Predicated Ranges will exist:
* E0zz_zzzz (Execute if True, Repeats F0)
* E1zz_zzzz (Execute if True, Repeats F1)
* E2zz_zzzz (Execute if True, Repeats F2)
* E3zz_zzzz (Execute if True, Repeats F3)
* E4zz_zzzz (Execute if False, Repeats F0)
* E5zz_zzzz (Execute if False, Repeats F1)
* E6zz_zzzz (Execute if False, Repeats F2)
* E7zz_zzzz (Execute if False, Repeats F3)
* E8zz_zzzz (Execute if True, Repeats F8)
* E9zz_zzzz (Execute if True, Repeats F9)
* EAzz_zzzz (PrWEX, Repeats F0, Execute if True)
* EBzz_zzzz (PrWEX, Repeats F2, Execute if True)
* ECzz_zzzz (Execute if False, Repeats F8)
* EDzz_zzzz (Execute if False, Repeats F9)
* EEzz_zzzz (PrWEX, Repeats F0, Execute if False)
* EFzz_zzzz (PrWEX, Repeats F2, Execute if False)

The ranges FAzz and FBzz, may not be encoded in predicated forms.
The encoding of predicated forms will be otherwise equivalent to their Fz counterparts.


==== F0zz Instruction Block ====

F0zz Instruction Block:
* F0zz_0zzz
** F0nm_0gd0  MOV.B		Rn, (Rm, Disp5)		//(PrWEX + WEXMV | XGPR)
** F0nm_0Gd0  LEA.B		(Rm, Disp5), Rn		//(PrWEX + WEXMV | XGPR)
** F0nm_0gd1  MOV.W		Rn, (Rm, Disp5)		//(PrWEX + WEXMV | XGPR)
** F0nm_0Gd1  LEA.W		(Rm, Disp5), Rn		//(PrWEX + WEXMV | XGPR)
** F0nm_0gd2  MOV.L		Rn, (Rm, Disp5)		//(PrWEX + WEXMV | XGPR)
** F0nm_0Gd2  LEA.L		(Rm, Disp5), Rn		//(PrWEX + WEXMV | XGPR)
** F0nm_0gd3  MOV.Q		Rn, (Rm, Disp5)		//(PrWEX + WEXMV | XGPR)
** F0nm_0Gd3  LEA.Q		(Rm, Disp5), Rn		//(PrWEX + WEXMV | XGPR)
** F0nm_0go4  MOV.B		Rn, (Rm, Ro)		//Q=0
** F0nm_0Go4  LEA.B		(Rm, Ro), Rn		//Q=1
** F0nm_0go5  MOV.W		Rn, (Rm, Ro)		//Q=0
** F0nm_0Go5  LEA.W		(Rm, Ro), Rn		//Q=1
** F0nm_0go6  MOV.L		Rn, (Rm, Ro)		//Q=0
** F0nm_0Go6  LEA.L		(Rm, Ro), Rn		//Q=1
** F0nm_0go7  MOV.Q		Rn, (Rm, Ro)		//Q=0
** F0nm_0Go7  LEA.Q		(Rm, Ro), Rn		//Q=1
** F0nm_0gd8  MOV.B		(Rm, Disp5), Rn		//(PrWEX + WEXMV | XGPR)
** F0nm_0Gd8  MOVU.B	(Rm, Disp5), Rn		//(PrWEX + WEXMV | XGPR)
** F0nm_0gd9  MOV.W		(Rm, Disp5), Rn		//(PrWEX + WEXMV | XGPR)
** F0nm_0Gd9  MOVU.W	(Rm, Disp5), Rn		//(PrWEX + WEXMV | XGPR)
** F0nm_0gdA  MOV.L		(Rm, Disp5), Rn		//(PrWEX + WEXMV | XGPR)
** F0nm_0GdA  MOVU.L	(Rm, Disp5), Rn		//(PrWEX + WEXMV | XGPR)
** F0nm_0gdB  MOV.Q		(Rm, Disp5), Rn		//(PrWEX + WEXMV | XGPR)
** F0nm_0GdB  LDTEX		(Rm, Ro), Rn		//(LDTEX)
** F0nm_0goC  MOV.B		(Rm, Ro), Rn		//Q=0
** F0nm_0GoC  MOVU.B	(Rm, Ro), Rn		//Q=1
** F0nm_0goD  MOV.W		(Rm, Ro), Rn		//Q=0
** F0nm_0GoD  MOVU.W	(Rm, Ro), Rn		//Q=1
** F0nm_0goE  MOV.L		(Rm, Ro), Rn		//Q=0
** F0nm_0GoE  MOVU.L	(Rm, Ro), Rn		//Q=1
** F0nm_0goF  MOV.Q		(Rm, Ro), Rn		//Q=0
** F0nm_0GoF ? LDTEX2

* F0zz_1zzz
** F0nm_1go0  ADD		Rm, Ro, Rn			//Rn=Rm+Ro
** F0nm_1Go0  ADDX		Xm, Xo, Xn			//(ALUX) Xn=Xm+Xo
** F0nm_1go1  SUB		Rm, Ro, Rn			//Rn=Rm-Ro
** F0nm_1Go1  SUBX		Xm, Xo, Xn			//(ALUX) Xn=Xm-Xo
** F0nm_1go2  MULS		Rm, Ro, Rn			//Rn=Rm*Ro (Signed Result)
** F0nm_1Go2  MULS.Q	Rm, Ro, Rn			//(MULQ) Signed 64b Result
** F0nm_1go3  MULU		Rm, Ro, Rn			//Rn=Rm*Ro (Unsigned Result)
** F0nm_1Go3  MULU.Q	Rm, Ro, Rn			//(MULQ) Unsigned 64b Result
** F0nm_1go4 ? MIN		Rm, Ro, Rn			//(MINMAX) Minimum Value
** F0nm_1Go4 ? MAX		Rm, Ro, Rn			//(MINMAX) Maximum Value
** F0nm_1go5  AND		Rm, Ro, Rn			//Rn=Rm AND Ro
** F0nm_1Go5  ANDX		Xm, Xo, Xn			//(ALUX) Xn=Xm&Xo
** F0nm_1go6  OR		Rm, Ro, Rn			//Rn=Rm OR  Ro
** F0nm_1Go6  ORX		Xm, Xo, Xn			//(ALUX) Xn=Xm|Xo
** F0nm_1go7  XOR		Rm, Ro, Rn			//Rn=Rm XOR Ro
** F0nm_1Go7  XORX		Xm, Xo, Xn			//(ALUX) Xn=Xm^Xo

** F0nm_1ez8
*** F0nm_1g08 BNDCHK.B	Rm, Rn				//Bounds Check (Byte Scale)
*** F0nm_1G08 -
*** F0nm_1g18 BNDCHK.W	Rm, Rn				//Bounds Check (Word Scale)
*** F0nm_1G18 -
*** F0nm_1g28 BNDCHK.L	Rm, Rn				//Bounds Check (DWord Scale)
*** F0nm_1G28 -
*** F0nm_1g38 BNDCHK.Q	Rm, Rn				//Bounds Check (QWord Scale)
*** F0nm_1G38 -

*** F0nm_1g48 BNDCHK	RmImm6u, Rn			//Bounds Compare
*** F0nm_1G48 ? MULU.X	Xm, Xn				//(MULX) Xn=Xn*Xm
*** F0nm_1g58 BNDCMP	RmImm6u, Rn			//Bounds Compare
*** F0nm_1G58 ? MULHU.X	Xm, Xn				//(MULX) Xn=(Xn*Xm)>>128
*** F0nm_1g68 VSKG		Rm, Rn				//Canary Generate
*** F0nm_1G68 ? DIVU.X	Xm, Xn				//(MULX) Xn=Xn/Xm
*** F0nm_1g78 VSKC		Rm, Rn				//Canary Check
*** F0nm_1G78 ? REMU.X	Xm, Xn				//(MULX) Xn=Xn%Xm

*** F0nm_1g88 BCDADC	Rm, Rn				//(BCD) BCD ADC
*** F0nm_1G88 BCDADCX	Xm, Xn				//(BCDX) BCD ADC
*** F0nm_1g98 BCDSBB	Rm, Rn				//(BCD) BCD SBB
*** F0nm_1G98 BCDSBBX	Xm, Xn				//(BCDX) BCD SBB

*** F0nm_1gA8 MOVZT		Rm, Rn				//MOV, Zero Tag
*** F0nm_1GA8 XMOVZT	Xm, Xn				//XMOV, Zero Tag
*** F0nm_1gB8 SNIPEDC	Rm, Rn				//Calculate L1 D$ Snipe Address
*** F0nm_1GB8 SNIPEIC	Rm, Rn				//Calculate L1 I$ Snipe Address

*** F0nm_1gC8 CMPTAEQ	Rm, Rn				//(TTAG) Zx(Rn[59:48]) EQ Rm
*** F0nm_1GC8 CMPXEQ	Rm, Rn				//(ALUX) Rn==Rm
*** F0nm_1gD8 CMPTAHI	Rm, Rn				//(TTAG) Zx(Rn[59:48]) GT Rm
*** F0nm_1GD8 CMPXHI	Rm, Rn				//(ALUX) Unsigned Rn GT Rm
*** F0nm_1gE8 CMPTAHS	Rm, Rn				//(TTAG) Zx(Rn[59:48]) GE Rm
*** F0nm_1GE8 CMPXGT	Rm, Rn				//(ALUX) Signed Rn GT Rm
*** F0nm_1gF8 CONVFXI	Rm, Rn				//Convert Int64 -> Fixint
*** F0nm_1GF8 CONVFLI	Rm, Rn				//Convert Binary64 -> Flonum

** F0nm_1ez9
*** F0nm_1g09 /? ADD	Rm, Rn				//Rn=Rn+Rm
*** F0nm_1G09 /? ADD.L	Rm, Rn				//Rn=Rn+Rm (Low 32)
*** F0nm_1g19 /? SUB	Rm, Rn				//Rn=Rn-Rm
*** F0nm_1G19 /? SUB.L	Rm, Rn				//Rn=Rn-Rm (Low 32)
*** F0nm_1g29 ADC		Rm, Rn				//Add with Carry (64b)
*** F0nm_1G29 ADC.L		Rm, Rn				//Add with Carry (32b)
*** F0nm_1g39 SBB		Rm, Rn				//Subtract with Borrow (64b)
*** F0nm_1G39 SBB.L		Rm, Rn				//Subtract with Borrow (32b)
*** F0nm_1e49 TST{Q}	Rm, Rn				//SR.T=!(Rm&Rn)
*** F0nm_1g59 /? AND	Rm, Rn
*** F0nm_1g69 /? OR		Rm, Rn				//
*** F0nm_1g79 /? XOR	Rm, Rn				//
*** F0nm_1g89 MOV		Rm, Rn				//Rn=Rm
*** F0nm_1G89 MOVX		Rm, Rn				//MOV, 128-bit
*** F0nm_1g99 MOV		Sm, Sn
*** F0nm_1G99 CMPNANTEQ	RmImm6u, Rn			//(TTAG) Compare NaN-Tag
*** F0nm_1gA9 MOV		Rm, Cn				//Cn=Rm
*** F0nm_1GA9 SETTRAP	Rn, (Rm)			//(Opt/Dbg) Set Trap Mode
*** F0nm_1gB9 MOV		Cm, Rn				//Rn=Cm
*** F0nm_1GB9 CMPTTEQ	RmImm6u, Rn			//(TTAG) Compare Type-Tag
*** F0nm_1gC9 CMPEQ		Rm, Rn				//Rn==Rm
*** F0nm_1GC9 CMPQEQ	Rm, Rn				//Rn==Rm
*** F0nm_1gD9 CMPHI		Rm, Rn				//Unsigned Rn GT Rm
*** F0nm_1GD9 CMPQHI	Rm, Rn				//Unsigned Rn GT Rm
*** F0nm_1gE9 CMPGT		Rm, Rn				//Signed Rn GT Rm
*** F0nm_1GE9 CMPQGT	Rm, Rn				//Signed Rn GT Rm
*** F0nm_1gF9 /
*** F0nm_1GF9 /

** F0nm_1ezA  (GSV Block)
*** F0nm_1g0A /
*** F0nm_1G0A /
*** F0nm_1g1A /
*** F0nm_1G1A /
*** F0nm_1g2A /
*** F0nm_1G2A /
*** F0nm_1g3A /
*** F0nm_1G3A /
*** F0nm_1g4A -
*** F0nm_1g5A / PADD.H		Rm, Rn			//(GSVF) Packed ADD Half
*** F0nm_1G5A / PADD.F		Rm, Rn			//(GSVF) Packed ADD Float
*** F0nm_1g6A / PSUB.H		Rm, Rn			//(GSVF) Packed SUB Half
*** F0nm_1G6A / PSUB.F		Rm, Rn			//(GSVF) Packed SUB Float
*** F0nm_1g7A / PMUL.H		Rm, Rn			//(GSVF) Packed MUL Half
*** F0nm_1G7A / PMUL.F		Rm, Rn			//(GSVF) Packed MUL Float
*** F0nm_1g8A / MOVHD		Rm, Rn			//(GSV) Move High DWord
*** F0nm_1G8A / MOVLD		Rm, Rn			//(GSV) Move Low DWord
*** F0ni_1g9A ? PDOTT.H		Imm5u, Rn		//(GSVF? Predict Dot Predicate)
*** F0ni_1G9A ? PDOTT.F		Imm5u, Rn		//(GSVF? Predict Dot Predicate)
*** F0nm_1gAA  PCMPEQ.H		Rm, Rn			//Packed Compare Half, Equal
*** F0nm_1GAA  PCMPEQ.F		Rm, Rn			//Packed Compare Single, Equal
*** F0nm_1gBA  PCMPGT.H		Rm, Rn			//Packed Compare Half, Greater
*** F0nm_1GBA  PCMPGT.F		Rm, Rn			//Packed Compare Single, Greater
*** F0nm_1gCA  PCMPEQ.W		Rm, Rn			//Pack Compare Word, Equal
*** F0nm_1GCA  PCMPEQ.L		Rm, Rn			//Pack Compare DWord, Equal
*** F0nm_1gDA  PCMPHI.W		Rm, Rn			//Pack Compare Word, Above
*** F0nm_1GDA  PCMPHI.L		Rm, Rn			//Pack Compare DWord, Above
*** F0nm_1gEA  PCMPGT.W		Rm, Rn			//Pack Compare Word, Greater
*** F0nm_1GEA  PCMPGT.L		Rm, Rn			//Pack Compare DWord, Greater
*** F0nm_1gFA  ? BSWAPU.L	Rm, Rn			//(TTAG) Byte Swap DWORD
*** F0nm_1GFA  ? BSWAP.Q	Rm, Rn			//(TTAG) Byte Swap QWORD

** F0nm_1ezB 
*** F0nm_1e0B  -
*** F0nm_1e1B  -
*** F0nm_1e2B  -
*** F0nm_1e3B  -
*** F0nm_1e4B  -
*** F0nm_1e5B  -
*** F0nm_1e6B  -
*** F0nm_1e7B  -
*** F0nm_1e8B  -
*** F0nm_1e9B  -
*** F0nm_1eAB  -
*** F0nm_1eBB  -
*** F0nm_1eCB  -
*** F0nm_1eDB  -
*** F0nm_1eEB  -
*** F0nm_1eFB  -

** F0nm_1ezC
*** F0nm_1g0C  NOT		Rm, Rn				//Rn=~Rn
*** F0nm_1G0C  NOTX		Xm, Xn				//(ALUX) Xn=~Xm
*** F0nm_1g1C  NEG		Rm, Rn				//Rn=(~Rn)+1
*** F0nm_1G1C  NEGX		Xm, Xn				//(ALUX) Xn=(~Xm)+1
*** F0nm_1e2C  CLZ{Q}	Rm, Rn				//(CLZ) Count Leading Zeroes
*** F0nm_1e3C  CTZ{Q}	Rm, Rn				//(CLZ) Count Trailing Zeroes
*** F0nm_1e4C  BTRNS{Q}	Rm, Rn				//(CLZ) Bit Transpose
*** F0nm_1g5C  EXTS.L	Rm, Rn				//Q=0
*** F0nm_1G5C  EXTU.L	Rm, Rn				//Q=1
*** F0nm_1e6C  SHAD{Q}	Rm, Rn				//Barrel Shift, Arithmetic
*** F0nm_1e7C  SHLD{Q}	Rm, Rn				//Barrel Shift, Logical
*** F0nm_1g8C  EXTS.B	Rm, Rn				//Q=0, I=0
*** F0nm_1G8C  EXTU.B	Rm, Rn				//Q=1, I=0
*** F0nm_1g9C  EXTS.W	Rm, Rn				//Q=0, I=0
*** F0nm_1G9C  EXTU.W	Rm, Rn				//Q=1, I=0

*** F0nm_1gAC  MOV		Rm, Sn				//Sn=Rm
*** F0nm_1GAC  MOVX		Xm, XCn				//(XMOV) XCn=Xm
*** F0nm_1gBC  MOV		Sm, Rn				//Rn=Sm
*** F0nm_1GBC  MOVX		XCm, Xn				//(XMOV) Xn=XCm

*** F0nm_1eCC / CLNZ{Q}	Rm, Rn				//(CLZ) Count Leading Ones
*** F0nm_1eDC / CTNZ{Q}	Rm, Rn				//(CLZ) Count Trailing Ones
*** F0nm_1eEC  CMP{Q}GE	Rm, Rn				//Signed Rn GE Rm
*** F0nm_1eFC  CMP{Q}HS	Rm, Rn				//Unsigned Rn GE Rm

** F0nm_1ezD ? (GFP, GPR FPU, Opt)
*** F0nm_1g0D  FLDCF		Rm, Rn		//Load Convert Float32 (Low Bits, ZX)
*** F0nm_1G0D  FLDCDX		Rm, Xn		//Load Convert Double (GFPX, F128)
*** F0nm_1e1D  FLDCHF		Rm, Rn		//Load Convert Float32 (High Bits)
*** F0nm_1g2D  FLDCI		Rm, Rn		//Load Convert Int64
*** F0nm_1G2D  FLDCXI		Rm, Xn		//Load Convert Int64 (GFPX, F128)
*** F0nm_1e3D  FLDCH		Rm, Rn		//Load Convert Half (Low16)
*** F0nm_1g4D  FSTCF		Rm, Rn		//Store Convert Float32 (Low Bits, ZX)
*** F0nm_1G4D  FSTCDX		Xm, Rn		//Store Convert Double (GFPX, F128)
*** F0nm_1e5D  FSTCHF		Rm, Rn		//Store Convert Float32 (High Bits)
*** F0nm_1g6D  FSTCI		Rm, Rn		//Store Convert Int64
*** F0nm_1G6D  FSTCXI		Xm, Rn		//Store Convert Int64 (GFPX, F128)
*** F0nm_1e7D  FSTCH		Rm, Rn		//Store Convert Half (Low16)
*** F0nm_1g8D  FNEG			Rm, Rn		//(GFP ) Negate
*** F0nm_1g9D  FABS			Rm, Rn		//(GFP ) Absolute
*** F0nm_1gAD  FCMPEQ		Rm, Rn		//(GFP ) SR.T=(FRn EQ FRm)
*** F0nm_1GAD  FCMPXEQ		Xm, Xn		//(GFPX) SR.T=(FRn EQ FRm)
*** F0nm_1gBD  FCMPGT		Rm, Rn		//(GFP ) SR.T=(FRn GT FRm)
*** F0nm_1GBD  FCMPXGT		Xm, Xn		//(GFPX) SR.T=(FRn GT FRm)
*** F0nm_1gCD ? FSQRT		Rm, Rn		//(FDIV) Square Root
*** F0nm_1GCD ? FSQRTX		Xm, Xn		//(FDIV) Square Root (GFPX)
*** F0nm_1gDD ? FSQRTA		Rm, Rn		//(FDIV) Square Root (Approx)
*** F0nm_1GDD ? FSQRTXA		Xm, Xn		//(FDIV) Square Root (GFPX, Approx)
*** F0nm_1gED ? FCMPGE		Rm, Rn		//(GFP ) SR.T=(FRn GE FRm)
*** F0nm_1GED ? FCMPXGE		Xm, Xn		//(GFPX) SR.T=(FRn GE FRm)
*** F0nm_1gFD  FLDCIU		Rm, Rn		//Load Convert UInt64
*** F0nm_1GFD  FLDCXIU		Rm, Rn		//Load Convert UInt64 (GFPX, F128)

** F0nm_1ezE ? (RGB 2R Block)
*** F0nm_1g0E  RGB5SHR1		Rm, Rn		//(RGB) RGB555 Shift Right
*** F0nm_1G0E ? RGB5MINMAX	Rm, Rn		//(RGB) RGB555 Min and Max
*** F0nm_1g1E  PMORT.L		Rm, Rn		//(GSV) Morton Shuffle
*** F0nm_1G1E  PMORT.Q		Rm, Rn		//(GSV) Morton Shuffle
*** F0nm_1g2E  RGB5PCK32	Rm, Rn		//(RGB) RGB555 Pack from RGB32
*** F0nm_1G2E  RGB5PCK64	Rm, Rn		//(RGB) RGB555 Pack from RGB64
*** F0nm_1g3E  RGB5UPCK32	Rm, Rn		//(RGB) RGB555 Unpack to RGB32
*** F0nm_1G3E  RGB5UPCK64	Rm, Rn		//(RGB) RGB555 Unpack to RGB64
*** F0nm_1G4E  RGB32PCK64	Rm, Rn		//(RGB) RGB32 Pack from RGB64
*** F0nm_1G5E  RGB32UPCK64	Rm, Rn		//(RGB) RGB32 Unpack to RGB64
*** F0nm_1g6E ? PSHAL.W		Rm, Rn		//Packed Shift Left 1-bit
*** F0nm_1G6E ? PSHLL.W		Rm, Rn
*** F0nm_1g7E ? PSHAR.W		Rm, Rn		//Packed Shift Right 1-bit
*** F0nm_1G7E ? PSHLR.W		Rm, Rn

*** F0nm_1g8E  PLDCM8SH		Rm, Rn		//(GSVF) RGB32SF Unpack to RGB64F
*** F0nm_1G8E  PLDCM8UH		Rm, Rn		//(GSVF) RGB32UF Unpack to RGB64F
*** F0nm_1g9E  PLDCM30AH	Rm, Rn		//(RGBF) RGB30A Unpack to RGB64F
*** F0nm_1G9E ? PLDCXH		Rm, Xn		//(GSVFX?) Packed Half to Single (4x)
*** F0nm_1gAE  PSTCM8SH		Rm, Rn		//(GSVF) RGB32SF Pack from RGB64F
*** F0nm_1GAE  PSTCM8UH		Rm, Rn		//(GSVF) RGB32UF Pack from RGB64F
*** F0nm_1gBE  PSTCM30AH	Rm, Rn		//(RGBF) RGB30A Pack from RGB64F
*** F0nm_1GBE ? PSTCXH		Xm, Rn		//(GSVFX?) Packed Half to Single (4x)
*** F0nm_1gCE  PLDCH		Rm, Rn		//(GSVF) Packed Half to Single (2x)
*** F0nm_1GCE  PLDCHH		Rm, Rn		//(GSVF) Packed Half to Single (Hi)
*** F0nm_1gDE  PLDCEHL		Rm, Rn		//(GSVF) Packed Ext-Half to Single (Lo)
*** F0nm_1GDE  PLDCEHH		Rm, Rn		//(GSVF) Packed Ext-Half to Single (Hi)
*** F0nm_1gEE  PSTCH		Rm, Rn		//(GSVF) Packed Single to Half (2x)
*** F0nm_1GEE ? XBLESS		Xm, Xn		//(BCE) Bless a Capability.
*** F0nm_1gFE ? MOVST		Rm, Rn		//(TTAG) Move Sign-Extend Low 48 bits
*** F0nm_1GFE ? XMOVST		Xm, Xn		//(TTAG) Move Sign-Extend Low 48 bits

** F0nm_1ezF  -
*** F0nm_1g0F  PCVTSB2HL	Rm, Rn		//(GSVF) Packed SB to FP16 (Low32)
*** F0nm_1G0F  PCVTUB2HL	Rm, Rn		//(GSVF) Packed UB to FP16
*** F0nm_1g1F  PCVTSB2HH	Rm, Rn		//(GSVF) Packed SB to FP16 (High32)
*** F0nm_1G1F  PCVTUB2HH	Rm, Rn		//(GSVF) Packed UB to FP16
*** F0nm_1g2F  PCVTSW2FL	Rm, Rn		//(GSVF) Packed SW to FP32 (Low32)
*** F0nm_1G2F  PCVTUW2FL	Rm, Rn		//(GSVF) Packed UW to FP32
*** F0nm_1g3F  PCVTSW2FH	Rm, Rn		//(GSVF) Packed SW to FP32 (High32)
*** F0nm_1G3F  PCVTUW2FH	Rm, Rn		//(GSVF) Packed UW to FP32

*** F0nm_1g4F  PCVTH2SB		Rm, Rn		//(GSVF) Packed FP16 to SB (4x)
*** F0nm_1G4F  PCVTH2UB		Rm, Rn		//(GSVF) Packed FP16 to UB (4x)
*** F0nm_1g5F  PCVTSW2H		Rm, Rn		//(GSVF) Packed SW to FP16 (4x)
*** F0nm_1G5F  PCVTUW2H		Rm, Rn		//(GSVF) Packed UW to FP16 (4x)
*** F0nm_1g6F  PCVTF2SW		Rm, Rn		//(GSVF) Packed FP32 to SW (2x)
*** F0nm_1G6F  PCVTF2UW		Rm, Rn		//(GSVF) Packed FP32 to UW (2x)
*** F0nm_1g7F  PCVTH2SW		Rm, Rn		//(GSVF) Packed FP16 to SW (4x)
*** F0nm_1G7F  PCVTH2UW		Rm, Rn		//(GSVF) Packed FP16 to UW (4x)

*** F0nm_1g8F  PSQRTA.H		Rm, Rn		//(GSVF) SQRTA (4x FP16)
*** F0nm_1G8F  PSQRTUA.H	Rm, Rn		//(GSVF) SQRTA (4x FP16)
*** F0nm_1g9F  PSQRTA.F		Rm, Rn		//(GSVF) SQRTA (2x FP32)
*** F0nm_1G9F  PSQRTUA.F	Rm, Rn		//(GSVF) SQRTA (2x FP32)
*** F0nm_1gAF  PRCPA.H		Rm, Rn		//(GSVF) RCPA (4x FP16)
*** F0nm_1GAF  PRELU.H		Rm, Rn		//(GSVF) RELU (4x FP16)
*** F0nm_1gBF  PRCPA.F		Rm, Rn		//(GSVF) RCPA (2x FP32)
*** F0nm_1GBF  PRELU.F		Rm, Rn		//(GSVF) RELU (2x FP32)

*** F0nm_1gCF ? PCVTH2AL	Rm, Rn		//(GSVF) Packed FP16 to A-Law (4x)
*** F0nm_1GCF ? RGB5PCKI8   Rm, Rn		//(RGB?) Pack RGB555 to 8-bit
*** F0nm_1gDF ? PCVTAL2H	Rm, Rn		//(GSVF) Packed A-Law to FP16 (4x)
*** F0nm_1GDF ? RGB5UPCKI8  Rm, Rn		//(RGB?) Unpack RGB555 from 8-bit
*** F0nm_1gEF ? FLDCIU.L	Rm, Rn		//Load Convert UInt32
*** F0nm_1GEF ? FLDCXIU.L	Rm, Xn		//Load Convert UInt32 (GFPX, F128)
*** F0nm_1gEF ? FLDCIS.L	Rm, Rn		//Load Convert Int32
*** F0nm_1GEF ? FLDCXIS.L	Rm, Xn		//Load Convert Int32 (GFPX, F128)


* F0zz_2zzz (More 3R Ops)
** 
** F0nm_2go0  PADD.W	Rm, Ro, Rn			//(GSV) Packed ADD Word
** F0nm_2Go0  PADD.L	Rm, Ro, Rn			//(GSV) Packed ADD DWord
** F0nm_2go1  PSUB.W	Rm, Ro, Rn			//(GSV) Packed SUB Word
** F0nm_2Go1  PSUB.L	Rm, Ro, Rn			//(GSV) Packed SUB DWord
** F0nm_2eo2  SHAR{Q}	Rm, Ro, Rn			//Shift Arithmetic Right
** F0nm_2eo3  SHLR{Q}	Rm, Ro, Rn			//Shift Logical Right
** F0nm_2go4  PCSELT.W	Rm, Ro, Rn			//(GSV) Packed CSELT (Word)
** F0nm_2Go4  PCSELT.L	Rm, Ro, Rn			//(GSV) Packed CSELT (DWord)
** F0nm_2go5  PADD.F	Rm, Ro, Rn			//(GSVF) Packed FADD (2x Float)
** F0nm_2Go5  PADDX.F	Xm, Xo, Xn			//(GSVFX) Packed FADD (4x Float)
** F0nm_2go6  PSUB.F	Rm, Ro, Rn			//(GSVF) Packed FSUB (2x Float)
** F0nm_2Go6  PSUBX.F	Xm, Xo, Xn			//(GSVFX) Packed FSUB (4x Float)
** F0nm_2go7  PMUL.F	Rm, Ro, Rn			//(GSVF) Packed FMUL (2x Float)
** F0nm_2Go7  PMULX.F	Xm, Xo, Xn			//(GSVFX) Packed FMUL (4x Float)
** F0nm_2go8  MOVHD		Rm, Ro, Rn			//(GSV) MOV, High DWords
** F0nm_2Go8  MOVLD		Rm, Ro, Rn			//(GSV) MOV, Low DWords
** F0nm_2go9  MOVHLD	Rm, Ro, Rn			//(GSV) MOV, High and Low DWords
** F0nm_2Go9  MOVLHD	Rm, Ro, Rn			//(GSV) MOV, Low and High DWords
** F0nm_2goA  PSCHEQ.W	Rm, Ro, Rn			//(GSV) Check if Rm contains Ro
** F0nm_2GoA  PSCHEQ.B	Rm, Ro, Rn			//(GSV) Check if Rm contains Ro
** F0nm_2goB  PSCHNE.W	Rm, Ro, Rn			//(GSV) Check if Rm contains Ro
** F0nm_2GoB  PSCHNE.B	Rm, Ro, Rn			//(GSV) Check if Rm contains Ro
** F0nm_2goC  BLKUTX1	Rm, Ro, Rn			//(RGB) Extract Pixel, UTX1
** F0nm_2GoC  BLKUTX2	Rm, Ro, Rn			//(RGB) Extract Pixel, UTX2
** F0nm_2goD  PADD.H	Rm, Ro, Rn			//(GSVF) Packed FADD (4x Half)
** F0nm_2GoD  PADDX.D	Xm, Xo, Xn			//(GSVFX) Packed FADD (2x Double)
** F0nm_2goE  PSUB.H	Rm, Ro, Rn			//(GSVF) Packed FSUB (4x Half)
** F0nm_2GoE  PSUBX.D	Xm, Xo, Xn			//(GSVFX) Packed FSUB (2x Double)
** F0nm_2goF  PMUL.H	Rm, Ro, Rn			//(GSVF) Packed FMUL (4x Half)
** F0nm_2GoF  PMULX.D	Xm, Xo, Xn			//(GSVFX) Packed FMUL (2x Double)

* F0zz_3zzz (Various 1R/2R/3R Ops)

** F0zz_3en0  (Single Register Ops, Mirror 3znz)
*** 1111_0000_zzzz_zzzz_0011_qzzn_nnnn_0000 (1R)
*** 1111_0000_0000_zzzz_0011_qzzz_zzzz_0000 (0R)

*** F000_3000  NOP
*** F000_3010  RTS
*** F000_3020  SLEEP
*** F000_3030  BREAK
*** F000_3040  CLRT
*** F000_3050  SETT
*** F000_3060  CLRS
*** F000_3070  SETS
*** F000_3080  NOTT
*** F000_3090  NOTS
*** F000_30A0  -
*** F000_30B0  -
*** F000_30C0  RTE
*** F000_30D0 / DIV0		//Drop
*** F000_30E0 / DIV1		//Drop
*** F000_30F0  LDTLB

*** 3002_3000  KEYBRK_XG2	//NOP in Baseline, BREAK in XG2
*** 3002_3030  KEYBRK_BASE	//NOP in XG2, BREAK in Baseline

*** F002_3010  RTSU
*** F002_3020  SYSCALL

*** F002_3040  LDACL

*** F002_3070 ? SYNDCB	//Synchronize pipeline following DCB Update

*** F002_3080  SXENTR	//Enter Superuser Mode
*** F002_3090  SUENTR	//Enter User Mode
*** F002_30A0  SVEKRR	//Save Encoded Keyring
*** F002_30B0  SVENTR	//Enter Supervisor Mode (From Superuser Mode)
*** F002_30C0  LDEKRR	//Load Encoded Keyring
*** F002_30D0  LDEKEY	//Load Encoded Key
*** F002_30E0  LDEENC	//Encode Key
*** F002_30F0  INVTLB

*** F010_3en0  BRA		(PC, Rn)
*** F011_3en0  BSR		(PC, Rn)
*** F012_3en0 / BT		(PC, Rn)
*** F013_3en0 / BF		(PC, Rn)
*** F014_3en0  NOP3		Rn			//(WEX2) NOP with Register

*** F018_3en0  /
*** F019_3en0  /
*** F01A_3en0  /
*** F01B_3en0  /

*** F01C_3en0  INVIC	Rn
*** F01D_3en0  INVDC	Rn

*** F020_3gn0  JMP		Rn
*** F020_3Gn0  JMPX		Rn			//(XMOV) JMP
*** F021_3gn0  JSR		Rn
*** F021_3Gn0  JSRX		Rn			//(XMOV) JSR
*** F022_3gn0 / JT		Rn
*** F023_3gn0 / JF		Rn

*** F028_3en0 / BRA.B	(PC, Rn)	//(Op24) BRA, Byte Scale
*** F029_3en0 / BSR.B	(PC, Rn)	//(Op24) BSR, Byte Scale
*** F02A_3en0 / BT.B		(PC, Rn)	//(Op24) BT, Byte Scale
*** F02B_3en0 / BF.B		(PC, Rn)	//(Op24) BF, Byte Scale
*** F02C_3en0  BRA.L	(PC, Rn)	//Branch, DWord Scale
*** F02D_3en0  BSR.L	(PC, Rn)	//Branch, DWord Scale
*** F02E_3en0 / BT.L		(PC, Rn)	//Branch, DWord Scale
*** F02F_3en0 / BF.L		(PC, Rn)	//Branch, DWord Scale

*** F033_3gn0  MOVNT		Rn
*** F033_3Gn0  REGCHKG		Rn		//Debug: Generate Register Check Value
*** F034_3en0 ? ROTL		Rn
*** F035_3en0 ? ROTR		Rn
*** F036_3en0  ROTCL{Q}		Rn
*** F037_3en0  ROTCR{Q}		Rn
*** F038_3en0 ? SHLL		Rn		//Rn=Rn<<1
*** F039_3en0 ? SHLR		Rn		//Rn=Rn>>1 (Logical)
*** F03A_3en0 ? SHAR		Rn		//Rn=Rn>>1 (Arithmetic)

*** F03E_3en0  MOVST	Rn
*** F03F_3en0  MOVPQ	Rn

*** F060_3en0 / DAA		Rn			//(BCD) Decimal Adjust After ADD
*** F061_3en0 / DAS		Rn			//(BCD) Decimal Adjust After SUB

*** F068_3en0  TRAP		Rn
*** F069_3en0  WEXMD	Imm5u
*** F06A_3en0  CPUID	Imm5u
*** F06B_3en0 ? SRTTWID	Imm5u	//SR.T Twiddle
*** F06C_3en0 ? SRTPROP	Imm5u	//SR.T Predicate Op
*** F06F_3gn0  MOVT		Rn
*** F06F_3Gn0  REGCHKC	Rn		//Debug: Verify Register Check Value

*** F082_3en0  /
*** F083_3en0  /

** F0nm_3go1  MOVTT		Rm, Ri, Rn		//(TTAG) High 16 bits from Ri
** F0nm_3Go1  MOVTT		Rm, Imm5u, Rn	//(TTAG) Set Type-Tag
** F0nm_3go2  ROTLQ		Rm, Ro, Rn		//(ALU) Rotate Left (64b)
** F0nm_3Go2  SHARX		Xm, Ro, Xn		//(ALUX) Shift Arithmetic (128b)
** F0nm_3go3  ROTRQ		Rm, Ro, Rn		//(ALU) Rotate Right (64b)
** F0nm_3Go3  SHLRX		Xm, Ro, Xn		//(ALUX) Shift Logical (128b)
** F0nm_3go4  MULHS.Q	Rm, Ro, Rn		//(MULQ) MUL, High Results Signed
** F0nm_3Go4  SHADX		Xm, Ro, Xn		//(ALUX) Shift Arithmetic (128b)
** F0nm_3go5  MULHU.Q	Rm, Ro, Rn		//(MULQ) MUL, High Results Unsigned
** F0nm_3Go5  SHLDX		Xm, Ro, Xn		//(ALUX) Shift Logical (128b)
** F0nm_3go6  ROTL.L	Rm, Ro, Rn		//(ALU) Rotate Left (32b)
** F0nm_3Go6  ROTLX		Xm, Ro, Xn		//(ALUX) Rotate Left (128b)
** F0nm_3ez7  SWxP.x	Rm, Rn			//(GSV) SWAP and SWCP

** F0nm_3ez8  (Reserved 2R Block Space)
** F0nm_3ez8
*** F0nm_3g08 BNDCMP.B	Rm, Rn				//Bounds Compare (Byte Scale)
*** F0nm_3G08 -
*** F0nm_3g18 BNDCMP.W	Rm, Rn				//Bounds Compare (Word Scale)
*** F0nm_3G18 -
*** F0nm_3g28 BNDCMP.L	Rm, Rn				//Bounds Compare (DWord Scale)
*** F0nm_3G28 -
*** F0nm_3g38 BNDCMP.Q	Rm, Rn				//Bounds Compare (QWord Scale)
*** F0nm_3G38 -

*** F0nm_3g48 ? SWAPE2B.L	Rm, Rn				//(?) Swap Every 2-bits
*** F0nm_3G48 -
*** F0nm_3g58 ? SWAPE4B.L	Rm, Rn				//(?) Swap Every 4-bits
*** F0nm_3G58 -

*** F0nm_3g68 ? CMPPEQ		Rm, Rn				//(ALUPTR) Compare Pointer
*** F0nm_3G68 ? CMPPEQX		Xm, Xn				//(ALUPTR) Compare 128b Pointer
*** F0nm_3g78 ? CMPPGT		Rm, Rn				//(ALUPTR) Compare Pointer
*** F0nm_3G78 ? CMPPGTX		Xm, Xn				//(ALUPTR) Compare 128b Pointer

** F0nm_3ez9  (Reserved 2R Block Space)
** F0nm_3ezA  (Reserved 2R Block Space)
** F0nm_3ezB  (Reserved 2R Block Space)
** F0nm_3ezC  (Reserved 2R Block Space)
** F0nm_3ezD  (Reserved 2R Block Space)
** F0nm_3ezE  (Reserved 2R Block Space)
** F0nm_3ezF  (Reserved 2R Block Space)

* F0nm_4eoz (Follows a similar pattern as F0nm_0zz0)
** F0nm_4go0  MOV.X			Xn, (Rm, Disp5)		//(MOVX2) Store Pair
** F0nm_4Go0  LEAT.B		(Rm, Disp5), Rn		//(TTAG)
** F0nm_4go1 ? MOV.C		Cn, (Rm, Disp5)		//(MOVC)
** F0nm_4Go1  LEAT.W		(Rm, Disp5), Rn		//(TTAG)
** F0nm_4go2  FMOV.S		Rn, (Rm, Disp5)		//(FMOV)
** F0nm_4Go2  LEAT.L		(Rm, Disp5), Rn		//(TTAG)
** F0nm_4go3  FMOV.H		Rn, (Rm, Disp5)		//(FMOV)
** F0nm_4Go3  LEAT.Q		(Rm, Disp5), Rn		//(TTAG)
** F0nm_4go4  MOV.X			Xn, (Rm, Ro)		//(MOVX2) Store Pair
** F0nm_4Go4 ? LEAT.B		(Rm, Ro), Rn		//(TTAG)
** F0nm_4go5 ? MOV.C		Cn, (Rm, Ro)		//(MOVC)
** F0nm_4Go5 ? LEAT.W		(Rm, Ro), Rn		//(TTAG)
** F0nm_4go6  FMOV.S		Rn, (Rm, Ro)		//(FMOV)
** F0nm_4Go6 ? LEAT.L		(Rm, Ro), Rn		//(TTAG)
** F0nm_4go7  FMOV.H		Rn, (Rm, Ro)		//(FMOV)
** F0nm_4Go7 ? LEAT.Q		(Rm, Ro), Rn		//(TTAG)

** F0nm_4go8  MOV.X		(Rm, Disp5), Xn		//(MOVX2) Load Pair
** F0nm_4Go8  -
** F0nm_4go9  MOV.C		(Rm, Disp5), Cn		//(MOVC)
** F0nm_4Go9  -
** F0nm_4goA  FMOV.S	(Rm, Disp5), Rn		//(FMOV)
** F0nm_4GoA ? PMOV.4M	(Rm, Disp5), Rn		//(PMOV) 4x FP8S to Binary16
** F0nm_4goB  FMOV.H	(Rm, Disp5), Rn		//(FMOV)
** F0nm_4GoB ? PMOV.2H	(Rm, Disp5), Rn		//(PMOV) 2x Binary16 to Binary32
** F0nm_4goC  MOV.X		(Rm, Ro), Xn		//(MOVX2) Load Pair
** F0nm_4GoC  -
** F0nm_4goD  MOV.C		(Rm, Ro), Cn		//(MOVC)
** F0nm_4GoD  -
** F0nm_4goE  FMOV.S	(Rm, Ro), Rn		//(FMOV)
** F0nm_4GoE ? PMOV.4M	(Rm, Ro), Rn		//(PMOV) 4x FP8S to Binary16
** F0nm_4goF  FMOV.H	(Rm, Ro), Rn		//(FMOV)
** F0nm_4GoF ? PMOV.2H	(Rm, Ro), Rn		//(PMOV) 2x Binary16 to Binary32

* F0zz_5zzz
** F0nm_5go0  CSELT		Rm, Ro, Rn			//Rn=SR.T?Rm:Ro
** F0nm_5Go0  NOP3		Rm, Ro, Rn			//NOP (3-Register)
** F0nm_5go1  PMULS.W	Rm, Ro, Rn			//(GSV) Packed Mul (2x16->2x32)
** F0nm_5Go1  PMULU.W	Rm, Ro, Rn			//(GSV) Packed Mul (2x16->2x32)
** F0nm_5go2  DMULS.L	Rm, Ro, Rn			//(MULL) Sx 32b Mul (32*32->64)
** F0nm_5Go2  DMULS.Q	Rm, Ro, Xn			//(MULQ) Sx 64b Mul (64*64->128)
** F0nm_5go3  DMULU.L	Rm, Ro, Rn			//(MULL) Zx 32b Mul (32*32->64)
** F0nm_5Go3  DMULU.Q	Rm, Ro, Xn			//(MULQ) Zx 64b Mul (64*64->128)
** F0nm_5eo4  SHAD{Q}	Rm, Ro, Rn			//Arithmetic Shift Left
** F0nm_5eo5  SHLD{Q}	Rm, Ro, Rn			//Logical Shift Left
** F0nm_5go6  PMULS.LW	Rm, Ro, Rn			//(GSV) Packed Mul (Low Word)
** F0nm_5Go6  PMULU.LW	Rm, Ro, Rn			//(GSV) Packed Mul (Low Word)
** F0nm_5go7  PMULS.HW	Rm, Ro, Rn			//(GSV) Packed Mul (High Word)
** F0nm_5Go7  PMULU.HW	Rm, Ro, Rn			//(GSV) Packed Mul (High Word)
** F0nm_5go8  FADD		Rm, Ro, Rn			//(GFP) FADD
** F0nm_5Go8  FADDX		Xm, Xo, Xn			//(GFPX) FADD, Binary128
** F0nm_5go9  FSUB		Rm, Ro, Rn			//(GFP) FSUB
** F0nm_5Go9  FSUBX		Xm, Xo, Xn			//(GFPX) FSUB, Binary128
** F0nm_5goA  FMUL		Rm, Ro, Rn			//(GFP) FMUL
** F0nm_5GoA  FMULX		Xm, Xo, Xn			//(GFPX) FMUL, Binary128
** F0nm_5goB  FMAC		Rm, Ro, Rn			//(GFP_MAC) FMAC, Rn+=Rm*Ro
** F0nm_5GoB  FMACX		Xm, Xo, Xn			//(GFP_MAC) FMAC, Binary128
** F0nm_5goC  ADDS.L	Rm, Ro, Rn			//Rn=Rm+Ro, ADD 32-bit, Sign Extend
** F0nm_5GoC  ADDU.L	Rm, Ro, Rn			//Rn=Rm+Ro, ADD 32-bit, Zero Extend
** F0nm_5goD  SUBS.L	Rm, Ro, Rn			//Rn=Rm-Ro, ADD 32-bit, Sign Extend
** F0nm_5GoD  SUBU.L	Rm, Ro, Rn			//Rn=Rm-Ro, ADD 32-bit, Zero Extend
** F0nm_5goE  MULS.W	Rm, Ro, Rn			//(MULW) Sx 16b Mul (16*16->32)
** F0nm_5GoE  MULS.W	Rm, Imm5u, Rn		//(MULW) Sx 16b Mul (16*16->32)
** F0nm_5goF  MULU.W	Rm, Ro, Rn			//(MULW) Zx 16b Mul (16*16->32)
** F0nm_5GoF  MULU.W	Rm, Imm5u, Rn		//(MULW) Zx 16b Mul (16*16->32)

* F0zz_6zzz
** F0nm_6go0 ? MACS.L	Rm, Ro, Rn			//(DMAC) Rn=Sx(Rn+(Rm*Ro))
** F0nm_6Go0 ? MACS.L	Rm, Imm5u, Rn		//(DMAC) Rn=Sx(Rn+(Rm*Imm))
** F0nm_6go1 ? MACU.L	Rm, Ro, Rn			//(DMAC) Rn=Zx(Rn+(Rm*Ro))
** F0nm_6Go1 ? MACU.L	Rm, Imm5u, Rn		//(DMAC) Rn=Zx(Rn+(Rm*Imm))
** F0nm_6go2 ? DMACS.L	Rm, Ro, Rn			//(DMAC) Rn=Rn+(Rm*Ro)
** F0nm_6Go2 ? DMACS.L	Rm, Imm5u, Rn		//(DMAC) Rn=Rn+(Rm*Imm)
** F0nm_6go3 ? DMACU.L	Rm, Ro, Rn			//(DMAC) Rn=Rn+(Rm*Ro)
** F0nm_6Go3 ? DMACU.L	Rm, Imm5u, Rn		//(DMAC) Rn=Rn+(Rm*Imm)
** F0nm_6go4  DIVS.Q	Rm, Ro, Rn			//(MULQ) Rn=Rm/Ro (Signed)
** F0nm_6Go4  DIVU.Q	Rm, Ro, Rn			//(MULQ) Rn=Rm/Ro (Unsigned)
** F0nm_6go5  MODS.Q	Rm, Ro, Rn			//(MULQ) Rn=Rm%Ro (Signed)
** F0nm_6Go5  MODU.Q	Rm, Ro, Rn			//(MULQ) Rn=Rm%Ro (Unsigned)
** F0nm_6go6  FDIV		Rm, Ro, Rn			//(FDIV) FDIV
** F0nm_6Go6  FDIVX		Rm, Ro, Rn			//(FDIV+GFPX)
** F0nm_6go7  FDIVA		Rm, Ro, Rn			//(FDIV) FDIV Approx
** F0nm_6Go7  FDIVXA	Rm, Ro, Rn			//(FDIV+GFPX) FDIVX Approx
** F0nm_6go8 /? BLKUTX2	Rm, Ro, Rn			//(RGB) Extract Pixel, UTX2 (Alt)
** F0nm_6Go8  BLKUTX3H	Xm, Ro, Rn			//(RGBX) Extract Pixel, UTX3 HDR
** F0nm_6go9  BLERP		Rm, Ro, Rn			//(RGBX) Linear Interpolate
** F0nm_6Go9  BLKUTX3L	Xm, Ro, Rn			//(RGBX) Extract Pixel, UTX3 LDR
** F0nm_6goA  BLINTA	Rm, Ro, Rn			//(RGBX) Bilinear Interpolate (2p)
** F0nm_6GoA  BLINT		Xm, Xo, Xn			//(RGBX) Bilinear Interpolate
** F0nm_6goB  BITSEL	Rm, Ro, Rn			//(?) Rn = (Rm & Ro) | (Rn & ~Ro)
** F0nm_6GoB  BITSELX	Xm, Xo, Xn			//(?) Xn = (Xm & Xo) | (Xn & ~Xo)
** F0nm_6goC  BLKUAB1	Rm, Ro, Rn			//(UAB) Extract Sample
** F0nm_6GoC  BLKUAB2	Rm, Ro, Rn			//(UAB) Extract Sample
** F0nm_6goD  FADDG		Rm, Ro, Rn			//(GFP) FADD (Dynamic Rounding)
** F0nm_6GoD ? FADD		Rm, Imm5fp, Rn		//(FpImm) FADD (Imm5)
** F0nm_6goE  FSUBG		Rm, Ro, Rn			//(GFP) FSUB (Dynamic Rounding)
** F0nm_6GoE ? FSUB		Rm, Imm5fp, Rn		//(FpImm) FSUB (Imm5)
** F0nm_6goF  FMULG		Rm, Ro, Rn			//(GFP) FMUL (Dynamic Rounding)
** F0nm_6GoF ? FMUL		Rm, Imm5fp, Rn		//(FpImm) FMUL (Imm5)


* F0zz_7zzz
** F0nm_7go0 ? FADDA	Rm, Ro, Rn			//(GFP)
** F0nm_7Go0 ? FMIN		Rm, Ro, Rn			//(MINMAX) Binary64 Min Value
** F0nm_7go1 ? FSUBA	Rm, Ro, Rn			//(GFP)
** F0nm_7Go1 ? FMAX		Rm, Ro, Rn			//(MINMAX) Binary64 Max Value
** F0nm_7go2 ? FMULA	Rm, Ro, Rn			//(GFP)
** F0nm_7Go2  -
** F0nm_7go3  -
** F0nm_7Go3  -
** F0nm_7go4  DIVS.L	Rm, Ro, Rn			//(MULQ) Rn=Rm/Ro (Signed)
** F0nm_7Go4  DIVU.L	Rm, Ro, Rn			//(MULQ) Rn=Rm/Ro (Unsigned)
** F0nm_7go5  MODS.L	Rm, Ro, Rn			//(MULQ) Rn=Rm%Ro (Signed)
** F0nm_7Go5  MODU.L	Rm, Ro, Rn			//(MULQ) Rn=Rm%Ro (Unsigned)
** F0nm_7go6 ? RGB5CCENC	Rm, Ro, Rn		//(RGB-CCE)	Color-Cell Enc Helper
** F0nm_7Go6  -
** F0nm_7go7 ? SUB.P	Rm, Ro, Rn			//(ALUPTR) Pointer Subtract
** F0nm_7Go7 ? SUBX.P	Xm, Xo, Xn			//(ALUPTR) 128b Pointer Subtract
** F0nm_7ez8  (Reserved 2R Block Space)
** F0nm_7ez9  (Reserved 2R Block Space)
** F0nm_7ezA  (Reserved 2R Block Space)
** F0nm_7ezB  (Reserved 2R Block Space)
** F0nm_7eoC  (Op64 Extended 3R Block, F07C)
** F0nm_7eoD  (Op64 Extended 3R Block, F07D)
** F0nm_7eoE  (Op64 Extended 3R Block, F07E)
** F0nm_7eoF  (Op64 Extended 3R Block, F07F)

* F0zz_8zzz (XMOV Block)
** F0nm_8gd0  XMOV.B	Rn, (Xm, Disp5)		//
** F0nm_8Gd0  ? XLEA.B	(Xm, Disp5), Xn		//
** F0nm_8gd1  XMOV.W	Rn, (Xm, Disp5)		//
** F0nm_8Gd1  -
** F0nm_8gd2  XMOV.L	Rn, (Xm, Disp5)		//
** F0nm_8Gd2  -
** F0nm_8gd3  XMOV.Q	Rn, (Xm, Disp5)		//
** F0nm_8Gd3  XMOV.X	Xn, (Xm, Disp5)		//
** F0nm_8go4  XMOV.B	Rn, (Xm, Ro)		//
** F0nm_8Go4  ? XLEA.B	(Xm, Ro), Xn		//
** F0nm_8go5  XMOV.W	Rn, (Xm, Ro)		//
** F0nm_8Go5  -
** F0nm_8go6  XMOV.L	Rn, (Xm, Ro)		//
** F0nm_8Go6 ? XMOVTT	Xm, Ro, Xn			//Set Type Tag (BCE)
** F0nm_8go7  XMOV.Q	Rn, (Xm, Ro)		//
** F0nm_8Go7  XMOV.X	Xn, (Xm, Ro)		//
** F0nm_8gd8  XMOV.B	(Xm, Disp5), Rn		//
** F0nm_8Gd8  XMOVU.B	(Xm, Disp5), Rn		//
** F0nm_8gd9  XMOV.W	(Xm, Disp5), Rn		//
** F0nm_8Gd9  XMOVU.W	(Xm, Disp5), Rn		//
** F0nm_8gdA  XMOV.L	(Xm, Disp5), Rn		//
** F0nm_8GdA  XMOVU.L	(Xm, Disp5), Rn		//
** F0nm_8gdB  XMOV.Q	(Xm, Disp5), Rn		//
** F0nm_8GdB  XMOV.X	(Xm, Disp5), Xn		//
** F0nm_8goC  XMOV.B	(Xm, Ro), Rn		//
** F0nm_8GoC  XMOVU.B	(Xm, Ro), Rn		//
** F0nm_8goD  XMOV.W	(Xm, Ro), Rn		//
** F0nm_8GoD  XMOVU.W	(Xm, Ro), Rn		//
** F0nm_8goE  XMOV.L	(Xm, Ro), Rn		//
** F0nm_8GoE  XMOVU.L	(Xm, Ro), Rn		//
** F0nm_8goF  XMOV.Q	(Xm, Ro), Rn		//
** F0nm_8GoF  XMOV.X	(Xm, Ro), Xn		//


* F0nm_9eoZ  (More Load/Store Space)
** F0nm_9go0 ? MOV.TW	Rn, (Rm, Disp5)		//(LDST48) Store TWORD
** F0nm_9Go0 ? CMPQEQ	Rm, Imm5us, Rn		//(CMP3R) Rn = Rm==Imm5
** F0nm_9go1 ? MOV.HTW	Rn, (Rm, Disp5)		//(LDST48) Store High TWORD
** F0nm_9Go1 ? CMPQGT	Rm, Imm5us, Rn		//(CMP3R) Rn = Rm>Imm5
*** (Jumbo+WI)|WQ: Becomes CMPQHI (Unsigned Greater-Than)
** F0nm_9go2 -
** F0nm_9Go2 ? CMPQNE	Rm, Imm5us, Rn		//(CMP3R) Rn = Rm!=Imm5
** F0nm_9go3 -
** F0nm_9Go3 ? CMPQLT	Rm, Imm5us, Rn		//(CMP3R) Rn = Rm<Imm5
*** (Jumbo+WI)|WQ: Becomes CMPQBI (Unsigned Less-Than)

** F0nm_9go4 ? MOV.TW	Rn, (Rm, Ro)		//(LDST48) Store TWORD
** F0nm_9Go4 ? CMPQEQ	Rm, Ro, Rn			//(CMP3R) Rn = Rm==Ro
** F0nm_9go5 ? MOV.HTW	Rn, (Rm, Ro)		//(LDST48) Store High TWORD
** F0nm_9Go5 ? CMPQGT	Rm, Ro, Rn			//(CMP3R) Rn = Rm>Ro
** F0nm_9go6 -
** F0nm_9Go6 ? CMPQNE	Rm, Ro, Rn			//(CMP3R) Rn = Rm!=Ro
** F0nm_9go7 -
** F0nm_9Go7 ? CMPQGE	Rm, Ro, Rn			//(CMP3R) Rn = Rm>=Ro

** F0nm_9go8 ? MOV.TW	(Rm, Disp5), Rn		//(LDST48) Load TWORD
** F0nm_9Go8 ? MOVU.TW	(Rm, Disp5), Rn		//(LDST48) Load TWORD
** F0nm_9go9 ? MOV.HTW	(Rm, Disp5), Rn		//(LDST48) Load High TWORD
** F0nm_9Go9 -
** F0nm_9goA -
** F0nm_9GoA -
** F0nm_9goB -
** F0nm_9GoB -
** F0nm_9goC ? MOV.TW	(Rm, Ro), Rn		//(LDST48) Load TWORD
** F0nm_9GoC ? MOVU.TW	(Rm, Ro), Rn		//(LDST48) Load TWORD
** F0nm_9goD ? MOV.HTW	(Rm, Ro), Rn		//(LDST48) Load High TWORD
** F0nm_9GoD -
** F0nm_9goE -
** F0nm_9GoE -
** F0nm_9goF -
** F0nm_9GoF -

* F0nm_AeoZ  (3R Space)
* F0nm_BeoZ  (3R Space)

* F0dd_Cddd  BRA	(PC, disp20s)		//Branch, +/- 1MB
* F0dd_Dddd  BSR	(PC, disp20s)		//Call, +/- 1MB
* F0dd_Eddd / BT	(PC, disp20s)		//(Dropped), Branch True, 1
* F0dd_Fddd / BF	(PC, disp20s)		//(Dropped), Branch False, 1

Branch ops may only exist in lane 1.

1: These are now to be encoded as as E0dd_Cddd and E4dd_Cddd.
This space will now be marked as subject to being reclaimed.


==== F1zz Instruction Block ====

F1zz Instruction Block:

* F1nm_Xeii  (MOV Disp9 Block)
** F1nm_0gdd  MOV.B		Rn, (Rm, disp9u)
** F1nm_0Gdd  LEA.B		(Rm, disp9u), Rn
** F1nm_1gdd  MOV.W		Rn, (Rm, disp9u)
** F1nm_1Gdd  LEA.W		(Rm, disp9u), Rn
** F1nm_2gdd  MOV.L		Rn, (Rm, disp9u)
** F1nm_2Gdd  LEA.L		(Rm, disp9u), Rn
** F1nm_3gdd  MOV.Q		Rn, (Rm, disp9u)
** F1nm_3Gdd  LEA.Q		(Rm, disp9u), Rn
** F1nm_4gdd  FMOV.S	Rn, (Rm, disp9u)	//(FMOV)
** F1nm_4Gdd  FMOV.H	Rn, (Rm, disp9u)	//(FMOV)
** F1nm_5gdd  MOV.C		Cn, (Rm, disp9u)	//(MOVC)
** F1nm_5Gdd  MOV.X		Rn, (Rm, disp9u)	//(MOVX)
** F1nm_6gdd  FMOV.S	(Rm, disp9u), Rn	//(FMOV)
** F1nm_6Gdd  FMOV.H	(Rm, disp9u), Rn	//(FMOV)
** F1nm_7gdd  MOV.C		(Rm, disp9u), Cn	//(MOVC)
** F1nm_7Gdd  MOV.X		(Rm, disp9u), Rn	//(MOVX)
** F1nm_8gdd  MOV.B		(Rm, disp9u), Rn
** F1nm_8Gdd  MOVU.B	(Rm, disp9u), Rn
** F1nm_9gdd  MOV.W		(Rm, disp9u), Rn
** F1nm_9Gdd  MOVU.W	(Rm, disp9u), Rn
** F1nm_Agdd  MOV.L		(Rm, disp9u), Rn
** F1nm_AGdd  MOVU.L	(Rm, disp9u), Rn
** F1nm_Bgdd  MOV.Q		(Rm, disp9u), Rn
** F1nm_BGdd / MOVD.L	(Rm, disp9u), Rn
** F1zz_Czzz
*** F1nm_Cpdd ? JTSTT	Rm, Rn, disp8s	//(JCMP)
*** F1nm_CPdd ? JTSTQT	Rm, Rn, disp8s	//(JCMP)
*** F1nm_Cqdd ? JTSTF	Rm, Rn, disp8s	//(JCMP)
*** F1nm_CQdd ? JTSTQF	Rm, Rn, disp8s	//(JCMP)
** F1zz_Dzzz
*** F1nm_Dpdd ? JCMPGT	Rm, Rn, disp8s	//(JCMP)
*** F1nm_DPdd ? JCMPQGT	Rm, Rn, disp8s	//(JCMP)
*** F1nm_Dqdd ? JCMPLE	Rm, Rn, disp8s	//(JCMP)
*** F1nm_DQdd ? JCMPQLE	Rm, Rn, disp8s	//(JCMP)
** F1zz_Ezzz
*** F1nm_Epdd ? JCMPHI	Rm, Rn, disp8s	//(JCMP)
*** F1nm_EPdd ? JCMPQHI	Rm, Rn, disp8s	//(JCMP)
*** F1nm_Eqdd ? JCMPLS	Rm, Rn, disp8s	//(JCMP)
*** F1nm_EQdd ? JCMPQLS	Rm, Rn, disp8s	//(JCMP)
** F1zz_Fzzz
*** F1nm_Fpdd ? JCMPEQ	Rm, Rn, disp8s	//(JCMP)
*** F1nm_FPdd ? JCMPQEQ	Rm, Rn, disp8s	//(JCMP)
*** F1nm_Fqdd ? JCMPNE	Rm, Rn, disp8s	//(JCMP)
*** F1nm_FQdd ? JCMPQNE	Rm, Rn, disp8s	//(JCMP)


==== F2zz Instruction Block ====

F2zz Instruction Block  (Imm9 Block):

* F2nm_0gjj  ADD		Rm, Imm9u, Rn		// Rn=Rm+Imm9u
* F2nm_0Gjj  -
* F2nm_1gjj  ADD		Rm, Imm9n, Rn		// Rn=Rm+Imm9n
* F2nm_1Gjj  -
* F2nm_2gjj  MULS{.L}	Rm, Imm9u, Rn		//32*32=>32, Signed result
* F2nm_2Gjj  MULU{.L}	Rm, Imm9u, Rn		//32*32=>32, Unsigned result
* F2nm_3gjj  ADDS.L		Rm, Imm9u, Rn		//ADD 32 SX, Zero-Ext Imm
* F2nm_3Gjj  ADDU.L		Rm, Imm9u, Rn		//ADD 32 ZX, Zero-Ext Imm
* F2nm_4gjj  ADDS.L		Rm, Imm9n, Rn		//ADD 32 SX, One-Ext Imm
* F2nm_4Gjj  ADDU.L		Rm, Imm9n, Rn		//ADD 32 ZX, One-Ext Imm
* F2nm_5gjj  AND		Rm, Imm9us, Rn		//
* F2nm_5Gjj ? RSUB		Rm, Imm9us, Rn		// Rn=Imm9u-Rm
* F2nm_6gjj  OR			Rm, Imm9u, Rn		//
* F2nm_6Pjj  SHADX		Xm, Imm8s, Xn		//(ALUX)
* F2nm_7gjj  XOR		Rm, Imm9u, Rn		//
* F2nm_7Pjj  SHLDX		Xm, Imm8s, Xn		//(ALUX)

* F2nm_8pjj  SHAD{.L}	Rm, Imm8s, Rn		//E.i=0
* F2nm_8Pjj  SHAD.Q		Rm, Imm8s, Rn		//E.i=0
* F2nm_8qjj  PSHUF.B	Rm, Imm8u, Rn		//(GSV) E.i=1
* F2nm_8Qjj  PSHUF.W	Rm, Imm8u, Rn		//(GSV) E.i=1

* F2nm_9pjj  SHLD{.L}	Rm, Imm8s, Rn		//E.i=0
* F2nm_9Pjj  SHLD.Q		Rm, Imm8s, Rn		//E.i=0
* F2nm_9qjj ? PCONV		Rm, Imm8u, Rn		//(GSVX) E.i=1, Packed Convert
* F2nm_9Qjj ? PCONVX	Rm, Imm8u, Rn		//(GSVX) E.i=1, Packed Convert

* F2nz_Aejj  (Imm10 Space)
* F2nz_Bejj  (Imm10 Space)

* F2nz_Cfjj
** F2n0_Chjj  LDIZ		Imm10u, Rn				//(PrWEX) Alt
** F2n0_CHjj  LDIMIZ	Imm10u, Rn				//(PrWEX) Rn=(Imm10u<<16)
** F2n1_Chjj  LDIN		Imm10n, Rn				//(PrWEX) Alt
** F2n1_CHjj  LDIMIN	Imm10n, Rn				//(PrWEX) Rn=(Imm10n<<16)
** F2n2_Cpjj  LDISH		Imm8u, Rn				//(PrWEX) Rn=(Rn<<8)|Imm8
*** Jumbo: LDISH32	Imm32, Rn					//Q=0
** F2n3_Cfjj  LDIHI{Q}	Imm10u, Rn				//Load immed into High bits (31:22) or (63:54)
** F2n4_Cfjj  TST{Q}	Imm10u, Rn				//SR.T=!(Imm AND Rn)
** F2n5_Cfjj  TST{Q}	Imm10n, Rn				//SR.T=!(Imm AND Rn)
** F2n6_Cfjj  CMP{Q}HS	Imm10u, Rn				//Unsigned Rn GE Imm
** F2n7_Cfjj  CMP{Q}HS	Imm10n, Rn				//Unsigned Rn GE Imm
** F2n8_Cfjj  CMP{Q}HI	Imm10u, Rn				//Unsigned Rn GT Imm
** F2n9_Cfjj  CMP{Q}HI	Imm10n, Rn				//Unsigned Rn GT Imm
** F2nA_Cfjj  CMP{Q}GE	Imm10u, Rn				//Signed Rn GE Imm
** F2nB_Cfjj  CMP{Q}GE	Imm10n, Rn				//Signed Rn GE Imm
** F2nC_Cfjj  CMP{Q}EQ	Imm10u, Rn				//Rn==Imm
** F2nD_Cfjj  CMP{Q}EQ	Imm10n, Rn				//Rn==Imm
** F2nE_Cfjj  CMP{Q}GT	Imm10u, Rn				//Signed Rn GT Imm
** F2nF_Cfjj  CMP{Q}GT	Imm10n, Rn				//Signed Rn GT Imm

* F2nz_Dfjj
** F2n0_Dhjj  ADD			Imm10u, Rn			//Rn=Rn+Imm10u
** F2n1_Dhjj  ADD			Imm10n, Rn			//Rn=Rn+Imm10n
** F2n2_Dfjj  MUL{S/U}.W	Imm10u, Rn			//Rn=Rn*Imm10u
** F2n3_Dfjj  MUL{S/U}.W	Imm10n, Rn			//Rn=Rn*Imm10n
** F2n4_Dfjj ? LDIROZ{Q}	Imm10u, Rn			//Load + ROTL
** F2n5_Dfjj ? LDIRON{Q}	Imm10n, Rn			//Load + ROTL
** F2n6_Dfjj /? JCMPZ{Q}xx	Rn, Disp8s			//(JCMPZ) JCmp (Rn xx 0)
** F2n7_Dfjj /? JCMPZ{Q}xx	Rn, Disp8s			//(JCMPZ) JCmp (Rn xx 0)

** F2n8_Dhjj ? MOV.L	Rn, (GBR, Disp10u*4)	// (E.q=0)
** F2n8_DHjj -
** F2n9_Dhjj ? MOV.Q	Rn, (GBR, Disp10u*8)	// (E.q=0)
** F2n9_DHjj ? MOV.X	Xn, (GBR, Disp10u*8)	// (E.q=1)
** F2nA_Dhjj ? MOV.L	(GBR, Disp10u*4), Rn	// (E.q=0)
** F2nA_DHjj ? MOVU.L	(GBR, Disp10u*4), Rn	// (E.q=1)
** F2nB_Dhjj ? MOV.Q	(GBR, Disp10u*8), Rn	// (E.q=0)
** F2nB_DHjj ? MOV.X	(GBR, Disp10u*8), Xn	// (E.q=1)

** F2nC_Dhjj ? FCMPEQ	Imm10fp, Rn				//(FpImm) (E.q=0) Rn==Imm
** F2nC_DHjj ? FLDI		Imm10fp, Rn				//(FpImm) (E.q=1) Rn=Imm10fp
** F2nD_Dfjj ? -
** F2nE_Dhjj ? FCMPGT	Imm10fp, Rn				//(FpImm) (E.q=0) FCMPGT
** F2nE_DHjj ? FADD		Imm10fp, Rn				//(FpImm) (E.q=1) FADD
** F2nF_Dhjj ? FCMPGE	Imm10fp, Rn				//(FpImm) (E.q=0) FCMPGE
** F2nF_DHjj ? FMUL		Imm10fp, Rn				//(FpImm) (E.q=1) FMUL

* F2nz_Efjj
** F2n0_Ehjj  LEAT.B	Imm10u, Rn				//(TTAG) LEAT.B (Rn, Imm10u), Rn
** F2n0_EHjj  XLEA.B	Imm10u, Rn				//(TTAG) XLEA.B (Xn, Imm10u), Xn
** F2n1_Ehjj  LEAT.B	Imm10n, Rn				//(TTAG) LEAT.B (Rn, Imm10n), Rn
** F2n1_EHjj  XLEA.B	Imm10n, Rn				//(TTAG) XLEA.B (Xn, Imm10n), Xn

** F2n4_Efjj ? BREQ.{L/Q}	Rn, Disp10u			//(JCMPZ) JCmp (Rn EQ 0)
** F2n5_Efjj ? BREQ.{L/Q}	Rn, Disp10n			//(JCMPZ) JCmp (Rn EQ 0)
** F2n6_Efjj ? BRNE.{L/Q}	Rn, Disp10u			//(JCMPZ) JCmp (Rn NE 0)
** F2n7_Efjj ? BRNE.{L/Q}	Rn, Disp10n			//(JCMPZ) JCmp (Rn NE 0)

** F2n8_Efjj ? BRLT.{L/Q}	Rn, Disp10u			//(JCMPZ) JCmp (Rn LT 0)
** F2n9_Efjj ? BRLT.{L/Q}	Rn, Disp10n			//(JCMPZ) JCmp (Rn LT 0)
** F2nA_Efjj ? BRGE.{L/Q}	Rn, Disp10u			//(JCMPZ) JCmp (Rn GE 0)
** F2nB_Efjj ? BRGE.{L/Q}	Rn, Disp10n			//(JCMPZ) JCmp (Rn GE 0)

** F2nC_Efjj ? BRLE.{L/Q}	Rn, Disp10u			//(JCMPZ) JCmp (Rn LE 0)
** F2nD_Efjj ? BRLE.{L/Q}	Rn, Disp10n			//(JCMPZ) JCmp (Rn LE 0)
** F2nE_Efjj ? BRGT.{L/Q}	Rn, Disp10u			//(JCMPZ) JCmp (Rn GT 0)
** F2nF_Efjj ? BRGT.{L/Q}	Rn, Disp10n			//(JCMPZ) JCmp (Rn GT 0)

* F2nz_Ffjj


Jumbo Prefixed Forms:
* FE F2nm_0gjj  ADD			Rm, Imm33s, Rn
* FE F2nm_0Gjj  -
* FE F2nm_1gjj  /? ADD		Rm, Imm33s, Rn			//(To Drop)
* FE F2nm_1Pjj  FADD		Rm, Imm32f, Rn			//(FPIMM)
* FE F2nm_1Qjj  FADDA		Rm, Imm32f, Rn			//(FPIMM)
* FE F2nm_2gjj  MULS.L		Rm, Imm33s, Rn
* FE F2nm_2Gjj  MULU.L		Rm, Imm33s, Rn
* FE F2nm_3gjj  ADDS.L		Rm, Imm33s, Rn
* FE F2nm_3Gjj  ADDU.L		Rm, Imm33s, Rn
* FE F2nm_4gjj  -
* FE F2nm_4Pjj  FMUL		Rm, Imm32f, Rn			//(FPIMM)
* FE F2nm_4Qjj  FMULA		Rm, Imm32f, Rn			//(FPIMM)
* FE F2nm_5gjj  AND			Rm, Imm33s, Rn
* FE F2nm_5Gjj  RSUB		Rm, Imm33s, Rn
* FE F2nm_6gjj  OR			Rm, Imm33s, Rn
* FE F2nm_6Gjj  -
* FE F2nm_7gjj  XOR			Rm, Imm33s, Rn
* FE F2nm_7Gjj  -

* FE F2nm_8pjj / FADD		Rm, Imm32f, Rn			//(FPIMM)
* FE F2nm_8Pjj  PADD.F		Rm, Imm32fv, Rn			//(NNX)
* FE F2nm_8qjj  -
* FE F2nm_8Qjj  -

* FE F2nm_9pjj / FMUL		Rm, Imm32f, Rn			//(FPIMM)
* FE F2nm_9Pjj  PMUL.F		Rm, Imm32fv, Rn			//(NNX)
* FE F2nm_9qjj  -
* FE F2nm_9Qjj  -

* FE F2nC_Dhjj  FCMPEQ		Imm32f, Rn				//(FpImm) (E.q=0) Rn==Imm
* FE F2nE_Dhjj  FCMPGT		Imm32f, Rn				//(FpImm) (E.q=0) FCMPGT
* FE F2nF_Dhjj  FCMPGE		Imm32f, Rn				//(FpImm) (E.q=0) FCMPGE


* FE-FE F2nm_0gjj  ADD			Rm, Imm57s, Rn
* FE-FE F2nm_0Gjj  -

* FE-FE F2nm_1gjj  -
* FE-FE F2nm_1Gjj  FADD			Rm, Imm56f, Rn			//(FPIMM)

* FE-FE F2nm_2gjj  -
* FE-FE F2nm_2Gjj  / FMUL		Rm, Imm56f, Rn			//(FPIMM)

* FE-FE F2nm_3gjj  -
* FE-FE F2nm_3Gjj  / FADD		Rm, Imm56f, Rn			//(FPIMM)

* FE-FE F2nm_4gjj  -
* FE-FE F2nm_4Gjj  FMUL			Rm, Imm56f, Rn			//(FPIMM)

* FE-FE F2nm_5gjj  AND			Rm, Imm57s, Rn
* FE-FE F2nm_5Gjj  -
* FE-FE F2nm_6gjj  OR			Rm, Imm57s, Rn
* FE-FE F2nm_6Gjj  -
* FE-FE F2nm_7gjj  XOR			Rm, Imm57s, Rn
* FE-FE F2nm_7Gjj  -


* FE-FE F2nm_6gjj  OR			Rm, Imm57s, Rn
* FE-FE F2nm_6Pjj / FADD			Rm, Imm56f, Rn			//(FPIMM)
* FE-FE F2nm_6Qjj / FMUL			Rm, Imm56f, Rn			//(FPIMM)

* FE-FE F2nm_7gjj  XOR			Rm, Imm57s, Rn
* FE-FE F2nm_7Pjj  PMACSH.H		Rm, Imm48fv8sh, Rn		//(NNX)
* FE-FE F2nm_7Qjj  PMACSH.F		Xm, Imm48fv8sh, Xn		//(NNX)

* FE-FE F2nm_8pjj  PADD.H		Rm, Imm56fv, Rn			//(NNX)
* FE-FE F2nm_8Pjj  PADD.F		Xm, Imm56fv, Xn			//(NNX)
* FE-FE F2nm_8qjj  PMUL.H		Rm, Imm56fv, Rn			//(NNX)
* FE-FE F2nm_8Qjj  PMUL.F		Xm, Imm56fv, Xn			//(NNX)
* FE-FE F2nm_9pjj  PADDSH.H		Rm, Imm48fv8sh, Rn		//(NNX)
* FE-FE F2nm_9Pjj  PADDSH.F		Xm, Imm48fv8sh, Xn		//(NNX)
* FE-FE F2nm_9qjj  PMULSH.H		Rm, Imm48fv8sh, Rn		//(NNX)
* FE-FE F2nm_9Qjj  PMULSH.F		Xm, Imm48fv8sh, Xn		//(NNX)


==== F3zz Instruction Block ====

The F3zz Block is currently intended for user-defined or implementation dependent instructions. It will be assumed that it followes the same basic instruction format as the F0, F1, or F2 blocks.

Potentially this block will consist of multiple dynamically configurable blocks.


==== F8zz Instruction Block ====

F8zz Instruction Block:

* F80n_iiii  LDIZ		Imm16u, Rn		//R0..R15, Zero Extend
* F81n_iiii  LDIZ		Imm16u, Rk		//R16..R31, Zero Extend
* F82n_iiii  LDIN		Imm16n, Rn		//R0..R15, One Extend
* F83n_iiii  LDIN		Imm16n, Rk		//R16..R31, One Extend
* F84n_iiii  ADD		Imm16s, Rn		//R0..R15
* F85n_iiii  ADD		Imm16s, Rk		//R16..R31
* F86n_iiii  LDISH16	Imm16u, Rn		//R0..R15
* F87n_iiii  LDISH16	Imm16u, Rk		//R16..R31
* F88n_iiii  FLDCH		Imm16u, Rn		//R0..R15, Load Half-Float
* F89n_iiii  FLDCH		Imm16u, Rk		//R16..R31, Load Half-Float
* F8An_dddd ? LEA.Q		(GBR, Disp16u), Rn
* F8Bn_dddd ? LEA.Q		(GBR, Disp16u), Rk
* F8Cn_iiii  -
* F8Dn_iiii  -
* F8En_iiii  -
* F8Fn_iiii  -

The FCzz block will repeat the F8zz block, but with the primary difference that FCzz will indicate a WEX2 form.

The layout of the F8 block will be modified with two Jumbo prefixes (Jumbo96):
* FE-FE F80n_iiii  LDI		Imm64, Rn		//R0..R15
* FE-FE F81n_iiii  LDI		Imm64, Rk		//R16..R31
* FE-FE F82n_iiii  LDI		Imm64, Rk		//R32..R47
* FE-FE F83n_iiii  LDI		Imm64, Rk		//R48..R63
* FE-FE F84n_iiii  ADD		Imm64, Rn		//R0..R15
* FE-FE F85n_iiii  ADD		Imm64, Rk		//R16..R31
* FE-FE F86n_iiii  ADD		Imm64, Rn		//R32..R47
* FE-FE F87n_iiii  ADD		Imm64, Rk		//R48..R63

* FE-FE F88n_iiii ? PLDCXH	Imm64, Xn		//R0..R14, R32..R46		//(GSVFX?)
* FE-FE F89n_iiii ? PLDCXH	Imm64, Xn		//R16..R30, R48..R62	//(GSVFX?)

Or, in effect, the Rn field will be expanded to 6 bits.


As a special case:
* F88F_iiii  BREAK		Imm16u			//Breakpoint with Imm16
** The Imm16 may be used for debugging.
** This special case will be N/A in XG2RV mode.


==== F9zz Instruction Block ====

Reserved for now. This block will use the same format as the F0 block.


==== FAzz/FBzz Instruction Block ====

FAzz/FBzz Instructions:

* FAjj_jjjj	 LDIZ	Imm24u, DLR		//Zero Extend
* FBjj_jjjj	 LDIN	Imm24n, DLR		//One Extend

Jumbo64 (Optional):
* FEjj_jjjj_FAjj_jjjj	 LDIZ	Imm48u, DLR		//Zero Extend
* FEjj_jjjj_FBjj_jjjj	 LDIN	Imm48n, DLR		//One Extend
* FFjj_jjjj_FAjj_jjjj	 JMP	Abs48			//Abs48 Branch
* FFjj_jjjj_FBjj_jjjj	 JSR	Abs48			//Abs48 Call


==== EAzz/EBzz/EEzz/EFzz PrWEX Instruction Blocks ====

* EAnm_0eoz..EAnm_Beoz
** Maps to the same instruction space as F0nm_0eoz..F0nm_Beoz.
* EBnm_0fjj..EBnm_Ffjj
** Maps to the same instruction space as F2nm_0fjj..F2nm_Ffjj.


The PrWEX Block will be special:
* Support for PrWEX is optional.
* The PrWEX Block does not exist in Lane 1 or in scalar code.


The purpose of the PrWEX block will be to make WEX usable with predicated instructions. PrWEX instructions will exist in the same lanes as normal WEX instructions, and will follow similar restrictions.

PrWEX may be mixed with unconditional instructions, and may be used with a predicated op in Lane 1. The predication state of each execute lane will be independent.

The EDzz and EFzz block will repeat the EAzz and EBzz block, but differ in their predicate.
* EAzz will be Execute if True.
* EEzz will be Execute if False.


=== Jumbo Instructions ===

Several Jumbo blocks will be defined:
* FEjj_jjjj Jumbo_Imm24
* FFwZ_zzzz Jumbo_Op64

Jumbo Instructions will be larger instructions (64 or 96 bits), composed of multiple conjoined instruction words.
Jumbo instructions will begin with FEzz_zzzz, which will serve as a "jumbo prefix".
The following 32-bit instructions will be considered part of the same logical instruction.
The final instruction in the sequence (which is not a Jumbo Prefix) will encode the operation.

Some instructions may only be allowed with a single or with a pair of prefixes, and other cases may potentially encode alternate instructions.


If the instruction following the prefix is a another jumbo prefix, the instruction will be 96 bits, otherwise it will be 64.

All instruction words in the sequence will be required to be 32-bit encodings.
* A jumbo prefix followed by a 16-bit op is currently undefined.
** It is possible this could serve as an alternative 48-bit space.

Jumbo64 forms of F1, F2, or F8 block instructions will match their 32-bit counterparts, but with the immediate field expanded.
F1 and F2 block Imm9/Disp9 and Imm10 will have a 33 bit immediate.

Exapansion within the F1 and F2 blocks will use the jumbo prefix as bits 
(31:8) of the immediate, with bits (7:0) coming from the base instruction.

F1 block immediates will be expanded to being 33-bit sign-extended.
* The disp8s instructions will become disp32s.
* The disp9 instructions will become disp33s.
** EI will serve as a sign-extension bit.


F2 block immediates will be expanded to being 33-bit sign-extended.
* The Imm9 and Imm10 instructions will become Imm33s.
** EI will serve as a sign-extension bit.
* Ops with Zero/One extension are to match the EI bit.
* For Imm10 forms, EM is reserved and set to zero.

With the FpImm extension, some instructions will encode an Imm32f. This will be encoded as the high 32 bits of a Binary64 value. The sign extension is to be set to 0.


Two Jumbo_Imm prefixes on a F2 Block instruction will result in an Imm57s.
* In Baseline Mode, this will have a 57-bit sign-extended value.
** First jumbo prefix will give (55:32);
** Section Jumbo prefix will give (31:8);
** Base instruction will give (7:0);
** Ei will give the sign extension.
* In XG2 Mode, the Wnmi bits will modify the high bits.
** First prefix will modify (62:60);
** Second prefix will modify (59:57);
** The Wi from the base instruction will modify (56).
** These will XOR against the sign extension from Ei.
** This will effectively encode an Imm64.

With the FpImm extension, there will also be an Imm56f.
* This will encode a Binary64 as a modified Imm57s.
* Imm57s(55:0) will map to Double(63:8)
* Imm57s(62:56) will map to Double(7:1)
* Imm57s(63) will map to Double(0)


Use with the F0 block will depend on the instruction:
* Branch ops will extend the size of the displacement to 44 bits.
** However, branches beyond 32-bits are currently undefined.
* Imm5u/Disp5u ops will be expanded to 29-bits sign-extended (Imm29s).
** Bits (27:4) will come from the Jumbo Prefix, (3:0) from the op.
** EI will serve as a sign-extension bit.


Use of two Jumbo Imm24 prefixes followed by an F8-block instruction will encode an instruction with a 64-bit immediate.

Note that in the Imm64 case, the zero vs one extended distinction no longer exists.

The layout of the F8 block will thus be modified slighly with XGPR and two Imm24 prefixes (Jumbo96):
* FE-FE F80n_iiii  LDI		Imm64, Rn		//R0..R15
* FE-FE F81n_iiii  LDI		Imm64, Rk		//R16..R31
* FE-FE F82n_iiii  LDI		Imm64, Rk		//R32..R47
* FE-FE F83n_iiii  LDI		Imm64, Rk		//R48..R63
* FE-FE F84n_iiii  ADD		Imm64, Rn		//R0..R15
* FE-FE F85n_iiii  ADD		Imm64, Rk		//R16..R31
* FE-FE F86n_iiii  ADD		Imm64, Rn		//R32..R47
* FE-FE F87n_iiii  ADD		Imm64, Rk		//R48..R63


==== Jumbo Op64 ====

* FFw0_0ddd_F0dd_Cddd  BRA	(PC, Disp33s)
* FFw0_0ddd_F0dd_Dddd  BSR	(PC, Disp33s)
* FFw0_0ddd_E0dd_Cddd  BT	(PC, Disp33s)
* FFw0_0ddd_E4dd_Cddd  BF	(PC, Disp33s)


* FFjj_jjjj_FAjj_jjjj  JMP	Abs48				//Abs48 Branch
* FFjj_jjjj_FBjj_jjjj  JSR	Abs48				//Abs48 Call

Alternative 2 (Hx, W0):
* FFw0_0vzz_F0nm_ZeoZ  "OP Rm, Ro, Rn"
* FFw0_0vii_F0nm_ZeoZ  "OP Rm, Ro, Rn, Imm8"

* FFw0_0Vii_F0nm_ZgoZ  "OP Rm, Imm9s, Ro, Rn"	//3R/3RI: 4R
* FFw0_0Vii_F0nm_ZGiZ  "OP Rm, Imm17s, Rn"		//3R/3RI: Imm5u

* FFw0_0Vii_F0nm_ZeiZ  "OP Rm, Imm17s, Rn"		//3R Ops
* FFw0_0Vpp_F0nm_ZeoZ  "OP Rm, Ro, Rp, Rn"		//4R Ops (MAC/etc)

* FFw0_00ii_F1nm_Zeii  "OP (Rm, Disp17s), Rn"
* FFw0_00ii_F2nm_Zeii  "OP Rm, Imm17, Rn"
* FFw0_00ii_F2nZ_Zeii  "OP Imm18, Rn"

* FFw0_0vzz_E0nm_ZeoZ  "OP Rm, Ro, Rn" 

* FFw0_00ii_E1nm_Zeii  "OP (Rm, Disp17s), Rn" 
* FFw0_00ii_E2nm_Zeii  "OP Rm, Imm17, Rn" 

* FFw0_iiii_F8Zn_iiii  "OP Imm33s, Rn"

* FFw0_iiii_F84n_iiii  ADD	Imm33s, Rn		//(Op64)
* FFw0_iiii_F85n_iiii  ADD	Imm33s, Rk		//(Op64)
** Wnmi=zz00: ADD
** Wnmi=zz01: MOVLD		//Load Imm32 into high 32-bits of dest.
** Wnmi=zz10: ? REQDCB	//Request Dynamic Config Block
** Wnmi=zz11: XMOVTT	//Split Imm32 into high 16-bits of target 

* FFw0_iiii_F88n_iiii  FLDCF	Imm32u, Rn		//(Op64)
* FFw0_iiii_F89n_iiii  FLDCF	Imm32u, Rk		//(Op64)
** Wnmi=zz00: FLDCF
** Wnmi=zz01: ? PLDCH (2x Binary16 to 2x Single)
** Wnmi=zz10: ? PLDCM8SH (4x FP8S to 4x Binary16)
** Wnmi=zz11: ? PLDCM8UH (4x FP8U to 4x Binary16)

** These extend the existing encoding space
** Relying on the base-op for predication and/or WEX.

For instructions within the W0 encoding spaces, the 32-bits following the Jumbo Op64 prefix will represent the same encoding space as used by 32-bit ops. This is not necessarily true of operations in other encoding spaces.

The w and v fields:
* W: wnmi (or qnmi)
* V: wiii (3R / 3RI)
* V: wssi (Ld / St)
** ss=Scale

The W field, in Baseline mode, will encode the Wn, Wm, and Wi fields.
* Nominally, these will extend the register field to 6 bits.
* In XG2, these bits will nominally be encoded in the final instruction.
** The Op64 prefix bits may be used if the instruction bits are 111.
* The Wq field will effectively be an opcode bit.



The V.w flag will apply to 3R/3RI (Imm5u) encodings (V.w, E.q):
* { 0, 0 }: 3R ( Rm, Ro, Rn, Rn )
* { 1, 0 }: 4R ( Rm, Ro, Rp, Rn )
* { 0, 1 }: 3R ( Rm, Imm17s, Rn, Rn )
* { 1, 1 }: 4R ( Rm, Imm11s, Ro, Rn )

The V.w flag will apply to 3R (Basic) encodings (V.w):
* 0: 3R ( Rm, Ro, Rn )
* 1: 3R ( Rm, Imm17s, Rn )

The V.w flag will apply to 3R/4R encodings (V.w):
* 0: 3R ( Rm, Ro, Rn, Rn )
* 1: 3R ( Rm, Ro, Rp, Rn )

Where, 3R/4R will be limited to 3R instructions which could make use of a 4th register if available, such as MAC/FMAC.



Immediate values may be extended by 8 bits:
* Imm5u becomes Imm11s / Imm17s
* Imm9u becomes Imm17u or Imm18s
* Imm9n becomes Imm17n
* Imm10u becomes Imm18u or Imm19s
* Imm10n becomes Imm18n
* Imm16u becomes Imm33s
* Imm16n becomes Imm33s
* Disp9u becomes Imm18s

The extra bits are added between the base bits and E.i, so:
* Imm18 = { Wi ? -1 : 0, Em, Ei, jbits(7:0), opw_b(7:0) }
* Imm33s = { Wi ? -1 : 0, jbits(15:0), opw_b(15:0) }

For certain ops, Imm17u or Imm18u may function instead as Imm18s or Imm19s.
* In these cases, W.i will serve as a sign bit.
* In other cases, W.i is Reserved or Must Be Zero.
* In most cases, this will be for ops where:
** No one-extended partner exists;
** The existence of a one-extended partner would makes sense.

For Imm5, W.i will function as a sign bit:
* Imm17s = { Wi ? -1 : 0, Ei, jbits(11:0), opw_b(7:4) }


The W.w bit may be used to select the use of SR.S as a Predicate.

For CMPxx ops within F2zz, W.m will select SR.S as the output bit.
Note that this use will be exclusive to Op64, and will not apply to the XGPR or XG2 encodings.


==== Jumbo Op64 Extended 3R ====

* FFw0_0yyy_F0nm_ZeoZ  "OP Rm, Ro, Rn"

In these blocks, yyy will be interpreted as opcode bits.
* 000..03F will be special, in that they may be encoded via Op40x2
* 040..FFF will be assumed non-encodable via Op40x2.

These encodings are assumed for low-probability instructions, where the bulkier encoding is less of an issue.



==== Jumbo Op40x2 (Optional) ====

Will define an Op40x2 Prefix (78ww_vvii).

An Op64 Prefix may be used in a bundle:
* Op40x2 | OpB | OpA
Where 
* Op40x2 will have the form: 78ww_vvii
* OpB will be: F4..F7, or EA/EB, EE/EF
* OpA will be: F0..F3, or E0..E7
Which will be decoded as-if this were a pair of Op64 instructions.

Each virtual Op64 instruction will have a prefix encoded as if it were:
* FFw0_0vxi

Where vi will expand to vxi:
* wxjj_iiii -> wxxx_xxjj_iiii

Within RiMOV encodings, the unpacking will differ:
* 0xjj_iiii -> 0xxx_xxjj_iiii
* 1xjj_iiii -> 1xxx_xxjj_iiii	//(Rm, Disp13s)
* 1xjj_iiii -> 1jjx_xxxx_iiii	//(Rm, Ro*Sc, Disp4u/0)

* Namely: F0/F4 Block, Major=0/4/8, Minor=4..7, C..F


Possible, Op40x1:
* 78nm_ZeoZ_Ywvi
Will be unpacked as if it were:
* FFw0_0vxi_FYnm_ZeoZ
Where: Y=0..3



==== Op64 RiMOV ====

The RiMOV Extension will be a special case of Op64, which will extend the capabilities of Load/Store instructions.

This extension will provide:
* Larger displacements via Op64 encodings.
* Ability to use an encode the Scale explicitly for Loads/Stores.
* Ability to use both an index register and displacement.
** Unlike normal Load/Store operations, this displacement is unscaled.


The V.w flag will apply to Ld/St:
* 0: (Rm, Ro)		-> (Rm, Ro, Disp11u/0)
* 0: (Rm, Disp5)	-> (Rm, Disp17s)
* 1: (Rm, Ro)		-> (Rm, Ro*Sc, Disp9u/0)
* 1: (Rm, Disp5)	-> (Rm, Disp17s*1)

With both an index and displacement, the displacement field will be unscaled.
Support for non-zero displacements will be an optional extension.


Possible:
* FFw0_Uvii_F0nm_0eoZ  "LDOP/x Rm, Ro, Rn"

The U field will encode an ALU operation to be executed directly within memory:
* 0: None (Load/Store)
* 1: EXCH.x
* 2: ADD.x
* 3: SUB.x (Mem-Reg)
* 4: SUB.x (Reg-Mem)
* 5: AND.x
* 6: OR.x
* 7: XOR.x
* 8..F: Repeat
** For Load: Reserved
** For Store: Interpret Rn as Imm6u.

If Load, this will perform a Load-Operate Instruction.


==== Jumbo Op48 (Possible/Drop) ====

Possible:
Use of a jumbo prefix followed by a 16-bit instruction may be used to encode 48-bit instruction forms.

The FE and FF Op48 encodings will follow a similar pattern to the Ez and Fz Op32 encodings.

These blocks will be called FE0..FE7 and FF0..FF7.
* FE0..FE3: Execute if True
* FE4..FE7: Execute if False
* FF0..FF3: Execute Always
* FF4..FF7: Wide Execute

Where FF0..FF3 is:
* FFgz_zzzz_0Znm  -
* ...
* FFgz_zzzz_3Znm  -

While following a generic pattern:
* FFeo-xxxx-Zxnm (Generic)
* FFeZ-iiii-ZZnm "Rm, Imm17, Rn"
* FFeZ-iiii-ZZnZ "Imm17, Rn"

* FFg0_dddd_00nm  MOV.B		Rn, (Rm, Disp17s)
* FFG0_dddd_00nm  LEA.B		Rn, (Rm, Disp17s)
* FFg1_dddd_00nm  MOV.W		Rn, (Rm, Disp17s)
* FFG1_dddd_00nm  LEA.W		Rn, (Rm, Disp17s)
* FFg2_dddd_00nm  MOV.L		Rn, (Rm, Disp17s)
* FFG2_dddd_00nm  LEA.L		Rn, (Rm, Disp17s)
* FFg3_dddd_00nm  MOV.Q		Rn, (Rm, Disp17s)
* FFG3_dddd_00nm  LEA.Q		Rn, (Rm, Disp17s)
* FFg4_dddd_00nm  -
* FFg5_dddd_00nm  -
* FFg6_dddd_00nm  -
* FFg7_dddd_00nm  -
* FFg8_dddd_00nm  MOV.B		(Rm, Disp17s), Rn
* FFG8_dddd_00nm  MOVU.B	(Rm, Disp17s), Rn
* FFg9_dddd_00nm  MOV.W		(Rm, Disp17s), Rn
* FFG9_dddd_00nm  MOVU.W	(Rm, Disp17s), Rn
* FFgA_dddd_00nm  MOV.L		(Rm, Disp17s), Rn
* FFGA_dddd_00nm  MOVU.L	(Rm, Disp17s), Rn
* FFgB_dddd_00nm  MOV.Q		(Rm, Disp17s), Rn
* FFGB_dddd_00nm  MOV.X		(Rm, Disp17s), Rn
* FFgC_dddd_00nm  -
* FFgD_dddd_00nm  -
* FFgE_dddd_00nm  -
* FFgF_dddd_00nm  -

* FFe0_iiii_01nm  ADD		Rm, Imm17s, Rn
* FFe1_iiii_01nm  SUB		Rm, Imm17s, Rn
* FFe2_iiii_01nm  MULS		Rm, Imm17s, Rn
* FFe3_iiii_01nm  MULU		Rm, Imm17s, Rn
* FFe4_iiii_01nm  -
* FFe5_iiii_01nm  AND		Rm, Imm17s, Rn
* FFe6_iiii_01nm  OR		Rm, Imm17s, Rn
* FFe7_iiii_01nm  XOR		Rm, Imm17s, Rn

* FFe8_iiii_01nm  -

* FEzz_zzzz_Ezzz  (N/E)
* FEzz_zzzz_Fzzz  (N/E)


=== Op24 (Drop) ===

These are intended to only exist in microcontroller configurations.
This encoding is mutually exclusive with Op32_XGPR.
Op24 and OP32_XGPR encodings are not allowed to exist at the same time.

Alignment Restrictions:
* Note odd addresses (LSB set) may only encode a 16 or 24-bit operation.
* The normal (word-based) branch ops will always land on an even target.
* Function entry points are required to be on an even address.

Escape case for 24-bit instructions:
* ZZnm_eo (3R)
* ZZnm_eZ (2R)

These could be used to improve code density over using 32-bit encodings.

* 70nm_go  ADD		Rm, Ro, Rn
* 70nm_Go  ADD		Rm, Imm5, Rn
* 71nm_go  SUB		Rm, Ro, Rn
* 71nm_Go  SUB		Rm, Imm5, Rn
* 72nm_go  MULS		Rm, Ro, Rn
* 73nm_go  MULU		Rm, Ro, Rn
* 74nm_go  ADDS.L	Rm, Ro, Rn
* 74nm_Go  SHAD		Rm, Imm5u, Rn
* 75nm_go  AND		Rm, Ro, Rn
* 75nm_Go  SHAD		Rm, Imm5n, Rn
* 76nm_go  OR		Rm, Ro, Rn
* 76nm_Go  SHLD		Rm, Imm5u, Rn
* 77nm_go  XOR		Rm, Ro, Rn
* 77nm_Go  SHLD		Rm, Imm5n, Rn

* 78nm_ez  (2R Block)
* ...
* 7Fnm_ez  (2R Block)

The 2R blocks encode the same instructions as the F0nm_1eo8 .. F0nm_1eoF block.


Map:
* 90nm_go  MOV.B	Rn, (Rm, Disp5)
* 90nm_Go  LEA.B	(Rm, Disp5), Rn
* 91nm_go  MOV.W	Rn, (Rm, Disp5)
* 91nm_Go  LEA.W	(Rm, Disp5), Rn
* 92nm_go  MOV.L	Rn, (Rm, Disp5)
* 92nm_Go  LEA.L	(Rm, Disp5), Rn
* 93nm_go  MOV.Q	Rn, (Rm, Disp5)
* 93nm_Go  LEA.Q	(Rm, Disp5), Rn
* 94nm_go  MOV.B	Rn, (Rm, Ro)
* 94nm_Go  LEA.B	(Rm, Ro), Rn
* 95nm_go  MOV.W	Rn, (Rm, Ro)
* 95nm_Go  LEA.W	(Rm, Ro), Rn
* 96nm_go  MOV.L	Rn, (Rm, Ro)
* 96nm_Go  LEA.L	(Rm, Ro), Rn
* 97nm_go  MOV.Q	Rn, (Rm, Ro)
* 97nm_Go  LEA.Q	(Rm, Ro), Rn
* 98nm_go  MOV.B	(Rm, Disp5), Rn
* 98nm_Go  MOVU.B	(Rm, Disp5), Rn
* 99nm_go  MOV.W	(Rm, Disp5), Rn
* 99nm_Go  MOVU.W	(Rm, Disp5), Rn
* 9Anm_go  MOV.L	(Rm, Disp5), Rn
* 9Anm_Go  MOVU.L	(Rm, Disp5), Rn
* 9Bnm_go  MOV.Q	(Rm, Disp5), Rn
* 9Bdd_Gd  Bxx.B	(PC, disp13s)
* 9Cnm_go  MOV.B	(Rm, Ro), Rn
* 9Cnm_Go  MOVU.B	(Rm, Ro), Rn
* 9Dnm_go  MOV.W	(Rm, Ro), Rn
* 9Dnm_Go  MOVU.W	(Rm, Ro), Rn
* 9Enm_go  MOV.L	(Rm, Ro), Rn
* 9Enm_Go  MOVU.L	(Rm, Ro), Rn
* 9Fnm_go  MOV.Q	(Rm, Ro), Rn
* 9Fzz_Gn  (0R / 1R Space)


Bxx.B will be a byte-aligned branch op.


=== Op32 XGPR (Optional) ===

The Op32_XGPR encoding space will use an encoding space which overlaps with Op24, and is as such mutually exclusive.

Will use the encoding spaces:
* 7wnm_ZeoZ, Maps to F0nm_ZeoZ
* 9wnm_Zejj, Maps to F2nm_Zejj or F1nm_Zejj (W.i)

The w and e fields will combine to add two additional bits to GPRs:
* w: wnmi (Bit 5)
* e: qnmi (Bit 4)

With 'W.w' as the WEX bit.
* With the Imm9/Disp9 space, W.i selects between the F2 and F1 block.
** W.i=0: Map to F2
** W.i=1: Map to F1

For Disp5u encodings in the F0nm_ZeoZ block, the Disp5u is expanded to Disp6s.
* This allows these to encode a load or store with a negative displacement.


XGPR Encodings which are redundant with the Fz encodings will be reserved/invalid:
* 70 (F0), Reserved
* 78 (F4), Op40x2 Prefix
* 90 (F2), Reserved
* 91 (F1), Reserved
* 98 (F6), Jumbo Reserved
* 99 (F5), Jumbo Reserved


=== XG2 Mode (Optional) ===

This mode will disable both 16-bit ops and the normal XGPR encodings.

Instruction formats will become:
* WZnm_ZeoZ
* Wznm_Zeii

Where the high 3 bits of the first instruction word become the inverted N,M,I bits for Bit5 of each register field.
If no register field exists, and no other meaning is assigned to the bit, it is to be kept as 1. Behaviors will be otherwise similar to normal XGPR encodings.

Apart from the loss of 16-bit encodings (setting 32 bits as the smallest instruction size), the encoding rules for XG2 will be the same as those for the base ISA and XGPR.

This mode will also lose Op40x2, which will become non-encodable.

In XG2 Mode, Disp9u encodings will use W.i as a sign-extension bit.
Imm9u and Imm9n encodings will use W.i as an additional immediate bit.
Imm9us will use W.i as a sign-extension bit.

Imm5us will also use W.i as a sign-extension bit.


Imm10u and Imm10n encodings will use W.i and W.m as an additional immediate bits, effectively expanding the range of the immediate to 12 bits (using an inverted-xor scheme).

Possible: W.nmi will expand Disp20 by 3 bits (inverted-xor against the original sign extension), expanding the reach of BRA/BSR to 8MB.


Within XG2 Mode, the instruction stream will be required to have a 32-bit alignment, and the LSB for branch displacements Must Be Zero.

This mode will use (WX4,WX3,WX2,WXE) == 0100 or 0101.


=== XG2RV Mode (Optional) ===

In this mode, the encoding scheme will be the same as XG2 Mode, but differing primarily in that it will use RISC-V Mode's register remapping scheme.

Semantically, XG2RV Mode will be treated as if it were part of the RISC-V Context.

XG2RV may only exist if both XG2 Mode and RVI Mode are available.


This mode will use (WX4,WX3,WX2,WXE) == 0110 or 0111.


=== Wide Execution 2 (Optional) ===

WEX2 Would be based around 32-bit instructions, where:
* F0zz..F3zz represent the last instruction in a block.
* F4zz..F7zz represent an instruction which may execute in parallel.

* A scalar implementation will treat F4zz..F7zz as equivalent to F0zz..F3zz.
* There isn't a predefined maximum to the length of a block.
** The core will limit it to the maximum it supports.
** Dependencies between instructions in a block are not allowed.
** A block also does not require parallel execution.

Operations will be required to be valid both when executed in scalar and parallel orderings. In scalar ordering, the results of operations may be written in the sequential order of the instructions. In parallel ordering, writeback may not necessarily take place until after the end of the block.


Instructions operating in parallel will be organized into lanes.
The lanes will be encoded in reverse order, with the last instruction being 'Lane 1'. All scalar operations will take place in Lane 1.

For example:
* F4zz_zzzz F4yy_yyyy F0xx_xxxx
* Will have X in Lane 1, Y in Lane 2, and Z in Lane 3.


Operations which need to read the value from Rn will overlap with Lane 2, and will consume the Ro port for this lane (in scalar ops). In WEX, an operation may Lane 2 with such an operation if its Ro matches the Rn in Lane 1 (otherwise, the results are undefined).

For implementations with 3 or more lanes, putting a NOP3 instruction in Lane 2 with the Rn from Lane 1 will allow running other instructions in parallel with such an operation (such as a memory store). For a 2 lane implementation, operations which need 3 ports will typically be scalar-only ops.

This restriction is to allow WEX to be implemented with 2 register read ports per lane.


WEX2 Would be based around 32-bit instructions, where:
* F0zz..F3zz, represent the last instruction in a series.
* F4zz..F7zz, represent an instruction which may execute in parallel.
* F8zz/F9zz, may be used for Imm16 ops.
* FAzz/FBzz, are not allowed in a WEX2 construct (Scalar Only).
* FCzz/FDzz, Imm16 ops, in parallel.
* FEzz/FFzz, Jumbo Prefixes
* E0zz..EFzz, Conditional ops are Scalar Only.
** With the exception of the PrWEX block (WEX only).

Within a scalar implementation, F4zz..F7zz will be functionally equivalent to instructions in the F0zz..F3zz range. 


=== WEX2 Profiles ===

Thoughts:
* WEX2 may have "profiles".
* If the profiles are not compatible between the program and core:
** The core may be told to disable WEX2 via an SR flag.
** The code in question is to be executed as scalar code.
* Narrower layouts are still allowed in wider profiles.
** However, going wider than the specified profile is not allowed.
* Currently, only 32-bit instruction forms are allowed in WEX constructs.


The "WEXMD Imm4" instruction will indicate the WEX2 Profile. It will update the SR bits as appropriate for the given profile.
* This instruction will either enable or disable WEX based on the result.
* If WEX is not supported on the core, it will be treated as a NOP.
* No WEX forms will be allowed within the following 3 instructions.


Profile 0: No WEX / WEX Disabled.
* This profile will disable the use of WEX.
* Only scalar instructions are to be used in this mode.
* The use of WEX sequences will be technically undefined in this profile.
** Though, the core will most likely disable WEX, as in an unsupported profile.

Profile 1 (WEX 2-Wide): Dual Lane ALU, Single IO Lane.
* YYYY_YYYY XXXX_XXXX
* Second lane precedes first lane.
* Second lane only has ALU ops.
* Restrictions on the use of memory store.

Profile 2 (WEX 3-Wide, WEX3W): Triple Lane ALU, Single IO Lane.
* ZZZZ_ZZZZ YYYY_YYYY XXXX_XXXX
* Lanes are in a 3/2/1 ordering.
* The second and third lanes only have ALU ops.

Profile 3 (WEX 3-Wide): Triple Lane ALU, Dual Lane IO
* Lane 2 is able to perform memory Loads.
* Store is only available in Lane 1.
* Accessing MMIO is only allowed from Lane 1.
* Simultaneous load and store to the same cache line will be undefined as to whether the old or new value is read.

Profile 4 (WEX 3-Wide): Triple Lane ALU, Single IO Lane, Dual Lane FPU (FP2).
* Similar to 2, but allows FPU ops to be issued from Lane 2.
** Only certain ops may be co-issued.

Profile 5 (WEX 3-Wide): Triple Lane ALU, Dual Lane IO, Dual Lane FPU (FP2)
* Similar to 3, but allows FPU ops to be issued from Lane 2.
** Only certain ops may be co-issued.



Profile 8 (WEX 5-Wide, WEX5W): Five Lane ALU, Dual Lane IO, No Interleave
* There are 5 WEX lanes (5/4/3/2/1)
* Both the first and second lane may perform IO.
* No interleaved encodings are allowed.
** Jumbo encodings which would use interleaving are seen as Non-Encodable.
** This will be to preserve the linear / parallel equivalence rule.

Profile 9 (WEX 5-Wide, WEX5W): Five Lane ALU, Dual Lane IO
* There are 5 WEX lanes (5/4/3/2/1)
* Both the first and second lane may perform IO.
* Lanes will use interleaving rules as defined for SMT.
** SMT Interleaving rules will apply even if SMT is not used.

Profile 10 (WEX 6-Wide, WEX6W): Six Lane ALU, Dual Lane IO
* There are 6 WEX lanes (6/5/4/3/2/1)
* Both the first and second lane may perform IO.
* Lanes will use interleaving rules as defined for SMT.
** SMT Interleaving rules will apply even if SMT is not used.

Many instructions will only exist in the first lane:
* Memory operations
* Branch operations
* Compare operations
* Integer multiply
* ...

The first lane will also be the primary lane for scalar operations, and as such will have full access to the instruction set, albeit with some restrictions as to which operations are allowed to exist in the other lanes.

The result is that the second and third lanes will only have a subset of ALU operations available:
* ADD, SUB, AND, OR, XOR, MOV, ...
* SHAD, SHLD
* LDIZ, LDIN (in forms which are encodable).

The processor may behave in an implementation specific manner when faced with combinations of instructions which are not defined for the given active profile.

Note that the core is only required to provide two GPR Read ports per lane, and as such operations which use all 3 read ports (such as memory store) will face restrictions.

In Profile 1, operations which use the second read port may not be executed in parallel with an operation which uses 3 reads.

In Profile 2, only single-port ops may be used in Lane 3 if the op in Lane 1 uses 3 read ports. Nothing is allowed in Lane 3 if the operation in Lane 2 uses 3 read ports.

Note that in these scenarios, register values may "alias" in an implementation specific manner.


